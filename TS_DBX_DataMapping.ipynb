{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a968148d-00aa-4478-966e-8c60867842ef",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"tml_columns_used\":265},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760525207639}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## TML Metadata Extractor for Table/Column Mapping\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# --- Configuration ---\n",
    "CATALOG = \"ds_training_1\"\n",
    "SCHEMA = \"thoughtspot_inventory_ak\"\n",
    "TML_VOLUME = \"lvdash_files_ak/liveboard\"\n",
    "\n",
    "TML_INPUT_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{TML_VOLUME}/\"\n",
    "MAPPING_TABLE = f\"{CATALOG}.{SCHEMA}.tml_dbx_metadata_mapping\"\n",
    "FAILURE_LOG_TABLE = f\"{CATALOG}.{SCHEMA}.tml_dbx_mapping_failures\"\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Setup Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def setup_failure_log_table():\n",
    "    \"\"\"Create or recreate the failure log table.\"\"\"\n",
    "    parts = FAILURE_LOG_TABLE.split('.')\n",
    "    catalog = parts[0]\n",
    "    schema = parts[1]\n",
    "    table_name = parts[2]\n",
    "    \n",
    "    # Ensure schema exists\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog}`.`{schema}`\")\n",
    "    \n",
    "    # Drop existing table\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS `{catalog}`.`{schema}`.`{table_name}`\")\n",
    "        print(f\"Dropped existing failure log table: {table_name}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Create new failure log table\n",
    "    create_sql = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{catalog}`.`{schema}`.`{table_name}` (\n",
    "            tml_file STRING,\n",
    "            error_type STRING,\n",
    "            error_message STRING,\n",
    "            failure_timestamp TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(create_sql)\n",
    "    print(f\"Created failure log table: {table_name}\")\n",
    "\n",
    "def setup_mapping_table():\n",
    "    \"\"\"Create or recreate the metadata mapping table.\"\"\"\n",
    "    parts = MAPPING_TABLE.split('.')\n",
    "    catalog = parts[0]\n",
    "    schema = parts[1]\n",
    "    table_name = parts[2]\n",
    "    \n",
    "    # Ensure schema exists\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog}`.`{schema}`\")\n",
    "    \n",
    "    # Drop existing table\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS `{catalog}`.`{schema}`.`{table_name}`\")\n",
    "        print(f\"Dropped existing table: {table_name}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Create new table with mapping structure\n",
    "    # Note: Columns ending with _ToBeFilled are for user input\n",
    "    create_sql = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{catalog}`.`{schema}`.`{table_name}` (\n",
    "            tml_file STRING,\n",
    "            visualization_id STRING,\n",
    "            visualization_name STRING,\n",
    "            chart_type STRING,\n",
    "            tml_table_name STRING,\n",
    "            tml_table_id STRING,\n",
    "            tml_columns_used ARRAY<STRING>,\n",
    "            databricks_table_name_ToBeFilled STRING COMMENT 'For unique datasets per viz',\n",
    "            databricks_column_mapping_ToBeFilled STRING COMMENT 'For unique datasets per viz - JSON format',\n",
    "            common_dataset_name STRING COMMENT 'Shared dataset name for reuse across visualizations',\n",
    "            common_sql_query STRING COMMENT 'Common SQL query for the shared dataset',\n",
    "            common_column_mapping STRING COMMENT 'JSON mapping of common columns for shared dataset',\n",
    "            search_query STRING,\n",
    "            notes STRING,\n",
    "            extraction_timestamp TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(create_sql)\n",
    "    print(f\"Created mapping table: {table_name}\")\n",
    "\n",
    "def parse_tml_file(file_path):\n",
    "    \"\"\"Parse TML file (YAML or JSON).\"\"\"\n",
    "    content = dbutils.fs.head(file_path, 10 * 1024 * 1024)\n",
    "    try:\n",
    "        return yaml.safe_load(content)\n",
    "    except yaml.YAMLError:\n",
    "        return json.loads(content)\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Metadata Extraction Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def extract_columns_from_answer(answer: Dict) -> List[str]:\n",
    "    \"\"\"Extract all column names used in the answer.\"\"\"\n",
    "    columns = []\n",
    "    \n",
    "    # From answer_columns\n",
    "    for col in answer.get('answer_columns', []):\n",
    "        col_name = col.get('name')\n",
    "        if col_name:\n",
    "            columns.append(col_name)\n",
    "    \n",
    "    # From table ordered columns\n",
    "    table_cols = answer.get('table', {}).get('ordered_column_ids', [])\n",
    "    columns.extend([c for c in table_cols if c and c not in columns])\n",
    "    \n",
    "    return columns\n",
    "\n",
    "def extract_table_info(answer: Dict) -> tuple:\n",
    "    \"\"\"Extract table name and ID from answer.\"\"\"\n",
    "    tables = answer.get('tables', [])\n",
    "    if tables and len(tables) > 0:\n",
    "        first_table = tables[0]\n",
    "        return (\n",
    "            first_table.get('name', ''),\n",
    "            first_table.get('id', '')\n",
    "        )\n",
    "    return ('', '')\n",
    "\n",
    "def clean_field_name(field_name: str) -> str:\n",
    "    \"\"\"Remove aggregate prefixes from field names.\"\"\"\n",
    "    if not field_name:\n",
    "        return \"\"\n",
    "    import re\n",
    "    cleaned = re.sub(r'^(Total |sum\\(|count\\(|avg\\(|min\\(|max\\(|Unique Number of )', \n",
    "                     '', field_name, flags=re.IGNORECASE)\n",
    "    cleaned = re.sub(r'\\)$', '', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def extract_base_columns(columns: List[str]) -> List[str]:\n",
    "    \"\"\"Extract base column names without aggregations.\"\"\"\n",
    "    base_columns = []\n",
    "    for col in columns:\n",
    "        cleaned = clean_field_name(col)\n",
    "        if cleaned and cleaned not in base_columns:\n",
    "            base_columns.append(cleaned)\n",
    "    return base_columns\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Main Extraction Logic\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def extract_tml_metadata():\n",
    "    \"\"\"Extract metadata from all TML files for mapping purposes.\"\"\"\n",
    "    print(\"--- Setting up mapping and failure log tables ---\")\n",
    "    setup_mapping_table()\n",
    "    setup_failure_log_table()\n",
    "    \n",
    "    # Get TML files\n",
    "    try:\n",
    "        tml_files = [f.path for f in dbutils.fs.ls(TML_INPUT_PATH) \n",
    "                     if f.path.endswith(('.tml', '.yaml', '.json'))]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Cannot list files in '{TML_INPUT_PATH}'. Error: {e}\")\n",
    "        return\n",
    "    \n",
    "    if not tml_files:\n",
    "        print(f\"No TML files found in {TML_INPUT_PATH}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(tml_files)} TML files to process.\")\n",
    "    \n",
    "    metadata_records = []\n",
    "    failure_records = []\n",
    "    \n",
    "    for tml_file_path in tml_files:\n",
    "        filename = Path(tml_file_path).name\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n--- Processing: {filename} ---\")\n",
    "            \n",
    "            # Try to parse TML file\n",
    "            try:\n",
    "                tml_data = parse_tml_file(tml_file_path)\n",
    "            except Exception as parse_error:\n",
    "                print(f\"  ERROR: Failed to parse TML file - {parse_error}\")\n",
    "                failure_records.append({\n",
    "                    'tml_file': filename,\n",
    "                    'error_type': 'PARSE_ERROR',\n",
    "                    'error_message': str(parse_error)[:1000],\n",
    "                    'failure_timestamp': datetime.now()\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            liveboard = tml_data.get('liveboard')\n",
    "            if not liveboard:\n",
    "                print(f\"  WARNING: No 'liveboard' key found in {filename}\")\n",
    "                failure_records.append({\n",
    "                    'tml_file': filename,\n",
    "                    'error_type': 'INVALID_STRUCTURE',\n",
    "                    'error_message': \"Missing 'liveboard' root key in TML file\",\n",
    "                    'failure_timestamp': datetime.now()\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            visualizations = liveboard.get('visualizations', [])\n",
    "            \n",
    "            if not visualizations:\n",
    "                print(f\"  WARNING: No visualizations found in {filename}\")\n",
    "                failure_records.append({\n",
    "                    'tml_file': filename,\n",
    "                    'error_type': 'NO_VISUALIZATIONS',\n",
    "                    'error_message': \"No visualizations found in liveboard\",\n",
    "                    'failure_timestamp': datetime.now()\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Found {len(visualizations)} visualizations\")\n",
    "            \n",
    "            for viz in visualizations:\n",
    "                try:\n",
    "                    answer = viz.get('answer', {})\n",
    "                    chart = answer.get('chart', {})\n",
    "                    \n",
    "                    viz_id = viz.get('id', 'unknown')\n",
    "                    viz_name = answer.get('name', 'Unnamed')\n",
    "                    \n",
    "                    # Get chart type\n",
    "                    display_mode = answer.get('display_mode', '')\n",
    "                    chart_type = chart.get('type', 'TABLE_MODE' if display_mode == 'TABLE_MODE' else 'UNKNOWN')\n",
    "                    \n",
    "                    # Extract table info\n",
    "                    table_name, table_id = extract_table_info(answer)\n",
    "                    \n",
    "                    # Extract columns\n",
    "                    columns_used = extract_columns_from_answer(answer)\n",
    "                    base_columns = extract_base_columns(columns_used)\n",
    "                    \n",
    "                    # Get search query\n",
    "                    search_query = answer.get('search_query', '')\n",
    "                    \n",
    "                    # Build record with all new columns initialized as empty/null\n",
    "                    record = {\n",
    "                        'tml_file': filename,\n",
    "                        'visualization_id': viz_id,\n",
    "                        'visualization_name': viz_name,\n",
    "                        'chart_type': chart_type,\n",
    "                        'tml_table_name': table_name,\n",
    "                        'tml_table_id': table_id,\n",
    "                        'tml_columns_used': base_columns,\n",
    "                        'databricks_table_name_ToBeFilled': '',\n",
    "                        'databricks_column_mapping_ToBeFilled': '{}',\n",
    "                        'common_dataset_name': None,  # NULL by default - fill only for shared datasets\n",
    "                        'common_sql_query': None,  # NULL by default - fill only for shared datasets\n",
    "                        'common_column_mapping': None,  # NULL by default - fill only for shared datasets\n",
    "                        'search_query': search_query,\n",
    "                        'notes': f\"Extracted {len(base_columns)} unique columns\",\n",
    "                        'extraction_timestamp': datetime.now()\n",
    "                    }\n",
    "                    \n",
    "                    metadata_records.append(record)\n",
    "                    print(f\"  - {viz_name} ({chart_type}): {len(base_columns)} columns from table '{table_name}'\")\n",
    "                \n",
    "                except Exception as viz_error:\n",
    "                    print(f\"  ERROR processing visualization '{viz.get('id', 'unknown')}': {viz_error}\")\n",
    "                    failure_records.append({\n",
    "                        'tml_file': filename,\n",
    "                        'error_type': 'VISUALIZATION_ERROR',\n",
    "                        'error_message': f\"Viz ID: {viz.get('id', 'unknown')} - {str(viz_error)[:900]}\",\n",
    "                        'failure_timestamp': datetime.now()\n",
    "                    })\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing {filename}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            failure_records.append({\n",
    "                'tml_file': filename,\n",
    "                'error_type': 'PROCESSING_ERROR',\n",
    "                'error_message': str(e)[:1000],\n",
    "                'failure_timestamp': datetime.now()\n",
    "            })\n",
    "    \n",
    "    # Save metadata records to table\n",
    "    if metadata_records:\n",
    "        print(f\"\\n--- Saving {len(metadata_records)} metadata records ---\")\n",
    "        df = pd.DataFrame(metadata_records)\n",
    "        df['extraction_timestamp'] = pd.to_datetime(df['extraction_timestamp'])\n",
    "        \n",
    "        # CRITICAL FIX: Explicitly define schema for Spark DataFrame\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"tml_file\", StringType(), True),\n",
    "            StructField(\"visualization_id\", StringType(), True),\n",
    "            StructField(\"visualization_name\", StringType(), True),\n",
    "            StructField(\"chart_type\", StringType(), True),\n",
    "            StructField(\"tml_table_name\", StringType(), True),\n",
    "            StructField(\"tml_table_id\", StringType(), True),\n",
    "            StructField(\"tml_columns_used\", ArrayType(StringType()), True),\n",
    "            StructField(\"databricks_table_name_ToBeFilled\", StringType(), True),\n",
    "            StructField(\"databricks_column_mapping_ToBeFilled\", StringType(), True),\n",
    "            StructField(\"common_dataset_name\", StringType(), True),\n",
    "            StructField(\"common_sql_query\", StringType(), True),\n",
    "            StructField(\"common_column_mapping\", StringType(), True),\n",
    "            StructField(\"search_query\", StringType(), True),\n",
    "            StructField(\"notes\", StringType(), True),\n",
    "            StructField(\"extraction_timestamp\", TimestampType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Convert to Spark DataFrame with explicit schema\n",
    "        spark_df = spark.createDataFrame(df, schema=schema)\n",
    "        spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(MAPPING_TABLE)\n",
    "        \n",
    "        print(f\"Successfully saved metadata to {MAPPING_TABLE}\")\n",
    "        print(\"\\n=== MAPPING OPTIONS ===\")\n",
    "        print(\"\\nOPTION 1: Unique dataset per visualization\")\n",
    "        print(\"  - Fill 'databricks_table_name_ToBeFilled' with your Databricks table\")\n",
    "        print(\"  - Fill 'databricks_column_mapping_ToBeFilled' with JSON column mapping\")\n",
    "        print(\"  - Leave common_* columns as NULL\")\n",
    "        print(\"\\nOPTION 2: Shared dataset across multiple visualizations\")\n",
    "        print(\"  - Fill 'common_dataset_name' with a shared dataset identifier (e.g., 'ds_trips_shared')\")\n",
    "        print(\"  - Fill 'common_sql_query' with the complete SQL query\")\n",
    "        print(\"  - Fill 'common_column_mapping' with JSON column mapping for the shared dataset\")\n",
    "        print(\"  - You can leave databricks_table_name_ToBeFilled empty for shared datasets\")\n",
    "        print(\"\\nFor column mappings, use JSON string format:\")\n",
    "        print('Example: \\'{\"Order Date\": \"order_date\", \"Customer Name\": \"customer_name\"}\\'')\n",
    "    else:\n",
    "        print(\"\\nNo metadata records extracted.\")\n",
    "    \n",
    "    # Save failure records to table\n",
    "    if failure_records:\n",
    "        print(f\"\\n--- Saving {len(failure_records)} failure records ---\")\n",
    "        fail_df = pd.DataFrame(failure_records)\n",
    "        fail_df['failure_timestamp'] = pd.to_datetime(fail_df['failure_timestamp'])\n",
    "        \n",
    "        spark_fail_df = spark.createDataFrame(fail_df)\n",
    "        spark_fail_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(FAILURE_LOG_TABLE)\n",
    "        \n",
    "        print(f\"Failed TML files logged to {FAILURE_LOG_TABLE}\")\n",
    "    else:\n",
    "        print(\"\\nNo failures encountered - all TML files processed successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Execute Extraction\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "extract_tml_metadata()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## View Results\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n--- TML Metadata Mapping Table ---\")\n",
    "print(\"Choose one mapping approach per visualization:\")\n",
    "print(\"1. Unique dataset: Fill databricks_table_name_ToBeFilled + databricks_column_mapping_ToBeFilled\")\n",
    "print(\"2. Shared dataset: Fill common_dataset_name + common_sql_query + common_column_mapping\\n\")\n",
    "\n",
    "try:\n",
    "    df = spark.table(MAPPING_TABLE)\n",
    "    display(df.orderBy(\"tml_file\", \"visualization_name\"))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display table. Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Failed TML Files ---\")\n",
    "try:\n",
    "    fail_df = spark.table(FAILURE_LOG_TABLE)\n",
    "    fail_count = fail_df.count()\n",
    "    if fail_count > 0:\n",
    "        print(f\"Found {fail_count} failed TML files or visualizations:\")\n",
    "        display(fail_df.orderBy(\"failure_timestamp\", ascending=False))\n",
    "    else:\n",
    "        print(\"No failures - all TML files processed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not display failure log. Error: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Summary Statistics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n--- Extraction Summary ---\")\n",
    "\n",
    "try:\n",
    "    summary_query = f\"\"\"\n",
    "    SELECT \n",
    "        tml_file,\n",
    "        COUNT(*) as num_visualizations,\n",
    "        COUNT(DISTINCT tml_table_name) as num_unique_tables,\n",
    "        SUM(SIZE(tml_columns_used)) as total_columns_used\n",
    "    FROM {MAPPING_TABLE}\n",
    "    GROUP BY tml_file\n",
    "    ORDER BY tml_file\n",
    "    \"\"\"\n",
    "    display(spark.sql(summary_query))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display summary. Error: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Update Mapping Examples\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(f\"\"\"\n",
    "--- How to Update the Mapping Table ---\n",
    "\n",
    "OPTION 1: Unique dataset per visualization\n",
    "------------------------------------------\n",
    "UPDATE {MAPPING_TABLE}\n",
    "SET \n",
    "  databricks_table_name_ToBeFilled = 'my_catalog.my_schema.orders_table',\n",
    "  databricks_column_mapping_ToBeFilled = '{{\"Order Date\": \"order_date\", \"Customer Name\": \"customer_name\", \"Total Revenue\": \"total_revenue\"}}'\n",
    "WHERE visualization_id = 'your_viz_id_here'\n",
    "\n",
    "\n",
    "OPTION 2: Shared dataset across multiple visualizations\n",
    "-------------------------------------------------------\n",
    "UPDATE {MAPPING_TABLE}\n",
    "SET \n",
    "  common_dataset_name = 'ds_trips_shared',\n",
    "  common_sql_query = 'SELECT pickup_zip, dropoff_zip, fare_amount, trip_distance FROM samples.nyctaxi.trips WHERE trip_distance > 0',\n",
    "  common_column_mapping = '{{\"pickup_zip\": \"pickup_zip\", \"dropoff_zip\": \"dropoff_zip\", \"fare_amount\": \"fare_amount\"}}'\n",
    "WHERE tml_file = 'NYC_Dashboard.tml' \n",
    "  AND visualization_id IN ('viz_1', 'viz_2', 'viz_3')\n",
    "\n",
    "\n",
    "Query to see what needs mapping:\n",
    "---------------------------------\n",
    "SELECT \n",
    "  tml_file,\n",
    "  visualization_name,\n",
    "  tml_table_name,\n",
    "  tml_columns_used,\n",
    "  databricks_table_name_ToBeFilled,\n",
    "  common_dataset_name\n",
    "FROM {MAPPING_TABLE}\n",
    "WHERE (databricks_table_name_ToBeFilled = '' OR databricks_table_name_ToBeFilled IS NULL)\n",
    "  AND common_dataset_name IS NULL\n",
    "ORDER BY tml_file, visualization_name\n",
    "\n",
    "\n",
    "Query to identify potential shared datasets:\n",
    "--------------------------------------------\n",
    "SELECT \n",
    "  tml_table_name,\n",
    "  COUNT(*) as num_visualizations,\n",
    "  COLLECT_SET(visualization_name) as viz_names\n",
    "FROM {MAPPING_TABLE}\n",
    "GROUP BY tml_table_name\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY num_visualizations DESC\n",
    "\n",
    "Note: All *_column_mapping fields should be JSON strings mapping TML column names to Databricks column names.\n",
    "\"\"\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Next Steps\n",
    "# MAGIC\n",
    "# MAGIC 1. Review the metadata mapping table above\n",
    "# MAGIC 2. Decide which visualizations should share datasets (query provided above can help identify candidates)\n",
    "# MAGIC 3. For shared datasets:\n",
    "# MAGIC    - Set `common_dataset_name` (e.g., 'ds_trips_shared')\n",
    "# MAGIC    - Set `common_sql_query` (full SQL query)\n",
    "# MAGIC    - Set `common_column_mapping` (JSON string)\n",
    "# MAGIC 4. For unique datasets:\n",
    "# MAGIC    - Set `databricks_table_name_ToBeFilled` (catalog.schema.table)\n",
    "# MAGIC    - Set `databricks_column_mapping_ToBeFilled` (JSON string)\n",
    "# MAGIC 5. Check the failure log table for any TML files that couldn't be processed\n",
    "# MAGIC 6. Use this mapping table in your conversion script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8898739-8a92-4054-8bac-cc142271a311",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "filter test"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## TML Metadata Extractor for Table/Column Mapping\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# --- Configuration ---\n",
    "CATALOG = \"ds_training_1\"\n",
    "SCHEMA = \"thoughtspot_inventory_ak\"\n",
    "TML_VOLUME = \"lvdash_files_ak/liveboard\"\n",
    "\n",
    "TML_INPUT_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{TML_VOLUME}/\"\n",
    "MAPPING_TABLE = f\"{CATALOG}.{SCHEMA}.tml_dbx_metadata_mapping\"\n",
    "FAILURE_LOG_TABLE = f\"{CATALOG}.{SCHEMA}.tml_dbx_mapping_failures\"\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Setup Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def setup_failure_log_table():\n",
    "    \"\"\"Create or recreate the failure log table.\"\"\"\n",
    "    parts = FAILURE_LOG_TABLE.split('.')\n",
    "    catalog = parts[0]\n",
    "    schema = parts[1]\n",
    "    table_name = parts[2]\n",
    "    \n",
    "    # Ensure schema exists\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog}`.`{schema}`\")\n",
    "    \n",
    "    # Drop existing table\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS `{catalog}`.`{schema}`.`{table_name}`\")\n",
    "        print(f\"Dropped existing failure log table: {table_name}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Create new failure log table\n",
    "    create_sql = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{catalog}`.`{schema}`.`{table_name}` (\n",
    "            tml_file STRING,\n",
    "            error_type STRING,\n",
    "            error_message STRING,\n",
    "            failure_timestamp TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(create_sql)\n",
    "    print(f\"Created failure log table: {table_name}\")\n",
    "\n",
    "def setup_mapping_table():\n",
    "    \"\"\"Create or recreate the metadata mapping table.\"\"\"\n",
    "    parts = MAPPING_TABLE.split('.')\n",
    "    catalog = parts[0]\n",
    "    schema = parts[1]\n",
    "    table_name = parts[2]\n",
    "    \n",
    "    # Ensure schema exists\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog}`.`{schema}`\")\n",
    "    \n",
    "    # Drop existing table\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS `{catalog}`.`{schema}`.`{table_name}`\")\n",
    "        print(f\"Dropped existing table: {table_name}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Create new table with mapping structure\n",
    "    # Note: Columns ending with _ToBeFilled are for user input\n",
    "    create_sql = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{catalog}`.`{schema}`.`{table_name}` (\n",
    "            tml_file STRING,\n",
    "            visualization_id STRING,\n",
    "            visualization_name STRING,\n",
    "            chart_type STRING,\n",
    "            tml_table_name STRING,\n",
    "            tml_table_id STRING,\n",
    "            tml_columns_used ARRAY<STRING>,\n",
    "            databricks_table_name_ToBeFilled STRING COMMENT 'For unique datasets per viz',\n",
    "            databricks_column_mapping_ToBeFilled STRING COMMENT 'For unique datasets per viz - JSON format',\n",
    "            common_dataset_name STRING COMMENT 'Shared dataset name for reuse across visualizations',\n",
    "            common_sql_query STRING COMMENT 'Common SQL query for the shared dataset',\n",
    "            common_column_mapping STRING COMMENT 'JSON mapping of common columns for shared dataset',\n",
    "            search_query STRING,\n",
    "            notes STRING,\n",
    "            extraction_timestamp TIMESTAMP,\n",
    "            filter_column_mapping_ToBeFilled STRING COMMENT 'JSON mapping TML filter names to RAW cols\n",
    "        ) USING DELTA\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(create_sql)\n",
    "    print(f\"Created mapping table: {table_name}\")\n",
    "\n",
    "def parse_tml_file(file_path):\n",
    "    \"\"\"Parse TML file (YAML or JSON).\"\"\"\n",
    "    content = dbutils.fs.head(file_path, 10 * 1024 * 1024)\n",
    "    try:\n",
    "        return yaml.safe_load(content)\n",
    "    except yaml.YAMLError:\n",
    "        return json.loads(content)\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Metadata Extraction Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def extract_columns_from_answer(answer: Dict) -> List[str]:\n",
    "    \"\"\"Extract all column names used in the answer.\"\"\"\n",
    "    columns = []\n",
    "    \n",
    "    # From answer_columns\n",
    "    for col in answer.get('answer_columns', []):\n",
    "        col_name = col.get('name')\n",
    "        if col_name:\n",
    "            columns.append(col_name)\n",
    "    \n",
    "    # From table ordered columns\n",
    "    table_cols = answer.get('table', {}).get('ordered_column_ids', [])\n",
    "    columns.extend([c for c in table_cols if c and c not in columns])\n",
    "    \n",
    "    return columns\n",
    "\n",
    "def extract_table_info(answer: Dict) -> tuple:\n",
    "    \"\"\"Extract table name and ID from answer.\"\"\"\n",
    "    tables = answer.get('tables', [])\n",
    "    if tables and len(tables) > 0:\n",
    "        first_table = tables[0]\n",
    "        return (\n",
    "            first_table.get('name', ''),\n",
    "            first_table.get('id', '')\n",
    "        )\n",
    "    return ('', '')\n",
    "\n",
    "def clean_field_name(field_name: str) -> str:\n",
    "    \"\"\"Remove aggregate prefixes from field names.\"\"\"\n",
    "    if not field_name:\n",
    "        return \"\"\n",
    "    import re\n",
    "    cleaned = re.sub(r'^(Total |sum\\(|count\\(|avg\\(|min\\(|max\\(|Unique Number of )', \n",
    "                     '', field_name, flags=re.IGNORECASE)\n",
    "    cleaned = re.sub(r'\\)$', '', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def extract_base_columns(columns: List[str]) -> List[str]:\n",
    "    \"\"\"Extract base column names without aggregations.\"\"\"\n",
    "    base_columns = []\n",
    "    for col in columns:\n",
    "        cleaned = clean_field_name(col)\n",
    "        if cleaned and cleaned not in base_columns:\n",
    "            base_columns.append(cleaned)\n",
    "    return base_columns\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Main Extraction Logic\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def extract_tml_metadata():\n",
    "    \"\"\"Extract metadata from all TML files for mapping purposes.\"\"\"\n",
    "    print(\"--- Setting up mapping and failure log tables ---\")\n",
    "    setup_mapping_table() \n",
    "    setup_failure_log_table()\n",
    "    \n",
    "    # Get TML files\n",
    "    try:\n",
    "        tml_files = [f.path for f in dbutils.fs.ls(TML_INPUT_PATH) \n",
    "                     if f.path.endswith(('.tml', '.yaml', '.json'))]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Cannot list files in '{TML_INPUT_PATH}'. Error: {e}\")\n",
    "        return\n",
    "    \n",
    "    if not tml_files:\n",
    "        print(f\"No TML files found in {TML_INPUT_PATH}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(tml_files)} TML files to process.\")\n",
    "    \n",
    "    metadata_records = []\n",
    "    failure_records = []\n",
    "    \n",
    "    for tml_file_path in tml_files:\n",
    "        filename = Path(tml_file_path).name\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n--- Processing: {filename} ---\")\n",
    "            \n",
    "            # Try to parse TML file\n",
    "            try:\n",
    "                tml_data = parse_tml_file(tml_file_path)\n",
    "            except Exception as parse_error:\n",
    "                print(f\"  ERROR: Failed to parse TML file - {parse_error}\")\n",
    "                failure_records.append({\n",
    "                    'tml_file': filename,\n",
    "                    'error_type': 'PARSE_ERROR',\n",
    "                    'error_message': str(parse_error)[:1000],\n",
    "                    'failure_timestamp': datetime.now()\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            liveboard = tml_data.get('liveboard')\n",
    "            if not liveboard:\n",
    "                print(f\"  WARNING: No 'liveboard' key found in {filename}\")\n",
    "                failure_records.append({\n",
    "                    'tml_file': filename,\n",
    "                    'error_type': 'INVALID_STRUCTURE',\n",
    "                    'error_message': \"Missing 'liveboard' root key in TML file\",\n",
    "                    'failure_timestamp': datetime.now()\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            visualizations = liveboard.get('visualizations', [])\n",
    "            \n",
    "            if not visualizations:\n",
    "                print(f\"  WARNING: No visualizations found in {filename}\")\n",
    "                failure_records.append({\n",
    "                    'tml_file': filename,\n",
    "                    'error_type': 'NO_VISUALIZATIONS',\n",
    "                    'error_message': \"No visualizations found in liveboard\",\n",
    "                    'failure_timestamp': datetime.now()\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Found {len(visualizations)} visualizations\")\n",
    "            \n",
    "            for viz in visualizations:\n",
    "                try:\n",
    "                    answer = viz.get('answer', {})\n",
    "                    chart = answer.get('chart', {})\n",
    "                    \n",
    "                    viz_id = viz.get('id', 'unknown')\n",
    "                    viz_name = answer.get('name', 'Unnamed')\n",
    "                    \n",
    "                    # Get chart type\n",
    "                    display_mode = answer.get('display_mode', '')\n",
    "                    chart_type = chart.get('type', 'TABLE_MODE' if display_mode == 'TABLE_MODE' else 'UNKNOWN')\n",
    "                    \n",
    "                    # Extract table info\n",
    "                    table_name, table_id = extract_table_info(answer)\n",
    "                    \n",
    "                    # Extract columns\n",
    "                    columns_used = extract_columns_from_answer(answer)\n",
    "                    base_columns = extract_base_columns(columns_used)\n",
    "                    \n",
    "                    # Get search query\n",
    "                    search_query = answer.get('search_query', '')\n",
    "                    \n",
    "                    # Build record with all new columns initialized as empty/null\n",
    "                    record = {\n",
    "                        'tml_file': filename,\n",
    "                        'visualization_id': viz_id,\n",
    "                        'visualization_name': viz_name,\n",
    "                        'chart_type': chart_type,\n",
    "                        'tml_table_name': table_name,\n",
    "                        'tml_table_id': table_id,\n",
    "                        'tml_columns_used': base_columns,\n",
    "                        'databricks_table_name_ToBeFilled': '',\n",
    "                        'databricks_column_mapping_ToBeFilled': '{}',\n",
    "                        'common_dataset_name': None,  # NULL by default - fill only for shared datasets\n",
    "                        'common_sql_query': None,  # NULL by default - fill only for shared datasets\n",
    "                        'common_column_mapping': None,  # NULL by default - fill only for shared datasets\n",
    "                        'search_query': search_query,\n",
    "                        'notes': f\"Extracted {len(base_columns)} unique columns\",\n",
    "                        'extraction_timestamp': datetime.now()\n",
    "                    }\n",
    "                    \n",
    "                    metadata_records.append(record)\n",
    "                    print(f\"  - {viz_name} ({chart_type}): {len(base_columns)} columns from table '{table_name}'\")\n",
    "                \n",
    "                except Exception as viz_error:\n",
    "                    print(f\"  ERROR processing visualization '{viz.get('id', 'unknown')}': {viz_error}\")\n",
    "                    failure_records.append({\n",
    "                        'tml_file': filename,\n",
    "                        'error_type': 'VISUALIZATION_ERROR',\n",
    "                        'error_message': f\"Viz ID: {viz.get('id', 'unknown')} - {str(viz_error)[:900]}\",\n",
    "                        'failure_timestamp': datetime.now()\n",
    "                    })\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing {filename}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            failure_records.append({\n",
    "                'tml_file': filename,\n",
    "                'error_type': 'PROCESSING_ERROR',\n",
    "                'error_message': str(e)[:1000],\n",
    "                'failure_timestamp': datetime.now()\n",
    "            })\n",
    "    \n",
    "    # Save metadata records to table\n",
    "    if metadata_records:\n",
    "        print(f\"\\n--- Saving {len(metadata_records)} metadata records ---\")\n",
    "        df = pd.DataFrame(metadata_records)\n",
    "        df['extraction_timestamp'] = pd.to_datetime(df['extraction_timestamp'])\n",
    "        \n",
    "        # CRITICAL FIX: Explicitly define schema for Spark DataFrame\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"tml_file\", StringType(), True),\n",
    "            StructField(\"visualization_id\", StringType(), True),\n",
    "            StructField(\"visualization_name\", StringType(), True),\n",
    "            StructField(\"chart_type\", StringType(), True),\n",
    "            StructField(\"tml_table_name\", StringType(), True),\n",
    "            StructField(\"tml_table_id\", StringType(), True),\n",
    "            StructField(\"tml_columns_used\", ArrayType(StringType()), True),\n",
    "            StructField(\"databricks_table_name_ToBeFilled\", StringType(), True),\n",
    "            StructField(\"databricks_column_mapping_ToBeFilled\", StringType(), True),\n",
    "            StructField(\"common_dataset_name\", StringType(), True),\n",
    "            StructField(\"common_sql_query\", StringType(), True),\n",
    "            StructField(\"common_column_mapping\", StringType(), True),\n",
    "            StructField(\"search_query\", StringType(), True),\n",
    "            StructField(\"notes\", StringType(), True),\n",
    "            StructField(\"extraction_timestamp\", TimestampType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Convert to Spark DataFrame with explicit schema\n",
    "        spark_df = spark.createDataFrame(df, schema=schema)\n",
    "        spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(MAPPING_TABLE)\n",
    "        \n",
    "        print(f\"Successfully saved metadata to {MAPPING_TABLE}\")\n",
    "        print(\"\\n=== MAPPING OPTIONS ===\")\n",
    "        print(\"\\nOPTION 1: Unique dataset per visualization\")\n",
    "        print(\"  - Fill 'databricks_table_name_ToBeFilled' with your Databricks table\")\n",
    "        print(\"  - Fill 'databricks_column_mapping_ToBeFilled' with JSON column mapping\")\n",
    "        print(\"  - Leave common_* columns as NULL\")\n",
    "        print(\"\\nOPTION 2: Shared dataset across multiple visualizations\")\n",
    "        print(\"  - Fill 'common_dataset_name' with a shared dataset identifier (e.g., 'ds_trips_shared')\")\n",
    "        print(\"  - Fill 'common_sql_query' with the complete SQL query\")\n",
    "        print(\"  - Fill 'common_column_mapping' with JSON column mapping for the shared dataset\")\n",
    "        print(\"  - You can leave databricks_table_name_ToBeFilled empty for shared datasets\")\n",
    "        print(\"\\nFor column mappings, use JSON string format:\")\n",
    "        print('Example: \\'{\"Order Date\": \"order_date\", \"Customer Name\": \"customer_name\"}\\'')\n",
    "    else:\n",
    "        print(\"\\nNo metadata records extracted.\")\n",
    "    \n",
    "    # Save failure records to table\n",
    "    if failure_records:\n",
    "        print(f\"\\n--- Saving {len(failure_records)} failure records ---\")\n",
    "        fail_df = pd.DataFrame(failure_records)\n",
    "        fail_df['failure_timestamp'] = pd.to_datetime(fail_df['failure_timestamp'])\n",
    "        \n",
    "        spark_fail_df = spark.createDataFrame(fail_df)\n",
    "        spark_fail_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(FAILURE_LOG_TABLE)\n",
    "        \n",
    "        print(f\"Failed TML files logged to {FAILURE_LOG_TABLE}\")\n",
    "    else:\n",
    "        print(\"\\nNo failures encountered - all TML files processed successfully!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Execute Extraction\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "extract_tml_metadata()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## View Results\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n--- TML Metadata Mapping Table ---\")\n",
    "print(\"Choose one mapping approach per visualization:\")\n",
    "print(\"1. Unique dataset: Fill databricks_table_name_ToBeFilled + databricks_column_mapping_ToBeFilled\")\n",
    "print(\"2. Shared dataset: Fill common_dataset_name + common_sql_query + common_column_mapping\\n\")\n",
    "\n",
    "try:\n",
    "    df = spark.table(MAPPING_TABLE)\n",
    "    display(df.orderBy(\"tml_file\", \"visualization_name\"))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display table. Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Failed TML Files ---\")\n",
    "try:\n",
    "    fail_df = spark.table(FAILURE_LOG_TABLE)\n",
    "    fail_count = fail_df.count()\n",
    "    if fail_count > 0:\n",
    "        print(f\"Found {fail_count} failed TML files or visualizations:\")\n",
    "        display(fail_df.orderBy(\"failure_timestamp\", ascending=False))\n",
    "    else:\n",
    "        print(\"No failures - all TML files processed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not display failure log. Error: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Summary Statistics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n--- Extraction Summary ---\")\n",
    "\n",
    "try:\n",
    "    summary_query = f\"\"\"\n",
    "    SELECT \n",
    "        tml_file,\n",
    "        COUNT(*) as num_visualizations,\n",
    "        COUNT(DISTINCT tml_table_name) as num_unique_tables,\n",
    "        SUM(SIZE(tml_columns_used)) as total_columns_used\n",
    "    FROM {MAPPING_TABLE}\n",
    "    GROUP BY tml_file\n",
    "    ORDER BY tml_file\n",
    "    \"\"\"\n",
    "    display(spark.sql(summary_query))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display summary. Error: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Update Mapping Examples\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(f\"\"\"\n",
    "--- How to Update the Mapping Table ---\n",
    "\n",
    "OPTION 1: Unique dataset per visualization\n",
    "------------------------------------------\n",
    "UPDATE {MAPPING_TABLE}\n",
    "SET \n",
    "  databricks_table_name_ToBeFilled = 'my_catalog.my_schema.orders_table',\n",
    "  databricks_column_mapping_ToBeFilled = '{{\"Order Date\": \"order_date\", \"Customer Name\": \"customer_name\", \"Total Revenue\": \"total_revenue\"}}'\n",
    "WHERE visualization_id = 'your_viz_id_here'\n",
    "\n",
    "\n",
    "OPTION 2: Shared dataset across multiple visualizations\n",
    "-------------------------------------------------------\n",
    "UPDATE {MAPPING_TABLE}\n",
    "SET \n",
    "  common_dataset_name = 'ds_trips_shared',\n",
    "  common_sql_query = 'SELECT pickup_zip, dropoff_zip, fare_amount, trip_distance FROM samples.nyctaxi.trips WHERE trip_distance > 0',\n",
    "  common_column_mapping = '{{\"pickup_zip\": \"pickup_zip\", \"dropoff_zip\": \"dropoff_zip\", \"fare_amount\": \"fare_amount\"}}'\n",
    "WHERE tml_file = 'NYC_Dashboard.tml' \n",
    "  AND visualization_id IN ('viz_1', 'viz_2', 'viz_3')\n",
    "\n",
    "\n",
    "Query to see what needs mapping:\n",
    "---------------------------------\n",
    "SELECT \n",
    "  tml_file,\n",
    "  visualization_name,\n",
    "  tml_table_name,\n",
    "  tml_columns_used,\n",
    "  databricks_table_name_ToBeFilled,\n",
    "  common_dataset_name\n",
    "FROM {MAPPING_TABLE}\n",
    "WHERE (databricks_table_name_ToBeFilled = '' OR databricks_table_name_ToBeFilled IS NULL)\n",
    "  AND common_dataset_name IS NULL\n",
    "ORDER BY tml_file, visualization_name\n",
    "\n",
    "\n",
    "Query to identify potential shared datasets:\n",
    "--------------------------------------------\n",
    "SELECT \n",
    "  tml_table_name,\n",
    "  COUNT(*) as num_visualizations,\n",
    "  COLLECT_SET(visualization_name) as viz_names\n",
    "FROM {MAPPING_TABLE}\n",
    "GROUP BY tml_table_name\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY num_visualizations DESC\n",
    "\n",
    "Note: All *_column_mapping fields should be JSON strings mapping TML column names to Databricks column names.\n",
    "\"\"\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Next Steps\n",
    "# MAGIC\n",
    "# MAGIC 1. Review the metadata mapping table above\n",
    "# MAGIC 2. Decide which visualizations should share datasets (query provided above can help identify candidates)\n",
    "# MAGIC 3. For shared datasets:\n",
    "# MAGIC    - Set `common_dataset_name` (e.g., 'ds_trips_shared')\n",
    "# MAGIC    - Set `common_sql_query` (full SQL query)\n",
    "# MAGIC    - Set `common_column_mapping` (JSON string)\n",
    "# MAGIC 4. For unique datasets:\n",
    "# MAGIC    - Set `databricks_table_name_ToBeFilled` (catalog.schema.table)\n",
    "# MAGIC    - Set `databricks_column_mapping_ToBeFilled` (JSON string)\n",
    "# MAGIC 5. Check the failure log table for any TML files that couldn't be processed\n",
    "# MAGIC 6. Use this mapping table in your conversion script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9781ba94-db63-4028-a907-c3e8c1acb770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from ds_training_1.thoughtspot_inventory_ak.tml_dbx_metadata_mapping\n",
    "where tml_file = 'PaaS Tracking Card.liveboard.tml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f75135b-0223-45fc-ab2c-8c3648fa2faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE ds_training_1.thoughtspot_inventory_ak.tml_dbx_metadata_mapping\n",
    "SET\n",
    "  common_dataset_name = 'ds_paas_summary_by_os',\n",
    "  common_sql_query = 'SELECT Month_Date, os_platform, Viewed_PaaS_Tracking_Card, Clicked_Expand FROM ds_training_1.thoughtspot_inventory_ak.paas_tracking_card_summary',\n",
    "  common_column_mapping = '{\n",
    "    \"Month(Date)\": \"Month_Date\",\n",
    "    \"Os Platform\": \"os_platform\",\n",
    "    \"Viewed PaaS Tracking Card\": \"Viewed_PaaS_Tracking_Card\",\n",
    "    \"Clicked Expand\": \"Clicked_Expand\"\n",
    "  }',\n",
    "  databricks_table_name_ToBeFilled = NULL, \n",
    "  databricks_column_mapping_ToBeFilled = NULL \n",
    "WHERE\n",
    "  visualization_id IN ('Viz_1', 'Viz_2'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7e51974-6987-4ef4-908c-5b6a4fa920f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE ds_training_1.thoughtspot_inventory_ak.tml_dbx_metadata_mapping\n",
    "SET\n",
    "  common_dataset_name = 'ds_paas_summary_monthly', \n",
    "  common_sql_query = 'SELECT Month_Date, Clicked_Order_Confirmation, Clicked_Order_Processing, Clicked_Track_Delivery, Clicked_Complete_Setup, Delivered, Onboarded, Support_Cases, Order_Confirmed_Pill, Processing_Pill, Shipped_Pill, Delivered_Pill, Confirmed, Processed, Shipped FROM ds_training_1.thoughtspot_inventory_ak.paas_tracking_card_monthly_summary',\n",
    "  common_column_mapping = '{\n",
    "    \"Month(Date)\": \"Month_Date\",\n",
    "    \"Clicked Order Confirmation\": \"Clicked_Order_Confirmation\",\n",
    "    \"Clicked Order Processing\": \"Clicked_Order_Processing\",\n",
    "    \"Clicked Track Delivery\": \"Clicked_Track_Delivery\",\n",
    "    \"Clicked Complete Setup\": \"Clicked_Complete_Setup\",\n",
    "    \"Order Confirmation\": \"Clicked_Order_Confirmation\", \n",
    "    \"Processing\": \"Clicked_Order_Processing\", \n",
    "    \"Track Delivery\": \"Clicked_Track_Delivery\", \n",
    "    \"Complete Setup\": \"Clicked_Complete_Setup\", \n",
    "    \"Delivered\": \"Delivered\",\n",
    "    \"Onboarded\": \"Onboarded\",\n",
    "    \"Support Cases\": \"Support_Cases\",\n",
    "    \"Order Confirmed - Pill\": \"Order_Confirmed_Pill\",\n",
    "    \"Processing - Pill\": \"Processing_Pill\",\n",
    "    \"Shipped - Pill\": \"Shipped_Pill\",\n",
    "    \"Delivered - Pill\": \"Delivered_Pill\",\n",
    "    \"Order Confirmed\": \"Order_Confirmed_Pill\",\n",
    "    \"Shipped\": \"Shipped_Pill\",\n",
    "    \"Confirmed\": \"Confirmed\",\n",
    "    \"Processed\": \"Processed\"\n",
    "  }',\n",
    "  databricks_table_name_ToBeFilled = NULL,\n",
    "  databricks_column_mapping_ToBeFilled = NULL\n",
    "WHERE\n",
    "  visualization_id IN ('Viz_3', 'Viz_4', 'Viz_5', 'Viz_6');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9061de5-4830-4f17-a1e1-6e77e2d382a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE\n",
    "  ds_training_1.thoughtspot_inventory_ak.tml_dbx_metadata_mapping\n",
    "SET\n",
    "  databricks_table_name_ToBeFilled = 'ds_training_1.thoughtspot_inventory_ak.paas_tracking_card_summary',\n",
    "  databricks_column_mapping_ToBeFilled = '{\"Date\":\"Month_Date\",\"Os Platform\":\"os_platform\",\"Viewed PaaS Tracking Card\":\"Viewed_PaaS_Tracking_Card\"}',\n",
    "  common_dataset_name = NULL\n",
    "WHERE\n",
    "  visualization_id = 'Viz_1'\n",
    "  and tml_file = 'PaaS Tracking Card.liveboard.tml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe109903-1f0b-4652-be0b-8bbb03177747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE\n",
    "  ds_training_1.thoughtspot_inventory_ak.tml_dbx_metadata_mapping\n",
    "SET\n",
    "  databricks_table_name_ToBeFilled = 'ds_training_1.thoughtspot_inventory_ak.paas_tracking_card_summary',\n",
    "  databricks_column_mapping_ToBeFilled = '{\"Date\":\"Month_Date\",\"Os Platform\":\"os_platform\",\"Clicked Expand\":\"Clicked_Expand\"}',\n",
    "  common_dataset_name = NULL\n",
    "WHERE\n",
    "  visualization_id = 'Viz_2'\n",
    "  and tml_file = 'PaaS Tracking Card.liveboard.tml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c6b26ca-66cb-4c27-9594-a1ed4cf84112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE\n",
    "  ds_training_1.thoughtspot_inventory_ak.tml_dbx_metadata_mapping\n",
    "SET\n",
    "  databricks_table_name_ToBeFilled = 'ds_training_1.thoughtspot_inventory_ak.paas_tracking_card_monthly_summary',\n",
    "  databricks_column_mapping_ToBeFilled = '{\"Date\":\"Month_Date\",\"Clicked Complete Setup\":\"Clicked_Complete_Setup\",\"Clicked Order Confirmation\":\"Clicked_Order_Confirmation\",\"Clicked Order Processing\":\"Clicked_Order_Processing\",\"Clicked Track Delivery\":\"Clicked_Track_Delivery\"}',\n",
    "  common_dataset_name = NULL\n",
    "WHERE\n",
    "  visualization_id = 'Viz_3'\n",
    "  and tml_file = 'PaaS Tracking Card.liveboard.tml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bc36a22-1b13-4182-a21f-2f64db72e604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE\n",
    "  ds_training_1.thoughtspot_inventory_ak.tml_dbx_metadata_mapping\n",
    "SET\n",
    "  databricks_table_name_ToBeFilled = 'ds_training_1.thoughtspot_inventory_ak.paas_tracking_card_monthly_summary',\n",
    "  databricks_column_mapping_ToBeFilled = '{\"Date\":\"Month_Date\",\"Delivered\":\"Delivered\",\"Onboarded\":\"Onboarded\",\"Support Cases\":\"Support_Cases\"}',\n",
    "  common_dataset_name = NULL\n",
    "WHERE\n",
    "  visualization_id = 'Viz_4'\n",
    "  and tml_file = 'PaaS Tracking Card.liveboard.tml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c2271d-5bde-49f5-a35f-88eb1fad0d88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE\n",
    "  ds_training_1.thoughtspot_inventory_ak.tml_dbx_metadata_mapping\n",
    "SET\n",
    "  databricks_table_name_ToBeFilled = 'ds_training_1.thoughtspot_inventory_ak.paas_tracking_card_monthly_summary',\n",
    "  databricks_column_mapping_ToBeFilled = '{\"Date\":\"Month_Date\",\"Delivered - Pill\":\"Delivered_Pill\",\"Order Confirmed - Pill\":\"Order_Confirmed_Pill\",\"Processing - Pill\":\"Processing_Pill\",\"Shipped - Pill\":\"Shipped_Pill\"}',\n",
    "  common_dataset_name = NULL\n",
    "WHERE\n",
    "  visualization_id = 'Viz_5'\n",
    "  and tml_file = 'PaaS Tracking Card.liveboard.tml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8135628-0f96-4f3e-97c7-5f14fd65c246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE\n",
    "  ds_training_1.thoughtspot_inventory_ak.tml_dbx_metadata_mapping\n",
    "SET\n",
    "  databricks_table_name_ToBeFilled = 'ds_training_1.thoughtspot_inventory_ak.paas_tracking_card_monthly_summary',\n",
    "  databricks_column_mapping_ToBeFilled = '{\"Confirmed\":\"Confirmed\",\"Delivered\":\"Delivered\",\"Date\":\"Month_Date\",\"Processed\":\"Processed\",\"Shipped\":\"Shipped\"}',\n",
    "  common_dataset_name = NULL\n",
    "WHERE\n",
    "  visualization_id = 'Viz_6'\n",
    "  and tml_file = 'PaaS Tracking Card.liveboard.tml'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5869069174448032,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "TS_DBX_DataMapping",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
