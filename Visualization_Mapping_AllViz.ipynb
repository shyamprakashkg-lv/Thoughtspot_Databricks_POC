{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51d9aae5-dc90-4e00-b058-a4cde446f8bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "CATALOG = \"ds_training_1\"\n",
    "SCHEMA = \"thoughtspot_inventory_ak\"\n",
    "\n",
    "TML_VOLUME = \"tml_files_ak\"\n",
    "LVDASH_VOLUME = \"lvdash_files_ak_out\"\n",
    "\n",
    "TML_INPUT_PATH = f\"/Volumes/ds_training_1/thoughtspot_inventory_ak/lvdash_files_ak/liveboard/All_chart_Liveboard.liveboard.tml\"\n",
    "LVDASH_OUTPUT_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{LVDASH_VOLUME}/\"\n",
    "\n",
    "TRACKER_TABLE = f\"{CATALOG}.{SCHEMA}.tml_conversion_tracker\"\n",
    "SUMMARY_TABLE = f\"{CATALOG}.{SCHEMA}.tml_conversion_summary\"\n",
    "FAILURE_TABLE = f\"{CATALOG}.{SCHEMA}.tml_conversion_failures\"\n",
    "MAPPING_TABLE = f\"{CATALOG}.{SCHEMA}.tml_dbx_metadata_mapping\"\n",
    "\n",
    "# Configuration tables\n",
    "CHART_TYPE_MAPPING_TABLE = f\"{CATALOG}.{SCHEMA}.chart_type_mappings\"\n",
    "WIDGET_SIZE_CONFIG_TABLE = f\"{CATALOG}.{SCHEMA}.widget_size_config\"\n",
    "EXPRESSION_TRANSFORM_TABLE = f\"{CATALOG}.{SCHEMA}.expression_transformations\"\n",
    "SCALE_TYPE_DETECTION_TABLE = f\"{CATALOG}.{SCHEMA}.scale_type_detection\"\n",
    "\n",
    "def validate_configuration():\n",
    "    print(\"--- Validating configuration ---\")\n",
    "    error_found = False\n",
    "    for var_name, var_value in [(\"CATALOG\", CATALOG), (\"SCHEMA\", SCHEMA), (\"TML_VOLUME\", TML_VOLUME), (\"LVDASH_VOLUME\", LVDASH_VOLUME)]:\n",
    "        if \"/\" in var_value or \"\\\\\" in var_value:\n",
    "            print(f\"ERROR: '{var_name}' contains a slash. Must be a single name.\")\n",
    "            error_found = True\n",
    "    \n",
    "    if not TML_INPUT_PATH.startswith(\"/Volumes/\"):\n",
    "        print(f\"WARNING: 'TML_INPUT_PATH' does not look like a Volume path.\")\n",
    "        error_found = True\n",
    "    \n",
    "    if error_found:\n",
    "        raise ValueError(\"Invalid configuration. Please review errors above.\")\n",
    "    else:\n",
    "        print(\"Configuration looks good.\")\n",
    "\n",
    "validate_configuration()\n",
    "\n",
    "def load_chart_type_mappings():\n",
    "    \"\"\"Load chart type mappings from configuration table - REQUIRED\"\"\"\n",
    "    try:\n",
    "        df = spark.table(CHART_TYPE_MAPPING_TABLE).toPandas()\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(f\"Configuration table {CHART_TYPE_MAPPING_TABLE} is empty. Please run the configuration setup notebook first.\")\n",
    "        \n",
    "        mapping = {}\n",
    "        for _, row in df.iterrows():\n",
    "            tml_type = row['tml_chart_type']\n",
    "            widget_type = row['widget_type']\n",
    "            mapping[tml_type] = widget_type if pd.notna(widget_type) else None\n",
    "        \n",
    "        print(f\"✓ Loaded {len(mapping)} chart type mappings from {CHART_TYPE_MAPPING_TABLE}\")\n",
    "        return mapping\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\"\"\n",
    "ERROR: Failed to load required configuration table: {CHART_TYPE_MAPPING_TABLE}\n",
    "\n",
    "Details: {str(e)}\n",
    "\n",
    "SOLUTION: You must run the configuration setup notebook to create the required tables:\n",
    "1. Open the 'Configuration Tables Setup' notebook\n",
    "2. Run all cells to create: chart_type_mappings, widget_size_config, expression_transformations, scale_type_detection\n",
    "3. Then re-run this converter\n",
    "\n",
    "Configuration tables are REQUIRED and no fallback mappings are available.\n",
    "\"\"\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(f\"Required configuration table {CHART_TYPE_MAPPING_TABLE} not found or cannot be loaded. Run configuration setup first.\") from e\n",
    "\n",
    "def load_expression_transformations():\n",
    "    \"\"\"Load expression transformation rules from configuration table - REQUIRED\"\"\"\n",
    "    try:\n",
    "        df = spark.table(EXPRESSION_TRANSFORM_TABLE).toPandas()\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(f\"Configuration table {EXPRESSION_TRANSFORM_TABLE} is empty. Please run the configuration setup notebook first.\")\n",
    "        \n",
    "        transformations = []\n",
    "        for _, row in df.iterrows():\n",
    "            transformations.append({\n",
    "                'pattern': row['tml_pattern'],\n",
    "                'target': row['target_expression']\n",
    "            })\n",
    "        print(f\"✓ Loaded {len(transformations)} expression transformations from {EXPRESSION_TRANSFORM_TABLE}\")\n",
    "        return transformations\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\"\"\n",
    "ERROR: Failed to load required configuration table: {EXPRESSION_TRANSFORM_TABLE}\n",
    "\n",
    "Details: {str(e)}\n",
    "\n",
    "SOLUTION: Run the configuration setup notebook to create required tables.\n",
    "\"\"\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(f\"Required configuration table {EXPRESSION_TRANSFORM_TABLE} not found. Run configuration setup first.\") from e\n",
    "\n",
    "def load_scale_type_rules():\n",
    "    \"\"\"Load scale type detection rules from configuration table - REQUIRED\"\"\"\n",
    "    try:\n",
    "        df = spark.table(SCALE_TYPE_DETECTION_TABLE).toPandas()\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(f\"Configuration table {SCALE_TYPE_DETECTION_TABLE} is empty. Please run the configuration setup notebook first.\")\n",
    "        \n",
    "        rules = []\n",
    "        for _, row in df.iterrows():\n",
    "            rules.append({\n",
    "                'pattern': row['field_pattern'],\n",
    "                'scale_type': row['scale_type']\n",
    "            })\n",
    "        print(f\"✓ Loaded {len(rules)} scale type rules from {SCALE_TYPE_DETECTION_TABLE}\")\n",
    "        return rules\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\"\"\n",
    "ERROR: Failed to load required configuration table: {SCALE_TYPE_DETECTION_TABLE}\n",
    "\n",
    "Details: {str(e)}\n",
    "\n",
    "SOLUTION: Run the configuration setup notebook to create required tables.\n",
    "\"\"\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(f\"Required configuration table {SCALE_TYPE_DETECTION_TABLE} not found. Run configuration setup first.\") from e\n",
    "\n",
    "def load_widget_size_config():\n",
    "    \"\"\"Load widget size configuration from database table - REQUIRED\"\"\"\n",
    "    try:\n",
    "        df = spark.table(WIDGET_SIZE_CONFIG_TABLE).toPandas()\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(f\"Configuration table {WIDGET_SIZE_CONFIG_TABLE} is empty. Please run the configuration setup notebook first.\")\n",
    "        \n",
    "        size_map = {}\n",
    "        for _, row in df.iterrows():\n",
    "            size_map[row['size_category']] = {\n",
    "                'width': row['width'],\n",
    "                'height': row['height']\n",
    "            }\n",
    "        print(f\"✓ Loaded {len(size_map)} widget size configurations from {WIDGET_SIZE_CONFIG_TABLE}\")\n",
    "        return size_map\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\"\"\n",
    "ERROR: Failed to load required configuration table: {WIDGET_SIZE_CONFIG_TABLE}\n",
    "\n",
    "Details: {str(e)}\n",
    "\n",
    "SOLUTION: Run the configuration setup notebook to create required tables.\n",
    "\"\"\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(f\"Required configuration table {WIDGET_SIZE_CONFIG_TABLE} not found. Run configuration setup first.\") from e\n",
    "\n",
    "# Load all configuration data\n",
    "TML_TO_LVDASH_MAPPING = load_chart_type_mappings()\n",
    "WIDGET_SIZE_MAP = load_widget_size_config()\n",
    "EXPRESSION_TRANSFORMATIONS = load_expression_transformations()\n",
    "SCALE_TYPE_RULES = load_scale_type_rules()\n",
    "\n",
    "\n",
    "def load_mapping_data():\n",
    "    try:\n",
    "        mapping_df = spark.table(MAPPING_TABLE).toPandas()\n",
    "        print(f\"Loaded {len(mapping_df)} mappings from {MAPPING_TABLE}\")\n",
    "        \n",
    "        unmapped = mapping_df[\n",
    "            (mapping_df['databricks_table_name_ToBeFilled'].isna()) | \n",
    "            (mapping_df['databricks_table_name_ToBeFilled'] == '')\n",
    "        ]\n",
    "        \n",
    "        if len(unmapped) > 0:\n",
    "            print(f\"\\nWARNING: {len(unmapped)} visualizations are not mapped to Databricks tables:\")\n",
    "            for _, row in unmapped.head(10).iterrows():\n",
    "                print(f\"  - {row['tml_file']}: {row['visualization_name']}\")\n",
    "            if len(unmapped) > 10:\n",
    "                print(f\"  ... and {len(unmapped) - 10} more\")\n",
    "            print(\"\\nThese visualizations will use TML table names directly.\")\n",
    "        \n",
    "        return mapping_df\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not load mapping table: {e}\")\n",
    "        print(\"Conversion will proceed using TML table names directly.\")\n",
    "        return None\n",
    "\n",
    "MAPPING_DATA = load_mapping_data()\n",
    "\n",
    "def setup_environment():\n",
    "    def ensure_schema_exists(catalog, schema):\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog}`.`{schema}`\")\n",
    "    \n",
    "    def ensure_volume_exists(catalog, schema, volume):\n",
    "        try:\n",
    "            spark.sql(f\"DESCRIBE VOLUME `{catalog}`.`{schema}`.`{volume}`\")\n",
    "        except Exception:\n",
    "            print(f\"  Creating volume {volume}...\")\n",
    "            spark.sql(f\"CREATE VOLUME IF NOT EXISTS `{catalog}`.`{schema}`.`{volume}`\")\n",
    "    \n",
    "    def ensure_table_exists(full_table_name, table_type):\n",
    "        parts = full_table_name.split('.')\n",
    "        catalog = parts[0]\n",
    "        schema = parts[1]\n",
    "        table_name = parts[2]\n",
    "        \n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS `{catalog}`.`{schema}`.`{table_name}`\")\n",
    "            print(f\"  Dropped existing table: {table_name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        if table_type == \"tracker\":\n",
    "            create_sql = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE `{catalog}`.`{schema}`.`{table_name}` (\n",
    "                    tml_file STRING, \n",
    "                    widget_name STRING, \n",
    "                    tml_type STRING, \n",
    "                    lvdash_type STRING, \n",
    "                    status STRING, \n",
    "                    available_fields STRING, \n",
    "                    missing_fields STRING, \n",
    "                    unmapped_properties STRING, \n",
    "                    notes STRING, \n",
    "                    conversion_timestamp TIMESTAMP\n",
    "                ) USING DELTA\n",
    "            \"\"\"\n",
    "        elif table_type == \"failure\":\n",
    "            create_sql = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE `{catalog}`.`{schema}`.`{table_name}` (\n",
    "                    tml_file STRING,\n",
    "                    error_type STRING,\n",
    "                    error_message STRING,\n",
    "                    stack_trace STRING,\n",
    "                    failure_timestamp TIMESTAMP\n",
    "                ) USING DELTA\n",
    "            \"\"\"\n",
    "        else:\n",
    "            create_sql = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE `{catalog}`.`{schema}`.`{table_name}` (\n",
    "                    tml_file STRING, \n",
    "                    lvdash_file STRING, \n",
    "                    status STRING, \n",
    "                    num_datasets INT, \n",
    "                    num_pages INT, \n",
    "                    num_widgets INT, \n",
    "                    conversion_timestamp TIMESTAMP\n",
    "                ) USING DELTA\n",
    "            \"\"\"\n",
    "        \n",
    "        spark.sql(create_sql)\n",
    "        print(f\"  Created table: {table_name}\")\n",
    "    \n",
    "    print(\"\\n--- Setting up environment ---\")\n",
    "    ensure_schema_exists(CATALOG, SCHEMA)\n",
    "    ensure_volume_exists(CATALOG, SCHEMA, TML_VOLUME)\n",
    "    ensure_volume_exists(CATALOG, SCHEMA, LVDASH_VOLUME)\n",
    "    ensure_table_exists(TRACKER_TABLE, \"tracker\")\n",
    "    ensure_table_exists(SUMMARY_TABLE, \"summary\")\n",
    "    ensure_table_exists(FAILURE_TABLE, \"failure\")\n",
    "    print(\"--- Environment ready ---\\n\")\n",
    "\n",
    "def generate_unique_id(length=8):\n",
    "    return uuid.uuid4().hex[:length]\n",
    "\n",
    "def parse_tml_file(file_path):\n",
    "    try:\n",
    "        content = dbutils.fs.head(file_path, 10 * 1024 * 1024)\n",
    "        try:\n",
    "            return yaml.safe_load(content)\n",
    "        except yaml.YAMLError:\n",
    "            return json.loads(content)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to parse TML file: {str(e)}\")\n",
    "\n",
    "def extract_colors_from_tml(chart_data):\n",
    "    try:\n",
    "        client_state = chart_data.get('client_state_v2', '{}')\n",
    "        if isinstance(client_state, str):\n",
    "            client_state = json.loads(client_state)\n",
    "        \n",
    "        system_colors = client_state.get('systemSeriesColors', [])\n",
    "        if system_colors:\n",
    "            colors = []\n",
    "            for item in system_colors:\n",
    "                if item.get('color') and item['color'] not in colors:\n",
    "                    colors.append(item['color'])\n",
    "            if colors:\n",
    "                return colors\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return [\"#2E75F0\", \"#06BF7F\", \"#FCC838\", \"#48D1E0\", \"#71A1F4\", \"#8C62F5\"]\n",
    "\n",
    "def infer_scale_type(column_name):\n",
    "    \"\"\"Infer scale type using configuration rules\"\"\"\n",
    "    name = column_name.lower()\n",
    "    \n",
    "    # Use configuration rules if available\n",
    "    if SCALE_TYPE_RULES:\n",
    "        for rule in SCALE_TYPE_RULES:\n",
    "            pattern = rule['pattern'].lower()\n",
    "            # Remove wildcards and check if pattern is in the column name\n",
    "            pattern_parts = pattern.replace('*_', '').replace('_*', '').split(',')\n",
    "            for part in pattern_parts:\n",
    "                part = part.strip()\n",
    "                if part in name:\n",
    "                    return rule['scale_type']\n",
    "    \n",
    "    # Fallback to default logic\n",
    "    if any(t in name for t in ['date', 'time', 'year', 'month', 'day', 'timestamp', 'week']):\n",
    "        return 'temporal'\n",
    "    \n",
    "    if any(t in name for t in ['total', 'sum', 'count', 'avg', 'average', 'min', 'max', 'revenue', 'price', 'quantity', 'amount', 'sales', 'number']):\n",
    "        return 'quantitative'\n",
    "    \n",
    "    return 'categorical'\n",
    "\n",
    "def clean_field_name(field_name):\n",
    "    if not field_name:\n",
    "        return \"\"\n",
    "    cleaned = re.sub(\n",
    "        r'^(Total |sum\\(|count\\(|avg\\(|min\\(|max\\(|Unique Number of )',\n",
    "        '',\n",
    "        field_name,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    cleaned = re.sub(r'\\)$', '', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def sanitize_alias_name(alias_name):\n",
    "  if not alias_name:\n",
    "    return \"\"\n",
    "  # Replace sequences of one or more non-alphanumeric characters with a single underscore\n",
    "  safe_name = re.sub(r'\\W+', '_', alias_name)\n",
    "  # Remove leading/trailing underscores that might result from the substitution\n",
    "  sanitized_alias = safe_name.strip('_')\n",
    "  # Fallback if the name becomes empty after stripping (e.g., input was just '---')\n",
    "  return sanitized_alias if sanitized_alias else \"_\"\n",
    "\n",
    "class ConversionTracker:\n",
    "    def __init__(self):\n",
    "        self.records = []\n",
    "    \n",
    "    def add_record(self, tml_file, widget_name, tml_type, lvdash_type, status, available, missing, unmapped_props, notes=\"\"):\n",
    "        self.records.append({\n",
    "            'tml_file': tml_file, \n",
    "            'widget_name': widget_name, \n",
    "            'tml_type': tml_type,\n",
    "            'lvdash_type': lvdash_type, \n",
    "            'status': status,\n",
    "            'available_fields': ', '.join(available), \n",
    "            'missing_fields': ', '.join(missing),\n",
    "            'unmapped_properties': ', '.join(unmapped_props),\n",
    "            'notes': notes, \n",
    "            'conversion_timestamp': datetime.now()\n",
    "        })\n",
    "\n",
    "class TMLToLVDASHConverter:\n",
    "    def __init__(self, tml_data, tml_filename, tracker, mapping_data=None):\n",
    "        self.tml_data = tml_data\n",
    "        self.tml_filename = tml_filename\n",
    "        self.tracker = tracker\n",
    "        self.mapping_data = mapping_data\n",
    "        \n",
    "    def _get_mapping_for_viz(self, viz_id):\n",
    "        \"\"\"Get mapping for a specific visualization ID, returns Series or None\"\"\"\n",
    "        if self.mapping_data is None or len(self.mapping_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            mapping = self.mapping_data[\n",
    "                (self.mapping_data['tml_file'] == self.tml_filename) &\n",
    "                (self.mapping_data['visualization_id'] == viz_id)\n",
    "            ]\n",
    "            \n",
    "            # Always return a Series (single row) or None\n",
    "            if not mapping.empty:\n",
    "                return mapping.iloc[0]  # Returns Series\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Error getting mapping for viz {viz_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def convert(self):\n",
    "        liveboard = self.tml_data.get('liveboard')\n",
    "        if not liveboard:\n",
    "            raise ValueError(\"TML file missing 'liveboard' root key.\")\n",
    "\n",
    "        visualizations = liveboard.get('visualizations', [])\n",
    "        if not visualizations:\n",
    "            raise ValueError(\"No visualizations found in liveboard.\")\n",
    "        \n",
    "        # Track unique datasets by their name (supports reuse)\n",
    "        datasets_map = {}  # Key: dataset_name, Value: dataset object\n",
    "        dataset_usage = {}  # Key: dataset_name, Value: list of viz names using it\n",
    "        widgets = []\n",
    "\n",
    "        for viz in visualizations:\n",
    "            try:\n",
    "                viz_id = viz.get('id')\n",
    "                viz_name = viz.get('answer', {}).get('name', 'Unknown')\n",
    "                \n",
    "                # Check if this viz should use a common/shared dataset\n",
    "                mapping = self._get_mapping_for_viz(viz_id)\n",
    "                print(\"Check1\", mapping)\n",
    "                \n",
    "                if mapping is not None:\n",
    "                    common_ds = mapping.get('common_dataset_name')\n",
    "                    # Check if common_dataset_name is truly populated\n",
    "                    if pd.notna(common_ds) and common_ds and str(common_ds).strip() != '' and str(common_ds).lower() != 'null':\n",
    "                        # USE SHARED DATASET from mapping table\n",
    "                        dataset_name = common_ds\n",
    "                        # Create the shared dataset only once\n",
    "                        if dataset_name not in datasets_map:\n",
    "                            dataset = self._create_shared_dataset(viz, mapping)\n",
    "                            datasets_map[dataset_name] = dataset\n",
    "                            dataset_usage[dataset_name] = []\n",
    "                            print(f\"  INFO: Created shared dataset '{dataset_name}' for visualization '{viz_name}'\")\n",
    "                        else:\n",
    "                            print(f\"  INFO: Reusing shared dataset '{dataset_name}' for visualization '{viz_name}'\")\n",
    "                        \n",
    "                        dataset_usage[dataset_name].append(viz_name)\n",
    "                    else:\n",
    "                        # CREATE UNIQUE DATASET per visualization\n",
    "                        dataset = self._create_dataset(viz)\n",
    "                        dataset_name = dataset['name']\n",
    "                        datasets_map[dataset_name] = dataset\n",
    "                        dataset_usage[dataset_name] = [viz_name]\n",
    "                        print(f\"  INFO: Created unique dataset '{dataset_name}' for visualization '{viz_name}'\")\n",
    "                else:\n",
    "                    # CREATE UNIQUE DATASET per visualization (no mapping found)\n",
    "                    dataset = self._create_dataset(viz)\n",
    "                    dataset_name = dataset['name']\n",
    "                    datasets_map[dataset_name] = dataset\n",
    "                    dataset_usage[dataset_name] = [viz_name]\n",
    "                    print(f\"  INFO: Created unique dataset '{dataset_name}' for visualization '{viz_name}'\")\n",
    "                        \n",
    "                # Create widget using the appropriate dataset name\n",
    "                widget_data = self._create_widget(viz, dataset_name)\n",
    "                \n",
    "                # CRITICAL: Handle both single widget and multiple widgets\n",
    "                # _create_widget returns either:\n",
    "                #   - A dict (single widget)\n",
    "                #   - A list of dicts (multiple widgets, e.g., counter + trend)\n",
    "                if isinstance(widget_data, list):\n",
    "                    # Multiple widgets - extend the list\n",
    "                    for w in widget_data:\n",
    "                        # Defensive check - ensure each item is a dict\n",
    "                        if isinstance(w, dict):\n",
    "                            widgets.append(w)\n",
    "                        else:\n",
    "                            print(f\"  WARNING: Skipping invalid widget object of type {type(w)}\")\n",
    "                elif isinstance(widget_data, dict):\n",
    "                    # Single widget - append directly\n",
    "                    widgets.append(widget_data)\n",
    "                else:\n",
    "                    print(f\"  WARNING: _create_widget returned unexpected type {type(widget_data)} for viz {viz_id}\")\n",
    "                \n",
    "            except Exception as viz_error:\n",
    "                viz_name = viz.get('answer', {}).get('name', 'Unknown')\n",
    "                print(f\"  WARNING: Failed to convert visualization '{viz_name}': {viz_error}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                self.tracker.add_record(\n",
    "                    self.tml_filename,\n",
    "                    viz_name,\n",
    "                    viz.get('answer', {}).get('chart', {}).get('type', 'UNKNOWN'),\n",
    "                    'ERROR',\n",
    "                    'ERROR',\n",
    "                    [],\n",
    "                    [],\n",
    "                    [],\n",
    "                    f\"Conversion failed: {str(viz_error)[:500]}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "        if not widgets:\n",
    "            raise ValueError(\"No widgets could be converted successfully.\")\n",
    "\n",
    "        # Log dataset reuse statistics\n",
    "        print(f\"\\n--- Dataset Reuse Summary ---\")\n",
    "        for ds_name, viz_list in dataset_usage.items():\n",
    "            if len(viz_list) > 1:\n",
    "                print(f\"  ✓ Shared dataset '{ds_name}' used by {len(viz_list)} visualizations: {', '.join(viz_list[:3])}{'...' if len(viz_list) > 3 else ''}\")\n",
    "        print(f\"Total datasets created: {len(datasets_map)}\")\n",
    "        print(f\"Total widgets created: {len(widgets)}\")\n",
    "        \n",
    "        # DEBUG: Check widget structure\n",
    "        print(f\"\\n--- Widget Structure Check ---\")\n",
    "        for i, w in enumerate(widgets):\n",
    "            if not isinstance(w, dict):\n",
    "                print(f\"  ERROR: Widget at index {i} is type {type(w)}, not dict!\")\n",
    "            elif 'widget' not in w:\n",
    "                print(f\"  WARNING: Widget at index {i} missing 'widget' key\")\n",
    "            else:\n",
    "                print(f\"  ✓ Widget {i}: viz_id={w.get('viz_id')}, type={w.get('widget', {}).get('spec', {}).get('widgetType')}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        return {\n",
    "            \"datasets\": list(datasets_map.values()),\n",
    "            \"pages\": [{\n",
    "                \"name\": generate_unique_id(),\n",
    "                \"displayName\": liveboard.get('name', 'Converted Dashboard'),\n",
    "                \"layout\": self._create_layout(liveboard.get('layout'), widgets),\n",
    "                \"pageType\": \"PAGE_TYPE_CANVAS\"\n",
    "            }]\n",
    "        }\n",
    "\n",
    "    def _translate_search_query_to_sql(self, viz):\n",
    "        answer = viz.get('answer', {})\n",
    "        query = answer.get('search_query', '')\n",
    "        viz_id = viz.get('id')\n",
    "        print(\"Viz_id\", viz_id)\n",
    "        print(\"Query\", query)\n",
    "        print(\"Answer\", answer)\n",
    "\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_table_name_ToBeFilled'):\n",
    "            table_name = mapping['databricks_table_name_ToBeFilled']\n",
    "        else:\n",
    "            table_name = answer.get('tables', [{}])[0].get('name', 'your_source_table')\n",
    "        \n",
    "        column_mapping = {}\n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                column_mapping = {}\n",
    "        \n",
    "        answer_cols = [col.get('name') for col in answer.get('answer_columns', []) if col.get('name')]\n",
    "        \n",
    "        select_clauses = []\n",
    "        group_by_cols = []\n",
    "        \n",
    "        for i, col_name in enumerate(answer_cols, 1):\n",
    "            if not col_name:\n",
    "                continue\n",
    "            \n",
    "            base_field = clean_field_name(col_name)\n",
    "            mapped_field = column_mapping.get(base_field, base_field)\n",
    "            mapped_col_name = column_mapping.get(col_name, col_name)\n",
    "            sanitized_alias = sanitize_alias_name(mapped_col_name)\n",
    "            sanitized_base_alias = sanitize_alias_name(base_field) \n",
    "\n",
    "            print(\"Fields : base mapped mapped_col sanitized_alias\", base_field, \" \", mapped_field, \" \", mapped_col_name, \" \", sanitized_alias)\n",
    "\n",
    "\n",
    "            if re.match(r\"(Day|Week|Month|Year)\\(\", col_name):\n",
    "                match = re.search(r\"(\\w+)\\((.*?)\\)\", col_name)\n",
    "                if match:\n",
    "                    func, field = match.groups()\n",
    "                    db_field = column_mapping.get(field, field)\n",
    "                    select_clauses.append(f\"  DATE_TRUNC('{func.upper()}', {db_field}) AS {sanitized_alias}\")\n",
    "                    group_by_cols.append(str(i))\n",
    "            \n",
    "            elif 'Unique Number of' in col_name or 'unique number of' in col_name.lower():\n",
    "                select_clauses.append(f\"  COUNT(DISTINCT {mapped_field}) AS {sanitized_alias}\")\n",
    "            \n",
    "            elif any(p in col_name for p in ['Total ', 'sum(', 'count(', 'Sum(', 'Count(']):\n",
    "                agg = 'COUNT' if 'count' in col_name.lower() else 'SUM'\n",
    "                select_clauses.append(f\"  {agg}({mapped_field}) AS {sanitized_base_alias}\")\n",
    "            \n",
    "            else:\n",
    "                select_clauses.append(f\"  {mapped_field} AS {sanitized_base_alias}\")\n",
    "                group_by_cols.append(str(i))\n",
    "\n",
    "        if not select_clauses:\n",
    "            return [f\"SELECT '' AS placeholder FROM {table_name}\"]\n",
    "\n",
    "        where_clauses = []\n",
    "        if \". 'this month'\" in query:\n",
    "            where_clauses.append(\"DATE >= DATE_TRUNC('MONTH', CURRENT_TIMESTAMP())\")\n",
    "        if \". 'last 30 days'\" in query:\n",
    "            where_clauses.append(\"DATE >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)\")\n",
    "\n",
    "        order_by = \"\"\n",
    "        limit = \"\"\n",
    "        if 'top 10' in query.lower():\n",
    "            measure_idx = next((i for i, col in enumerate(answer_cols, 1) if any(p in col for p in ['Total', 'Unique', 'sum(', 'count('])), 1)\n",
    "            order_by = f\" ORDER BY {measure_idx} DESC\"\n",
    "            limit = \" LIMIT 10\"\n",
    "\n",
    "        sql_parts = [\"SELECT\", \",\\n\".join(select_clauses), f\" FROM {table_name} \"]\n",
    "        \n",
    "        if where_clauses:\n",
    "            sql_parts.append(f\" WHERE {' AND '.join(where_clauses)} \")\n",
    "        \n",
    "        has_agg = any(agg in ' '.join(select_clauses) for agg in ['SUM(', 'COUNT(', 'AVG('])\n",
    "        if group_by_cols and has_agg:\n",
    "            sql_parts.append(f\" GROUP BY {', '.join(group_by_cols)} \")\n",
    "        \n",
    "        if order_by:\n",
    "            sql_parts.append(order_by)\n",
    "        if limit:\n",
    "            sql_parts.append(limit)\n",
    "        \n",
    "        return '\\n'.join(sql_parts).split('\\n')\n",
    "\n",
    "    def _create_dataset(self, viz):\n",
    "        answer = viz.get('answer', {})\n",
    "        print(\"create dataset inside\", f\"ds_{viz.get('id', generate_unique_id())}\" , \" \", answer.get('name', 'Untitled Dataset'),\" \", self._translate_search_query_to_sql(viz)  )\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"name\": f\"ds_{viz.get('id', generate_unique_id())}\",\n",
    "            \"displayName\": answer.get('name', 'Untitled Dataset'),\n",
    "            \"queryLines\": self._translate_search_query_to_sql(viz)\n",
    "        }\n",
    "\n",
    "    def _extract_fields_from_answer(self, answer, viz_id,widget_type=None):\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        \n",
    "        # Determine which column mapping to use\n",
    "        column_mapping = {}\n",
    "        if mapping is not None:\n",
    "            # PRIORITY 1: Use common_column_mapping if this is a shared dataset\n",
    "            common_dataset = mapping.get('common_dataset_name')\n",
    "            common_mapping = mapping.get('common_column_mapping')\n",
    "            if pd.notna(common_dataset) and pd.notna(common_mapping):\n",
    "                try:\n",
    "                    column_mapping = json.loads(mapping['common_column_mapping'])\n",
    "                    print(f\"  INFO: Using common_column_mapping for viz {viz_id}\")\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    print(f\"  WARNING: Failed to parse common_column_mapping for viz {viz_id}\")\n",
    "            \n",
    "            # PRIORITY 2: Fall back to regular column mapping\n",
    "            if not column_mapping and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "                try:\n",
    "                    column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "                    print(f\"  INFO: Using databricks_column_mapping for viz {viz_id}\")\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    pass\n",
    "        \n",
    "        fields = []\n",
    "        for col in answer.get('answer_columns', []):\n",
    "            col_name = col.get('name')\n",
    "            if not col_name:\n",
    "                continue\n",
    "            \n",
    "            # Use the same logic as SQL generation to determine the alias\n",
    "            base_field = clean_field_name(col_name)\n",
    "            sql_alias = sanitize_alias_name(base_field)\n",
    "            \n",
    "            mapped_field = column_mapping.get(base_field, base_field)\n",
    "            \n",
    "            # USE CONFIGURATION-DRIVEN EXPRESSION TRANSFORMATION\n",
    "            expression = self._apply_expression_transformation(col_name, sql_alias, base_field, column_mapping)\n",
    "            \n",
    "            fields.append({\n",
    "                \"name\": sql_alias,\n",
    "                \"expression\": expression\n",
    "            })\n",
    "            \n",
    "            print(f\"  Field created: name={sql_alias}, expression={expression}\")\n",
    "        \n",
    "        return fields\n",
    "\n",
    "\n",
    "    def _apply_expression_transformation(self, col_name, sql_alias, base_field, column_mapping):\n",
    "        \"\"\"\n",
    "        Apply expression transformation using configuration from EXPRESSION_TRANSFORMATIONS.\n",
    "        \n",
    "        Args:\n",
    "            col_name: Original column name from TML (e.g., \"Day(Event Date)\", \"Total Revenue\")\n",
    "            sql_alias: Sanitized SQL alias (e.g., \"Event_Date\", \"Revenue\")\n",
    "            base_field: Cleaned field name\n",
    "            column_mapping: Dictionary of column mappings\n",
    "            \n",
    "        Returns:\n",
    "            str: Transformed expression (e.g., \"DATE_TRUNC('DAY', Event_Date)\", \"SUM(Revenue)\")\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Try to match against configured expression patterns\n",
    "        for transform in EXPRESSION_TRANSFORMATIONS:\n",
    "            pattern = transform['pattern']\n",
    "            target_expr = transform['target']\n",
    "            \n",
    "            # Handle date functions: Day(field), Month(field), etc.\n",
    "            if pattern.startswith(('Day(', 'Week(', 'Month(', 'Year(')):\n",
    "                func_match = re.match(r\"(Day|Week|Month|Year)\\((.*?)\\)\", col_name, re.IGNORECASE)\n",
    "                if func_match:\n",
    "                    func_name, field_name = func_match.groups()\n",
    "                    # Get the mapped field from column_mapping\n",
    "                    db_field = column_mapping.get(field_name, field_name)\n",
    "                    sanitized_field = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    # Replace 'field' placeholder with actual field name\n",
    "                    # target_expr example: \"DATE_TRUNC('DAY', `field`)\"\n",
    "                    expression = target_expr.replace('field', sanitized_field)\n",
    "                    expression = expression.replace('DAY', func_name.upper())\n",
    "                    expression = expression.replace('WEEK', func_name.upper())\n",
    "                    expression = expression.replace('MONTH', func_name.upper())\n",
    "                    expression = expression.replace('YEAR', func_name.upper())\n",
    "                    \n",
    "                    return expression\n",
    "            \n",
    "            # Handle aggregation functions: sum(field), count(field), etc.\n",
    "            # SKIP THIS - Don't add SUM() wrapper for any fields\n",
    "            # The SQL query already has the aggregation\n",
    "            elif pattern.startswith(('sum(', 'count(', 'avg(', 'min(', 'max(')):\n",
    "                # Just skip aggregation patterns - return field as-is\n",
    "                continue\n",
    "        \n",
    "        # No transformation matched - return as-is (direct field reference)\n",
    "        return f'`{sql_alias}`'\n",
    "        \n",
    "    def _create_shared_dataset(self, viz, mapping):\n",
    "        \"\"\"\n",
    "        Create a shared dataset using common SQL and column mapping from the mapping table.\n",
    "        \n",
    "        Args:\n",
    "            viz: Visualization object (used for fallback display name)\n",
    "            mapping: Mapping row with common_dataset_name, common_sql_query, common_column_mapping\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dataset object with name, displayName, and queryLines\n",
    "        \"\"\"\n",
    "        dataset_name = mapping['common_dataset_name']\n",
    "        \n",
    "        # Get SQL query from mapping table\n",
    "        if mapping.get('common_sql_query'):\n",
    "            sql_query = mapping['common_sql_query']\n",
    "            # Split into lines for queryLines format\n",
    "            query_lines = sql_query.strip().split('\\n')\n",
    "        else:\n",
    "            # Fallback: generate SQL from viz if common_sql_query is empty\n",
    "            print(f\"  WARNING: No common_sql_query found for shared dataset '{dataset_name}', generating from viz\")\n",
    "            query_lines = self._translate_search_query_to_sql(viz)\n",
    "        \n",
    "        # Get display name from mapping or viz\n",
    "        display_name = mapping.get('common_dataset_name', viz.get('answer', {}).get('name', 'Shared Dataset'))\n",
    "        \n",
    "        print(f\"  INFO: Shared dataset '{dataset_name}' created with {len(query_lines)} query lines\")\n",
    "        \n",
    "        return {\n",
    "            \"name\": dataset_name,\n",
    "            \"displayName\": display_name,\n",
    "            \"queryLines\": query_lines\n",
    "        }\n",
    "        \n",
    "    def _create_widget(self, viz, dataset_name):\n",
    "        answer = viz.get('answer', {})\n",
    "        chart = answer.get('chart', {})\n",
    "        \n",
    "        display_mode = answer.get('display_mode', '')\n",
    "        tml_type = chart.get('type', 'TABLE_MODE' if display_mode == 'TABLE_MODE' else 'UNKNOWN')\n",
    "        \n",
    "        lvdash_type = TML_TO_LVDASH_MAPPING.get(tml_type)\n",
    "        \n",
    "        if lvdash_type is None:\n",
    "            lvdash_type = 'table'\n",
    "            status_note = f\"Chart type '{tml_type}' has no direct mapping, using fallback\"\n",
    "        else:\n",
    "            status_note = \"\"\n",
    "        \n",
    "        available = []\n",
    "        missing = []\n",
    "        unmapped_props = []\n",
    "        \n",
    "        spec_builder = getattr(self, f'_get_{lvdash_type}_spec', self._get_table_spec)\n",
    "        \n",
    "        # Get the spec (might be a tuple for KPI with trend)\n",
    "        spec_result = spec_builder(answer, chart, available, missing, unmapped_props, viz.get('id'))\n",
    "        \n",
    "        # Check if result is a tuple (counter + line chart)\n",
    "        if isinstance(spec_result, tuple):\n",
    "            counter_spec, line_spec = spec_result\n",
    "            \n",
    "            # Log the counter widget\n",
    "            self.tracker.add_record(\n",
    "                self.tml_filename, \n",
    "                answer.get('name', 'Unnamed'), \n",
    "                tml_type, \n",
    "                'counter',\n",
    "                'COMPLETE' if not missing and not unmapped_props else 'PARTIAL', \n",
    "                available, \n",
    "                missing,\n",
    "                unmapped_props,\n",
    "                status_note + \" (with trend line)\"\n",
    "            )\n",
    "            \n",
    "            fields = self._extract_fields_from_answer(answer, viz.get('id'))\n",
    "            \n",
    "            # Create counter widget\n",
    "            counter_widget = {\n",
    "                \"viz_id\": viz.get('id'),\n",
    "                \"viz_guid\": viz.get('viz_guid'),\n",
    "                \"widget\": {\n",
    "                    \"name\": f\"widget_{viz.get('id', generate_unique_id())}\",\n",
    "                    \"queries\": [{\n",
    "                        \"name\": \"main_query\",\n",
    "                        \"query\": {\n",
    "                            \"datasetName\": dataset_name,\n",
    "                            \"fields\": fields,\n",
    "                            \"disaggregated\": False\n",
    "                        }\n",
    "                    }],\n",
    "                    \"spec\": counter_spec\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Create line chart widget for trend\n",
    "            line_widget = {\n",
    "                \"viz_id\": f\"{viz.get('id')}_trend\",\n",
    "                \"viz_guid\": viz.get('viz_guid'),\n",
    "                \"is_trend_chart\": True,  # Flag to identify this as a trend chart\n",
    "                \"parent_viz_id\": viz.get('id'),  # Link to parent counter\n",
    "                \"widget\": {\n",
    "                    \"name\": f\"widget_{viz.get('id', generate_unique_id())}_trend\",\n",
    "                    \"queries\": [{\n",
    "                        \"name\": \"main_query\",\n",
    "                        \"query\": {\n",
    "                            \"datasetName\": dataset_name,\n",
    "                            \"fields\": fields,\n",
    "                            \"disaggregated\": False\n",
    "                        }\n",
    "                    }],\n",
    "                    \"spec\": line_spec\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Return LIST of two widgets\n",
    "            return [counter_widget, line_widget]\n",
    "        \n",
    "        # Normal single widget handling\n",
    "        self.tracker.add_record(\n",
    "            self.tml_filename, \n",
    "            answer.get('name', 'Unnamed'), \n",
    "            tml_type, \n",
    "            lvdash_type,\n",
    "            'COMPLETE' if not missing and not unmapped_props else 'PARTIAL', \n",
    "            available, \n",
    "            missing,\n",
    "            unmapped_props,\n",
    "            status_note\n",
    "        )\n",
    "        \n",
    "        fields = self._extract_fields_from_answer(answer, viz.get('id'))\n",
    "        \n",
    "        return {\n",
    "            \"viz_id\": viz.get('id'),\n",
    "            \"viz_guid\": viz.get('viz_guid'),\n",
    "            \"widget\": {\n",
    "                \"name\": f\"widget_{viz.get('id', generate_unique_id())}\",\n",
    "                \"queries\": [{\n",
    "                    \"name\": \"main_query\",\n",
    "                    \"query\": {\n",
    "                        \"datasetName\": dataset_name,\n",
    "                        \"fields\": fields,\n",
    "                        \"disaggregated\": False\n",
    "                    }\n",
    "                }],\n",
    "                \"spec\": spec_result  # Use spec_result instead of spec\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _get_base_frame(self, answer):\n",
    "        return {\n",
    "            \"title\": answer.get('name', 'Untitled'),\n",
    "            \"description\": answer.get('description', ''),\n",
    "            \"showTitle\": True,\n",
    "            \"showDescription\": bool(answer.get('description'))\n",
    "        }\n",
    "\n",
    "    def _get_bar_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"Generates bar/column chart spec, using original TML names for display.\"\"\"\n",
    "\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        colors = extract_colors_from_tml(chart) # Assumes helper exists\n",
    "        mark = {}\n",
    "\n",
    "        tml_type = chart.get('type', '')\n",
    "        is_column = 'COLUMN' in tml_type.upper()\n",
    "        is_stacked = 'STACKED' in tml_type.upper()\n",
    "\n",
    "        print(f\"\\n=== {'COLUMN' if is_column else 'BAR'} CHART DEBUG for '{widget_name}' (viz_id: {viz_id}) ===\")\n",
    "        print(f\"TML Type: {tml_type}, Is Column: {is_column}, Is Stacked: {is_stacked}\")\n",
    "\n",
    "        # --- NEW: Extract Custom Axis Names ---\n",
    "        custom_x_axis_title = None\n",
    "        custom_y_axis_title = None\n",
    "        try:\n",
    "            client_state = json.loads(chart.get('client_state_v2', '{}'))\n",
    "            axis_props = client_state.get('axisProperties', [])\n",
    "            for prop in axis_props:\n",
    "                prop_props = prop.get('properties', {})\n",
    "                if prop_props.get('axisType') == 'X':\n",
    "                    custom_x_axis_title = prop_props.get('name')\n",
    "                elif prop_props.get('axisType') == 'Y':\n",
    "                    custom_y_axis_title = prop_props.get('name')\n",
    "            if custom_x_axis_title: print(f\"  Found custom X-Axis Title: '{custom_x_axis_title}'\")\n",
    "            if custom_y_axis_title: print(f\"  Found custom Y-Axis Title: '{custom_y_axis_title}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Could not parse client_state_v2 for custom axis names. {e}\")\n",
    "        # --- END NEW BLOCK ---\n",
    "\n",
    "        # --- Get Column Mapping ---\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_mapping = {} # Holds the JSON mapping dict if found\n",
    "        if mapping is not None:\n",
    "            # Logic to load either common or specific mapping JSON\n",
    "            common_mapping_str = mapping.get('common_column_mapping')\n",
    "            specific_mapping_str = mapping.get('databricks_column_mapping_ToBeFilled')\n",
    "            mapping_str_to_use = None\n",
    "            if pd.notna(common_mapping_str) and common_mapping_str and common_mapping_str.strip() != '{}':\n",
    "                 mapping_str_to_use = common_mapping_str\n",
    "                 print(\"  Using common_column_mapping for lookups.\")\n",
    "            elif pd.notna(specific_mapping_str) and specific_mapping_str and specific_mapping_str.strip() != '{}':\n",
    "                 mapping_str_to_use = specific_mapping_str\n",
    "                 print(\"  Using databricks_column_mapping_ToBeFilled for lookups.\")\n",
    "            if mapping_str_to_use:\n",
    "                try: \n",
    "                    column_mapping = json.loads(mapping_str_to_use)\n",
    "                    print(f\"  Column mapping loaded: {column_mapping}\")\n",
    "                except (json.JSONDecodeError, TypeError) as e: \n",
    "                    print(f\"  WARNING: Failed to parse column mapping JSON: {e}\")\n",
    "        else: \n",
    "            print(\"  WARNING: No mapping data found for this visualization.\")\n",
    "\n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        print(f\"Answer columns from TML: {[col.get('name') for col in answer_cols]}\")\n",
    "\n",
    "        # --- Determine Axis Roles ---\n",
    "        if is_column: category_axis_key, measure_axis_key = 'x', 'y'\n",
    "        else: category_axis_key, measure_axis_key = 'y', 'x'\n",
    "\n",
    "        tml_category_config = axis_config.get('x', [])\n",
    "        tml_measure_config = axis_config.get('y', [])\n",
    "        tml_color_config = axis_config.get('color', [])\n",
    "\n",
    "        print(f\"Category Axis Key: {category_axis_key}, Measure Axis Key: {measure_axis_key}\")\n",
    "        print(f\"TML Category Config: {tml_category_config}\")\n",
    "        print(f\"TML Measure Config: {tml_measure_config}\")\n",
    "        print(f\"TML Color Config: {tml_color_config}\")\n",
    "\n",
    "        # --- Process Category Axis (Dimension) ---\n",
    "        if tml_category_config:\n",
    "            original_field_cat = tml_category_config[0] # << CAPTURE ORIGINAL NAME FROM TML CONFIG\n",
    "            matching_cat_col = next((col for col in answer_cols if col.get('name') == original_field_cat or clean_field_name(col.get('name')) == clean_field_name(original_field_cat)), None)\n",
    "\n",
    "            if matching_cat_col:\n",
    "                cat_col_name_from_answer = matching_cat_col.get('name') # Use this for type inference\n",
    "                cat_base_field = clean_field_name(cat_col_name_from_answer)\n",
    "                # !! CORRECTED: Look up ORIGINAL TML name in mapping, fallback to sanitizing the BASE name\n",
    "                cat_sql_alias = column_mapping.get(original_field_cat, sanitize_alias_name(cat_base_field))\n",
    "                print(f\"Category axis ({category_axis_key}): TML={original_field_cat}, FoundInAnswer={cat_col_name_from_answer}, MappedSQLAlias={cat_sql_alias}\")\n",
    "\n",
    "                encodings[category_axis_key] = {\n",
    "                    \"fieldName\": cat_sql_alias,                # Use mapped/sanitized name for data binding\n",
    "                    \"displayName\": original_field_cat,         # Use original TML name for display\n",
    "                    \"scale\": {\"type\": infer_scale_type(cat_col_name_from_answer)},\n",
    "                    \"axis\": {\"title\": custom_x_axis_title or original_field_cat} # Use custom or original name\n",
    "                }\n",
    "                available.append(f'{category_axis_key}-axis (category)')\n",
    "            else:\n",
    "                print(f\"Category axis WARNING: No match in answer_cols for TML field '{original_field_cat}'\")\n",
    "                missing.append(f'{category_axis_key}-axis (category)')\n",
    "        else:\n",
    "             print(f\"Category axis WARNING: No TML config found.\")\n",
    "             missing.append(f'{category_axis_key}-axis (category)')\n",
    "\n",
    "        # --- Process Measure Axis (One or More Measures) ---\n",
    "        measure_fields_processed = [] # Holds {\"fieldName\": \"sql_alias\", \"originalName\": \"original_field_measure\"}\n",
    "        original_measure_names = []   # Store original names from TML config for axis title\n",
    "        measure_axis_title = \"Value\"  # Default title\n",
    "\n",
    "        if tml_measure_config:\n",
    "            print(f\"Processing {len(tml_measure_config)} measure field(s)...\")\n",
    "            for original_field_measure in tml_measure_config: # << CAPTURE ORIGINAL NAME FROM TML CONFIG\n",
    "                original_measure_names.append(original_field_measure)\n",
    "                matching_measure_col = next((col for col in answer_cols if col.get('name') == original_field_measure or clean_field_name(col.get('name')) == clean_field_name(original_field_measure)), None)\n",
    "\n",
    "                if matching_measure_col:\n",
    "                    measure_col_name_from_answer = matching_measure_col.get('name')\n",
    "                    measure_base_field = clean_field_name(measure_col_name_from_answer)\n",
    "                    # !! CORRECTED: Look up ORIGINAL TML name in mapping, fallback to sanitizing the BASE name\n",
    "                    measure_sql_alias = column_mapping.get(original_field_measure, sanitize_alias_name(measure_base_field))\n",
    "                    print(f\"  - Measure field: TML={original_field_measure}, FoundInAnswer={measure_col_name_from_answer}, MappedSQLAlias={measure_sql_alias}\")\n",
    "                    measure_fields_processed.append({\"fieldName\": measure_sql_alias, \"originalName\": original_field_measure})\n",
    "                else:\n",
    "                    print(f\"Measure axis WARNING: No match in answer_cols for TML field '{original_field_measure}'\")\n",
    "                    missing.append(f'{measure_axis_key}-axis measure: {original_field_measure}')\n",
    "\n",
    "            # --- Construct Measure Axis Encoding ---\n",
    "            if len(measure_fields_processed) == 1:\n",
    "                measure_info = measure_fields_processed[0]\n",
    "                measure_axis_title = custom_y_axis_title or measure_info[\"originalName\"] # << USE CUSTOM/ORIGINAL NAME\n",
    "                encodings[measure_axis_key] = {\n",
    "                    \"fieldName\": measure_info[\"fieldName\"],    # Use mapped/sanitized alias\n",
    "                    \"displayName\": measure_info[\"originalName\"], # Use original TML name\n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": measure_axis_title}      # Use custom or original name\n",
    "                }\n",
    "                available.append(f'{measure_axis_key}-axis (single measure)')\n",
    "                print(f\"Measure axis ({measure_axis_key}): Single field = {measure_info['fieldName']}\")\n",
    "            elif len(measure_fields_processed) > 1:\n",
    "                measure_axis_title = custom_y_axis_title or \", \".join(original_measure_names[:3]) + (\"...\" if len(original_measure_names) > 3 else \"\") # << USE CUSTOM/ORIGINAL NAME\n",
    "                encodings[measure_axis_key] = {\n",
    "                    \"fields\": [{\"fieldName\": m[\"fieldName\"]} for m in measure_fields_processed], # Use mapped/sanitized aliases\n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": measure_axis_title} # Use custom or combined original TML names\n",
    "                }\n",
    "                available.append(f'{measure_axis_key}-axis ({len(measure_fields_processed)} measures)')\n",
    "                print(f\"Measure axis ({measure_axis_key}): Multiple fields = {[m['fieldName'] for m in measure_fields_processed]}\")\n",
    "            else: \n",
    "                 if not any(m.startswith(f'{measure_axis_key}-axis measure:') for m in missing):\n",
    "                    missing.append(f'{measure_axis_key}-axis (measure)')\n",
    "        else:\n",
    "             print(f\"Measure axis WARNING: No TML config found.\")\n",
    "             missing.append(f'{measure_axis_key}-axis (measure)')\n",
    "\n",
    "        # --- Process Color (Grouping/Stacking Dimension) ---\n",
    "        color_dimension_present = False\n",
    "        if tml_color_config:\n",
    "            original_field_color = tml_color_config[0] # << CAPTURE ORIGINAL NAME FROM TML CONFIG\n",
    "            matching_color_col = next((col for col in answer_cols if col.get('name') == original_field_color or clean_field_name(col.get('name')) == clean_field_name(original_field_color)), None)\n",
    "\n",
    "            if matching_color_col:\n",
    "                color_col_name_from_answer = matching_color_col.get('name')\n",
    "                color_base_field = clean_field_name(color_col_name_from_answer)\n",
    "                \n",
    "                # !! CORRECTED: Look up ORIGINAL TML name in mapping, fallback to sanitizing the BASE name\n",
    "                color_sql_alias = column_mapping.get(original_field_color, sanitize_alias_name(color_base_field))\n",
    "                print(f\"Color axis: TML={original_field_color}, FoundInAnswer={color_col_name_from_answer}, MappedSQLAlias={color_sql_alias}\")\n",
    "\n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": color_sql_alias,        # Use mapped/sanitized alias\n",
    "                    \"displayName\": original_field_color, # Use original TML name\n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                \n",
    "                color_dimension_present = True\n",
    "                available.append('color')\n",
    "            else:\n",
    "                 print(f\"Color axis WARNING: No match in answer_cols for TML field '{original_field_color}'\")\n",
    "                 missing.append(f'color axis: {original_field_color}')\n",
    "        else:\n",
    "             print(\"Color axis: No color dimension specified in TML.\")\n",
    "\n",
    "        # --- Determine Stacking/Grouping in `mark` ---\n",
    "        if is_stacked:\n",
    "            if color_dimension_present: \n",
    "                mark = {\"stackType\": \"stacked\"}\n",
    "                available.append('stacking (by dimension)')\n",
    "                print(\"Mark: Stacking by color dimension\")\n",
    "            elif len(measure_fields_processed) > 1: \n",
    "                mark = {\"stackType\": \"stacked\"}\n",
    "                available.append('stacking (multiple measures)')\n",
    "                print(\"Mark: Stacking by multiple measures\")\n",
    "            else: \n",
    "                print(\"Mark: Single measure, no color dimension - No stacking needed.\")\n",
    "        else: # Not stacked\n",
    "            if color_dimension_present: \n",
    "                mark = {\"stackType\": \"grouped\"}\n",
    "                available.append('grouping (by dimension)')\n",
    "                print(\"Mark: Grouping by color dimension\")\n",
    "\n",
    "        if not mark and not color_dimension_present and colors:\n",
    "             print(f\"Mark: Applying first TML color (fallback): {colors[0]}\")\n",
    "             mark = {\"colors\": colors[:1]}\n",
    "\n",
    "        print(f\"Final encodings: {encodings}\")\n",
    "        print(f\"Final Mark: {mark}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "        return { \"widgetType\": \"bar\", \"version\": 3, \"frame\": self._get_base_frame(answer), \"encodings\": encodings, \"mark\": mark }\n",
    "\n",
    "    def _get_line_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"Generates line chart spec, using original TML names for display.\"\"\"\n",
    "\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        mark = {} \n",
    "        tml_type = chart.get('type', '')\n",
    "        is_stacked_area = 'STACKED_AREA' in tml_type.upper()\n",
    "\n",
    "        print(f\"\\n=== LINE/AREA CHART DEBUG for '{widget_name}' (viz_id: {viz_id}) ===\")\n",
    "        print(f\"TML Type: {tml_type}\")\n",
    "\n",
    "        # --- NEW: Extract Custom Axis Names ---\n",
    "        custom_x_axis_title = None\n",
    "        custom_y_axis_title = None\n",
    "        try:\n",
    "            client_state = json.loads(chart.get('client_state_v2', '{}'))\n",
    "            axis_props = client_state.get('axisProperties', [])\n",
    "            for prop in axis_props:\n",
    "                prop_props = prop.get('properties', {})\n",
    "                if prop_props.get('axisType') == 'X':\n",
    "                    custom_x_axis_title = prop_props.get('name')\n",
    "                elif prop_props.get('axisType') == 'Y':\n",
    "                    custom_y_axis_title = prop_props.get('name')\n",
    "            if custom_x_axis_title: print(f\"  Found custom X-Axis Title: '{custom_x_axis_title}'\")\n",
    "            if custom_y_axis_title: print(f\"  Found custom Y-Axis Title: '{custom_y_axis_title}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Could not parse client_state_v2 for custom axis names. {e}\")\n",
    "        # --- END NEW BLOCK ---\n",
    "\n",
    "        # --- Get Column Mapping ---\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_mapping = {}\n",
    "        if mapping is not None:\n",
    "            common_mapping_str = mapping.get('common_column_mapping')\n",
    "            specific_mapping_str = mapping.get('databricks_column_mapping_ToBeFilled')\n",
    "            mapping_str_to_use = None\n",
    "            if pd.notna(common_mapping_str) and common_mapping_str and common_mapping_str.strip() != '{}':\n",
    "                 mapping_str_to_use = common_mapping_str\n",
    "                 print(\"  Using common_column_mapping for lookups.\")\n",
    "            elif pd.notna(specific_mapping_str) and specific_mapping_str and specific_mapping_str.strip() != '{}':\n",
    "                 mapping_str_to_use = specific_mapping_str\n",
    "                 print(\"  Using databricks_column_mapping_ToBeFilled for lookups.\")\n",
    "            if mapping_str_to_use:\n",
    "                try: column_mapping = json.loads(mapping_str_to_use)\n",
    "                except (json.JSONDecodeError, TypeError) as e: print(f\"  WARNING: Failed to parse column mapping JSON: {e}\")\n",
    "        else: print(\"  WARNING: No mapping data found for this visualization.\")\n",
    "\n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        print(f\"Answer columns from TML: {[col.get('name') for col in answer_cols]}\")\n",
    "\n",
    "        # --- TML Config Extraction ---\n",
    "        tml_x_config = axis_config.get('x', [])\n",
    "        tml_y_config = axis_config.get('y', [])\n",
    "        tml_color_config = axis_config.get('color', [])\n",
    "\n",
    "        print(f\"TML X Config: {tml_x_config}\")\n",
    "        print(f\"TML Y Config: {tml_y_config}\")\n",
    "        print(f\"TML Color Config: {tml_color_config}\")\n",
    "\n",
    "        # --- Process X-axis (Dimension - usually temporal) ---\n",
    "        if tml_x_config:\n",
    "            original_field_x = tml_x_config[0] # << CAPTURE ORIGINAL NAME\n",
    "            matching_x_col = next((col for col in answer_cols if col.get('name') == original_field_x or clean_field_name(col.get('name')) == clean_field_name(original_field_x)), None)\n",
    "\n",
    "            if matching_x_col:\n",
    "                x_col_name_from_answer = matching_x_col.get('name')\n",
    "                x_base_field = clean_field_name(x_col_name_from_answer)\n",
    "                # !! CORRECTED LOGIC: Look up ORIGINAL TML name in mapping, fallback to sanitizing the BASE name\n",
    "                x_sql_alias = column_mapping.get(original_field_x, sanitize_alias_name(x_base_field))\n",
    "                print(f\"X-axis: TML={original_field_x}, FoundInAnswer={x_col_name_from_answer}, MappedSQLAlias={x_sql_alias}\")\n",
    "\n",
    "                encodings['x'] = {\n",
    "                    \"fieldName\": x_sql_alias,\n",
    "                    \"displayName\": original_field_x, # Use original TML name\n",
    "                    \"scale\": {\"type\": infer_scale_type(x_col_name_from_answer)},\n",
    "                    \"axis\": {\"title\": custom_x_axis_title or original_field_x} # << USE CUSTOM/ORIGINAL NAME\n",
    "                }\n",
    "                available.append('x-axis')\n",
    "            else:\n",
    "                print(f\"X-axis WARNING: No match in answer_cols for TML field '{original_field_x}'\")\n",
    "                missing.append('x-axis')\n",
    "        else:\n",
    "            print(f\"X-axis WARNING: No TML config found.\")\n",
    "            missing.append('x-axis')\n",
    "\n",
    "        # --- Process Y-axis (One or More Measures) ---\n",
    "        measure_fields_processed = [] \n",
    "        original_y_names = []         \n",
    "        y_axis_title = \"Value\"        \n",
    "\n",
    "        if tml_y_config:\n",
    "            print(f\"Processing {len(tml_y_config)} Y-axis measure field(s)...\")\n",
    "            for original_field_y in tml_y_config: # << CAPTURE ORIGINAL NAME\n",
    "                original_y_names.append(original_field_y)\n",
    "                matching_y_col = next((col for col in answer_cols if col.get('name') == original_field_y or clean_field_name(col.get('name')) == clean_field_name(original_field_y)), None)\n",
    "\n",
    "                if matching_y_col:\n",
    "                    y_col_name_from_answer = matching_y_col.get('name')\n",
    "                    y_base_field = clean_field_name(y_col_name_from_answer)\n",
    "                    # !! CORRECTED LOGIC: Look up ORIGINAL TML name in mapping, fallback to sanitizing the BASE name\n",
    "                    y_sql_alias = column_mapping.get(original_field_y, sanitize_alias_name(y_base_field))\n",
    "                    print(f\"  - Y-axis measure: TML={original_field_y}, FoundInAnswer={y_col_name_from_answer}, MappedSQLAlias={y_sql_alias}\")\n",
    "                    measure_fields_processed.append({\"fieldName\": y_sql_alias, \"originalName\": original_field_y})\n",
    "                else:\n",
    "                    print(f\"Y-axis WARNING: No match in answer_cols for TML field '{original_field_y}'\")\n",
    "                    missing.append(f'y-axis measure: {original_field_y}')\n",
    "\n",
    "            # --- Construct Y-axis Encoding ---\n",
    "            if len(measure_fields_processed) == 1:\n",
    "                measure_info = measure_fields_processed[0]\n",
    "                y_axis_title = custom_y_axis_title or measure_info[\"originalName\"] # << USE CUSTOM/ORIGINAL NAME\n",
    "                encodings['y'] = {\n",
    "                    \"fieldName\": measure_info[\"fieldName\"],    \n",
    "                    \"displayName\": measure_info[\"originalName\"], \n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": y_axis_title}          \n",
    "                }\n",
    "                available.append('y-axis (single measure)')\n",
    "                print(f\"Y-axis: Single field = {measure_info['fieldName']}\")\n",
    "            elif len(measure_fields_processed) > 1:\n",
    "                y_axis_title = custom_y_axis_title or \", \".join(original_y_names[:3]) + (\"...\" if len(original_y_names) > 3 else \"\") # << USE CUSTOM/ORIGINAL NAME\n",
    "                encodings['y'] = {\n",
    "                    \"fields\": [{\"fieldName\": m[\"fieldName\"]} for m in measure_fields_processed], \n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": y_axis_title} # Use combined original TML names\n",
    "                }\n",
    "                available.append(f'y-axis ({len(measure_fields_processed)} measures)')\n",
    "                print(f\"Y-axis: Multiple fields = {[m['fieldName'] for m in measure_fields_processed]}\")\n",
    "            else: # No valid measures found\n",
    "                 if not any(m.startswith('y-axis measure:') for m in missing):\n",
    "                    missing.append('y-axis (measure)')\n",
    "        else:\n",
    "             print(f\"Y-axis WARNING: No TML config found.\")\n",
    "             missing.append('y-axis (measure)')\n",
    "\n",
    "        # --- Process Color (Dimension for Multiple Lines) ---\n",
    "        color_dimension_present = False\n",
    "        if tml_color_config:\n",
    "            original_field_color = tml_color_config[0] # << CAPTURE ORIGINAL NAME\n",
    "            matching_color_col = next((col for col in answer_cols if col.get('name') == original_field_color or clean_field_name(col.get('name')) == clean_field_name(original_field_color)), None)\n",
    "\n",
    "            if matching_color_col:\n",
    "                color_col_name_from_answer = matching_color_col.get('name')\n",
    "                color_base_field = clean_field_name(color_col_name_from_answer)\n",
    "                # !! CORRECTED LOGIC: Look up ORIGINAL TML name in mapping, fallback to sanitizing the BASE name\n",
    "                color_sql_alias = column_mapping.get(original_field_color, sanitize_alias_name(color_base_field))\n",
    "                print(f\"Color axis: TML={original_field_color}, FoundInAnswer={color_col_name_from_answer}, MappedSQLAlias={color_sql_alias}\")\n",
    "\n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": color_sql_alias,\n",
    "                    \"displayName\": original_field_color, # Use original TML name\n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                color_dimension_present = True\n",
    "                available.append('color (series)')\n",
    "            else:\n",
    "                 print(f\"Color axis WARNING: No match in answer_cols for TML field '{original_field_color}'\")\n",
    "                 missing.append(f'color axis: {original_field_color}')\n",
    "        else:\n",
    "             if len(measure_fields_processed) > 1:\n",
    "                 # This is for Viz 4. Databricks will use the measure names for the legend.\n",
    "                 print(\"Color axis: No TML config, Databricks will use multiple Y-fields for color.\")\n",
    "                 available.append('color (implicit from measures)')\n",
    "             else:\n",
    "                 print(\"Color axis: No color dimension specified in TML.\")\n",
    "        \n",
    "        # Handle Stacked Area\n",
    "        if is_stacked_area:\n",
    "             if color_dimension_present or len(measure_fields_processed) > 1:\n",
    "                 mark = {\"stackType\": \"stacked\"}\n",
    "                 available.append('stacking (area)')\n",
    "                 print(\"Mark: Stacking enabled for area chart\")\n",
    "\n",
    "        print(f\"Final encodings: {encodings}\")\n",
    "        print(f\"Final Mark: {mark}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "        widget_type = \"area\" if \"AREA\" in tml_type.upper() else \"line\"\n",
    "\n",
    "        return {\n",
    "            \"widgetType\": widget_type,\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings,\n",
    "            \"mark\": mark \n",
    "        }\n",
    "\n",
    "    def _create_widget(self, viz, dataset_name):\n",
    "        answer = viz.get('answer', {})\n",
    "        chart = answer.get('chart', {})\n",
    "        \n",
    "        display_mode = answer.get('display_mode', '')\n",
    "        tml_type = chart.get('type', 'TABLE_MODE' if display_mode == 'TABLE_MODE' else 'UNKNOWN')\n",
    "        \n",
    "        lvdash_type = TML_TO_LVDASH_MAPPING.get(tml_type)\n",
    "        \n",
    "        if lvdash_type is None:\n",
    "            lvdash_type = 'table'\n",
    "            status_note = f\"Chart type '{tml_type}' has no direct mapping, using fallback\"\n",
    "        else:\n",
    "            status_note = \"\"\n",
    "        \n",
    "        available = []\n",
    "        missing = []\n",
    "        unmapped_props = []\n",
    "        \n",
    "        # Get spec builder function\n",
    "        spec_builder = getattr(self, f'_get_{lvdash_type}_spec', self._get_table_spec)\n",
    "        \n",
    "        # Call spec builder and check if it returns a tuple (KPI with trend)\n",
    "        spec_result = spec_builder(answer, chart, available, missing, unmapped_props, viz.get('id'))\n",
    "        \n",
    "        # Extract fields once (used by all widgets)\n",
    "        fields = self._extract_fields_from_answer(answer, viz.get('id'))\n",
    "        \n",
    "        # Check if result is a tuple (counter + line chart for KPI with temporal data)\n",
    "        if isinstance(spec_result, tuple) and len(spec_result) == 2:\n",
    "            counter_spec, line_spec = spec_result\n",
    "            \n",
    "            # Log the counter widget\n",
    "            self.tracker.add_record(\n",
    "                self.tml_filename, \n",
    "                answer.get('name', 'Unnamed'), \n",
    "                tml_type, \n",
    "                'counter',\n",
    "                'COMPLETE' if not missing and not unmapped_props else 'PARTIAL', \n",
    "                available, \n",
    "                missing,\n",
    "                unmapped_props,\n",
    "                status_note + \" (with trend line)\"\n",
    "            )\n",
    "            \n",
    "            # Create counter widget\n",
    "            counter_widget = {\n",
    "                \"viz_id\": viz.get('id'),\n",
    "                \"viz_guid\": viz.get('viz_guid'),\n",
    "                \"widget\": {\n",
    "                    \"name\": f\"widget_{viz.get('id', generate_unique_id())}\",\n",
    "                    \"queries\": [{\n",
    "                        \"name\": \"main_query\",\n",
    "                        \"query\": {\n",
    "                            \"datasetName\": dataset_name,\n",
    "                            \"fields\": fields,\n",
    "                            \"disaggregated\": False\n",
    "                        }\n",
    "                    }],\n",
    "                    \"spec\": counter_spec\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Create line chart widget for trend\n",
    "            line_widget = {\n",
    "                \"viz_id\": f\"{viz.get('id')}_trend\",\n",
    "                \"viz_guid\": viz.get('viz_guid'),\n",
    "                \"is_trend_chart\": True,  # Flag to identify this as a trend chart\n",
    "                \"parent_viz_id\": viz.get('id'),  # Link to parent counter\n",
    "                \"widget\": {\n",
    "                    \"name\": f\"widget_{viz.get('id', generate_unique_id())}_trend\",\n",
    "                    \"queries\": [{\n",
    "                        \"name\": \"main_query\",\n",
    "                        \"query\": {\n",
    "                            \"datasetName\": dataset_name,\n",
    "                            \"fields\": fields,\n",
    "                            \"disaggregated\": False\n",
    "                        }\n",
    "                    }],\n",
    "                    \"spec\": line_spec\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Return LIST of two widgets\n",
    "            return [counter_widget, line_widget]\n",
    "        \n",
    "        # Normal single widget handling (spec_result is a dict)\n",
    "        self.tracker.add_record(\n",
    "            self.tml_filename, \n",
    "            answer.get('name', 'Unnamed'), \n",
    "            tml_type, \n",
    "            lvdash_type,\n",
    "            'COMPLETE' if not missing and not unmapped_props else 'PARTIAL', \n",
    "            available, \n",
    "            missing,\n",
    "            unmapped_props,\n",
    "            status_note\n",
    "        )\n",
    "        \n",
    "        # Return single widget (NOT in a list)\n",
    "        return {\n",
    "            \"viz_id\": viz.get('id'),\n",
    "            \"viz_guid\": viz.get('viz_guid'),\n",
    "            \"widget\": {\n",
    "                \"name\": f\"widget_{viz.get('id', generate_unique_id())}\",\n",
    "                \"queries\": [{\n",
    "                    \"name\": \"main_query\",\n",
    "                    \"query\": {\n",
    "                        \"datasetName\": dataset_name,\n",
    "                        \"fields\": fields,\n",
    "                        \"disaggregated\": False\n",
    "                    }\n",
    "                }],\n",
    "                \"spec\": spec_result  # ✅ This is a dict, not a tuple\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _get_pie_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate pie chart specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Pie chart specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0] \n",
    "        encodings = {}\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  # Continue with empty mapping\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Process Y-axis (angle/value) - typically the measure\n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            original_field = axis_config['y'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['angle'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"quantitative\"}\n",
    "                }\n",
    "                available.append('angle')\n",
    "            else:\n",
    "                missing.append('angle')\n",
    "        else:\n",
    "            missing.append('angle')\n",
    "        \n",
    "        # Process X-axis (color/category) - typically the dimension\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            original_field = axis_config['x'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                available.append('color')\n",
    "            else:\n",
    "                missing.append('color')\n",
    "        else:\n",
    "            missing.append('color')\n",
    "        \n",
    "        encodings['label'] = {\"show\": True}\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"pie\", \n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "    def _get_area_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None): \n",
    "        \"\"\"Generates area/stacked area chart spec, using original TML names for display.\"\"\"\n",
    "\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        mark = {} \n",
    "        tml_type = chart.get('type', '')\n",
    "        is_stacked = 'STACKED' in tml_type.upper()\n",
    "\n",
    "        print(f\"\\n=== AREA CHART DEBUG for '{widget_name}' (viz_id: {viz_id}) ===\")\n",
    "        print(f\"TML Type: {tml_type}, Is Stacked: {is_stacked}\")\n",
    "\n",
    "        #Extract Custom Axis Names \n",
    "        custom_x_axis_title = None\n",
    "        custom_y_axis_title = None\n",
    "        try:\n",
    "            client_state = json.loads(chart.get('client_state_v2', '{}'))\n",
    "            axis_props = client_state.get('axisProperties', [])\n",
    "            for prop in axis_props:\n",
    "                prop_props = prop.get('properties', {})\n",
    "                if prop_props.get('axisType') == 'X':\n",
    "                    custom_x_axis_title = prop_props.get('name')\n",
    "                elif prop_props.get('axisType') == 'Y':\n",
    "                    custom_y_axis_title = prop_props.get('name')\n",
    "            if custom_x_axis_title: print(f\"  Found custom X-Axis Title: '{custom_x_axis_title}'\")\n",
    "            if custom_y_axis_title: print(f\"  Found custom Y-Axis Title: '{custom_y_axis_title}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Could not parse client_state_v2 for custom axis names. {e}\")\n",
    "\n",
    "        # Get Column Mapping ---\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_mapping = {}\n",
    "        if mapping is not None:\n",
    "            common_mapping_str = mapping.get('common_column_mapping')\n",
    "            specific_mapping_str = mapping.get('databricks_column_mapping_ToBeFilled')\n",
    "            mapping_str_to_use = None\n",
    "            if pd.notna(common_mapping_str) and common_mapping_str and common_mapping_str.strip() != '{}':\n",
    "                 mapping_str_to_use = common_mapping_str\n",
    "                 print(\"  Using common_column_mapping for lookups.\")\n",
    "            elif pd.notna(specific_mapping_str) and specific_mapping_str and specific_mapping_str.strip() != '{}':\n",
    "                 mapping_str_to_use = specific_mapping_str\n",
    "                 print(\"  Using databricks_column_mapping_ToBeFilled for lookups.\")\n",
    "            if mapping_str_to_use:\n",
    "                try: column_mapping = json.loads(mapping_str_to_use)\n",
    "                except (json.JSONDecodeError, TypeError) as e: print(f\"  WARNING: Failed to parse column mapping JSON: {e}\")\n",
    "        else: print(\"  WARNING: No mapping data found for this visualization.\")\n",
    "\n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        print(f\"Answer columns from TML: {[col.get('name') for col in answer_cols]}\")\n",
    "\n",
    "        #TML Config Extraction\n",
    "        tml_x_config = axis_config.get('x', [])\n",
    "        tml_y_config = axis_config.get('y', [])\n",
    "        tml_color_config = axis_config.get('color', [])\n",
    "\n",
    "        print(f\"TML X Config: {tml_x_config}\")\n",
    "        print(f\"TML Y Config: {tml_y_config}\")\n",
    "        print(f\"TML Color Config: {tml_color_config}\")\n",
    "\n",
    "        #Process X-axis (Dimension)\n",
    "        if tml_x_config:\n",
    "            original_field_x = tml_x_config[0] \n",
    "            matching_x_col = next((col for col in answer_cols if col.get('name') == original_field_x or clean_field_name(col.get('name')) == clean_field_name(original_field_x)), None)\n",
    "\n",
    "            if matching_x_col:\n",
    "                x_col_name_from_answer = matching_x_col.get('name')\n",
    "                x_base_field = clean_field_name(x_col_name_from_answer)\n",
    "                x_sql_alias = column_mapping.get(original_field_x, sanitize_alias_name(x_base_field))\n",
    "                print(f\"X-axis: TML={original_field_x}, FoundInAnswer={x_col_name_from_answer}, MappedSQLAlias={x_sql_alias}\")\n",
    "\n",
    "                encodings['x'] = {\n",
    "                    \"fieldName\": x_sql_alias,\n",
    "                    \"displayName\": original_field_x, \n",
    "                    \"scale\": {\"type\": infer_scale_type(x_col_name_from_answer)},\n",
    "                    \"axis\": {\"title\": custom_x_axis_title or original_field_x} \n",
    "                }\n",
    "                available.append('x-axis')\n",
    "            else:\n",
    "                print(f\"X-axis WARNING: No match in answer_cols for TML field '{original_field_x}'\")\n",
    "                missing.append('x-axis')\n",
    "        else:\n",
    "            print(f\"X-axis WARNING: No TML config found.\")\n",
    "            missing.append('x-axis')\n",
    "\n",
    "        #Process Y-axis (One or More Measures)\n",
    "        measure_fields_processed = [] \n",
    "        original_y_names = []         \n",
    "        y_axis_title = \"Value\"        \n",
    "\n",
    "        if tml_y_config:\n",
    "            print(f\"Processing {len(tml_y_config)} Y-axis measure field(s)...\")\n",
    "            for original_field_y in tml_y_config: \n",
    "                original_y_names.append(original_field_y)\n",
    "                matching_y_col = next((col for col in answer_cols if col.get('name') == original_field_y or clean_field_name(col.get('name')) == clean_field_name(original_field_y)), None)\n",
    "\n",
    "                if matching_y_col:\n",
    "                    y_col_name_from_answer = matching_y_col.get('name')\n",
    "                    y_base_field = clean_field_name(y_col_name_from_answer)\n",
    "                    y_sql_alias = column_mapping.get(original_field_y, sanitize_alias_name(y_base_field))\n",
    "                    print(f\"  - Y-axis measure: TML={original_field_y}, FoundInAnswer={y_col_name_from_answer}, MappedSQLAlias={y_sql_alias}\")\n",
    "                    measure_fields_processed.append({\"fieldName\": y_sql_alias, \"originalName\": original_field_y})\n",
    "                else:\n",
    "                    print(f\"Y-axis WARNING: No match in answer_cols for TML field '{original_field_y}'\")\n",
    "                    missing.append(f'y-axis measure: {original_field_y}')\n",
    "\n",
    "            #Construct Y-axis Encoding\n",
    "            if len(measure_fields_processed) == 1:\n",
    "                measure_info = measure_fields_processed[0]\n",
    "                y_axis_title = custom_y_axis_title or measure_info[\"originalName\"] \n",
    "                encodings['y'] = {\n",
    "                    \"fieldName\": measure_info[\"fieldName\"],    \n",
    "                    \"displayName\": measure_info[\"originalName\"], \n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": y_axis_title}          \n",
    "                }\n",
    "                available.append('y-axis (single measure)')\n",
    "                print(f\"Y-axis: Single field = {measure_info['fieldName']}\")\n",
    "            elif len(measure_fields_processed) > 1:\n",
    "                y_axis_title = custom_y_axis_title or \", \".join(original_y_names[:3]) + (\"...\" if len(original_y_names) > 3 else \"\") \n",
    "                encodings['y'] = {\n",
    "                    \"fields\": [{\"fieldName\": m[\"fieldName\"]} for m in measure_fields_processed], \n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": y_axis_title} \n",
    "                }\n",
    "                available.append(f'y-axis ({len(measure_fields_processed)} measures)')\n",
    "                print(f\"Y-axis: Multiple fields = {[m['fieldName'] for m in measure_fields_processed]}\")\n",
    "            else: \n",
    "                 if not any(m.startswith('y-axis measure:') for m in missing):\n",
    "                    missing.append('y-axis (measure)')\n",
    "        else:\n",
    "             print(f\"Y-axis WARNING: No TML config found.\")\n",
    "             missing.append('y-axis (measure)')\n",
    "\n",
    "        #Process Color (Dimension for Stacking)\n",
    "        color_dimension_present = False\n",
    "        if tml_color_config:\n",
    "            original_field_color = tml_color_config[0] \n",
    "            matching_color_col = next((col for col in answer_cols if col.get('name') == original_field_color or clean_field_name(col.get('name')) == clean_field_name(original_field_color)), None)\n",
    "\n",
    "            if matching_color_col:\n",
    "                color_col_name_from_answer = matching_color_col.get('name')\n",
    "                color_base_field = clean_field_name(color_col_name_from_answer)\n",
    "                color_sql_alias = column_mapping.get(original_field_color, sanitize_alias_name(color_base_field))\n",
    "                print(f\"Color axis: TML={original_field_color}, FoundInAnswer={color_col_name_from_answer}, MappedSQLAlias={color_sql_alias}\")\n",
    "\n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": color_sql_alias,\n",
    "                    \"displayName\": original_field_color, \n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                color_dimension_present = True\n",
    "                available.append('color (series)')\n",
    "            else:\n",
    "                 print(f\"Color axis WARNING: No match in answer_cols for TML field '{original_field_color}'\")\n",
    "                 missing.append(f'color axis: {original_field_color}')\n",
    "        else:\n",
    "             if len(measure_fields_processed) > 1:\n",
    "                 print(\"Color axis: No TML config, Databricks will use multiple Y-fields for color.\")\n",
    "                 available.append('color (implicit from measures)')\n",
    "             else:\n",
    "                 print(\"Color axis: No color dimension specified in TML.\")\n",
    "        \n",
    "        # Handle Stacking\n",
    "        if is_stacked:\n",
    "             if color_dimension_present or len(measure_fields_processed) > 1:\n",
    "                 mark = {\"stackType\": \"stacked\"}\n",
    "                 available.append('stacking (area)')\n",
    "                 print(\"Mark: Stacking enabled for area chart\")\n",
    "\n",
    "        print(f\"Final encodings: {encodings}\")\n",
    "        print(f\"Final Mark: {mark}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "        return {\n",
    "            \"widgetType\": \"area\", # Use 'area' for both AREA and STACKED_AREA\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings,\n",
    "            \"mark\": mark \n",
    "        }\n",
    "\n",
    "    def _get_scatter_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate scatter plot specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Scatter plot specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  # Continue with empty mapping\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Process X-axis\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            original_field = axis_config['x'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['x'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": infer_scale_type(base_field)},\n",
    "                    \"axis\": {\"title\": base_field}\n",
    "                }\n",
    "                available.append('x-axis')\n",
    "            else:\n",
    "                missing.append('x-axis')\n",
    "        else:\n",
    "            missing.append('x-axis')\n",
    "        \n",
    "        # Process Y-axis\n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            original_field = axis_config['y'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['y'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": base_field}\n",
    "                }\n",
    "                available.append('y-axis')\n",
    "            else:\n",
    "                missing.append('y-axis')\n",
    "        else:\n",
    "            missing.append('y-axis')\n",
    "        \n",
    "        # Process Color (optional)\n",
    "        if axis_config.get('color') and len(axis_config['color']) > 0:\n",
    "            original_field = axis_config['color'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                available.append('color')\n",
    "        \n",
    "        # Process Size (optional)\n",
    "        if axis_config.get('size') and len(axis_config['size']) > 0:\n",
    "            original_field = axis_config['size'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['size'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"quantitative\"}\n",
    "                }\n",
    "                available.append('size')\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"scatter\",\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "    def _get_table_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate table specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Table specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  # Continue with empty mapping\n",
    "        \n",
    "        # Get ordered columns from table config or answer columns\n",
    "        ordered_cols = answer.get('table', {}).get('ordered_column_ids', [])\n",
    "        if not ordered_cols:\n",
    "            ordered_cols = [c.get('name') for c in answer.get('answer_columns', []) if c.get('name')]\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        columns = []\n",
    "        \n",
    "        if ordered_cols:\n",
    "            available.append('columns')\n",
    "            for col_name in ordered_cols:\n",
    "                if not col_name:\n",
    "                    continue\n",
    "                \n",
    "                # Find matching answer column for proper type inference\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    answer_col_name = col.get('name')\n",
    "                    if answer_col_name == col_name or clean_field_name(answer_col_name) == clean_field_name(col_name):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    base_field = clean_field_name(matching_col.get('name'))\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    # Infer display type from column type\n",
    "                    col_type = matching_col.get('type', 'string').lower()\n",
    "                    display_as = \"string\"\n",
    "                    if col_type in ['int', 'integer', 'long', 'bigint', 'float', 'double', 'decimal', 'numeric']:\n",
    "                        display_as = \"number\"\n",
    "                    elif col_type in ['date', 'timestamp', 'datetime']:\n",
    "                        display_as = \"datetime\"\n",
    "                    elif col_type in ['boolean', 'bool']:\n",
    "                        display_as = \"boolean\"\n",
    "                    \n",
    "                    columns.append({\n",
    "                        \"fieldName\": sql_alias,\n",
    "                        \"title\": base_field,\n",
    "                        \"visible\": True,\n",
    "                        \"alignContent\": \"left\",\n",
    "                        \"allowHTML\": False,\n",
    "                        \"displayAs\": display_as,\n",
    "                        \"type\": col_type if col_type else \"string\"\n",
    "                    })\n",
    "                else:\n",
    "                    # Fallback if column not found in answer_columns\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    columns.append({\n",
    "                        \"fieldName\": sql_alias,\n",
    "                        \"title\": base_field,\n",
    "                        \"visible\": True,\n",
    "                        \"alignContent\": \"left\",\n",
    "                        \"allowHTML\": False,\n",
    "                        \"displayAs\": \"string\",\n",
    "                        \"type\": \"string\"\n",
    "                    })\n",
    "        else:\n",
    "            missing.append('columns')\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"table\",\n",
    "            \"version\": 1,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": {\"columns\": columns},\n",
    "            \"allowHTMLByDefault\": False,\n",
    "            \"itemsPerPage\": 25\n",
    "        }\n",
    "\n",
    "    def _get_counter_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate counter/KPI specification with encodings matching SQL aliases.\n",
    "        \n",
    "        When temporal data is present, returns a TUPLE of (counter_spec, line_spec)\n",
    "        to create both a KPI counter and a trend line chart below it.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict OR tuple: Counter spec, or (counter_spec, line_spec) if temporal data exists\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  # Continue with empty mapping\n",
    "        \n",
    "        # Get answer columns and axis config\n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        axis_config = chart.get('axis_configs', [{}])[0] if chart.get('axis_configs') else {}\n",
    "        \n",
    "        # Check if we have temporal data (x-axis with date/time field)\n",
    "        has_temporal_field = False\n",
    "        temporal_field_name = None\n",
    "        \n",
    "        # Check x-axis for temporal field\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            x_field = axis_config['x'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == x_field or clean_field_name(col_name) == clean_field_name(x_field):\n",
    "                    # Check if it's a temporal field (contains Month, Day, Year, etc.)\n",
    "                    if any(keyword in col_name for keyword in ['Month(', 'Day(', 'Week(', 'Year(', 'Date']):\n",
    "                        has_temporal_field = True\n",
    "                        temporal_field_name = col_name\n",
    "                        print(f\"  INFO: Detected temporal field '{temporal_field_name}' for KPI trend visualization\")\n",
    "                    break\n",
    "        \n",
    "        # Process primary value field (y-axis or first measure column)\n",
    "        value_field_alias = None\n",
    "        value_field_display = None\n",
    "        \n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            # Get from y-axis if specified\n",
    "            y_field = axis_config['y'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == y_field or clean_field_name(col_name) == clean_field_name(y_field):\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    value_field_alias = sanitize_alias_name(base_field)\n",
    "                    value_field_display = base_field\n",
    "                    break\n",
    "        \n",
    "        # If no value found from y-axis, find first non-temporal column\n",
    "        if not value_field_alias and answer_cols:\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name:\n",
    "                    # Skip temporal columns\n",
    "                    if not any(keyword in col_name for keyword in ['Month(', 'Day(', 'Week(', 'Year(', 'Date']):\n",
    "                        base_field = clean_field_name(col_name)\n",
    "                        value_field_alias = sanitize_alias_name(base_field)\n",
    "                        value_field_display = base_field\n",
    "                        break\n",
    "        \n",
    "        if not value_field_alias:\n",
    "            missing.append('value')\n",
    "            return {\n",
    "                \"widgetType\": \"counter\",\n",
    "                \"version\": 2,\n",
    "                \"frame\": self._get_base_frame(answer),\n",
    "                \"encodings\": {}\n",
    "            }\n",
    "        \n",
    "        # BUILD COUNTER SPEC (always simple, no x-axis)\n",
    "        counter_encodings = {\n",
    "            \"value\": {\n",
    "                \"fieldName\": value_field_alias,\n",
    "                \"displayName\": value_field_display or value_field_alias\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        counter_spec = {\n",
    "            \"widgetType\": \"counter\",\n",
    "            \"version\": 2,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": counter_encodings\n",
    "        }\n",
    "        \n",
    "        available.append('value')\n",
    "        \n",
    "        # If we have temporal data, CREATE SEPARATE LINE CHART for trend\n",
    "        if has_temporal_field and temporal_field_name:\n",
    "            print(f\"  INFO: Creating separate line chart for KPI trend using _get_line_spec\")\n",
    "            \n",
    "            # Create separate lists for line chart tracking\n",
    "            line_available = []\n",
    "            line_missing = []\n",
    "            line_unmapped = []\n",
    "            \n",
    "            # Call the existing line chart spec builder\n",
    "            line_spec = self._get_line_spec(answer, chart, line_available, line_missing, line_unmapped, viz_id)\n",
    "            \n",
    "            # Customize the frame to hide title for trend chart\n",
    "            line_spec['frame']['title'] = f\"{answer.get('name', 'Trend')} - Trend\"\n",
    "            line_spec['frame']['showTitle'] = False  # Hide title for cleaner look\n",
    "            \n",
    "            # Merge tracking info\n",
    "            available.extend(line_available)\n",
    "            missing.extend(line_missing)\n",
    "            unmapped_props.extend(line_unmapped)\n",
    "            \n",
    "            # Return TUPLE: (counter, line chart)\n",
    "            return (counter_spec, line_spec)\n",
    "        \n",
    "        # No temporal data - return simple counter only\n",
    "        return counter_spec\n",
    "\n",
    "    def _get_heatmap_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate heatmap specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Heatmap specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  # Continue with empty mapping\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Process X-axis\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            original_field = axis_config['x'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['x'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": infer_scale_type(base_field)}\n",
    "                }\n",
    "                available.append('x')\n",
    "            else:\n",
    "                missing.append('x')\n",
    "        else:\n",
    "            missing.append('x')\n",
    "        \n",
    "        # Process Y-axis\n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            original_field = axis_config['y'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['y'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": infer_scale_type(base_field)}\n",
    "                }\n",
    "                available.append('y')\n",
    "            else:\n",
    "                missing.append('y')\n",
    "        else:\n",
    "            missing.append('y')\n",
    "        \n",
    "        # Process Color (intensity) - optional but typical for heatmaps\n",
    "        if axis_config.get('color') and len(axis_config['color']) > 0:\n",
    "            original_field = axis_config['color'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"quantitative\"}\n",
    "                }\n",
    "                available.append('color')\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"heatmap\",\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "    def _get_choropleth_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate choropleth (geo map) specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Choropleth specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  # Continue with empty mapping\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Process X-axis (geographic field)\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            original_field = axis_config['x'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['x'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                available.append('geographic field')\n",
    "            else:\n",
    "                missing.append('geographic field')\n",
    "        else:\n",
    "            missing.append('geographic field')\n",
    "        \n",
    "        # Process Y-axis (value field)\n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            original_field = axis_config['y'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['y'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"quantitative\"}\n",
    "                }\n",
    "                available.append('value')\n",
    "            else:\n",
    "                missing.append('value')\n",
    "        else:\n",
    "            missing.append('value')\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"choropleth map\",\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "    def _get_funnel_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate funnel specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Funnel specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  # Continue with empty mapping\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Process X-axis (stage/category field)\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            original_field = axis_config['x'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['x'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                available.append('stage')\n",
    "            else:\n",
    "                missing.append('stage')\n",
    "        else:\n",
    "            missing.append('stage')\n",
    "        \n",
    "        # Process Y-axis (value field)\n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            original_field = axis_config['y'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['y'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"quantitative\"}\n",
    "                }\n",
    "                available.append('value')\n",
    "            else:\n",
    "                missing.append('value')\n",
    "        else:\n",
    "            missing.append('value')\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"funnel\",\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "    def _get_sankey_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate Sankey diagram specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Sankey charts require:\n",
    "        - stages: Array of categorical fields representing the flow nodes\n",
    "        - value: Quantitative field for flow thickness/weight\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Sankey specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        \n",
    "        print(f\"\\n=== SANKEY CHART DEBUG for '{widget_name}' (viz_id: {viz_id}) ===\")\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "                print(f\"Column mapping loaded: {column_mapping}\")\n",
    "            except (json.JSONDecodeError, TypeError) as e:\n",
    "                print(f\"Failed to parse column mapping: {e}\")\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Collect stage fields (the flow nodes)\n",
    "        stages = []\n",
    "        \n",
    "        # Process X-axis fields (typically source and intermediate nodes)\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            for original_field in axis_config['x']:\n",
    "                # Find matching answer column\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    print(f\"Stage field: original={original_field}, col_name={col_name}, base={base_field}, sql_alias={sql_alias}\")\n",
    "                    \n",
    "                    stages.append({\n",
    "                        \"fieldName\": sql_alias\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"WARNING: Could not find matching column for stage field {original_field}\")\n",
    "        \n",
    "        # Process Color axis fields (typically target node)\n",
    "        if axis_config.get('color') and len(axis_config['color']) > 0:\n",
    "            for original_field in axis_config['color']:\n",
    "                # Find matching answer column\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    print(f\"Stage field (from color): original={original_field}, col_name={col_name}, base={base_field}, sql_alias={sql_alias}\")\n",
    "                    \n",
    "                    # Avoid duplicates\n",
    "                    if not any(s['fieldName'] == sql_alias for s in stages):\n",
    "                        stages.append({\n",
    "                            \"fieldName\": sql_alias\n",
    "                        })\n",
    "                else:\n",
    "                    print(f\"WARNING: Could not find matching column for stage field {original_field}\")\n",
    "        \n",
    "        if stages:\n",
    "            encodings['stages'] = stages\n",
    "            available.append(f'stages ({len(stages)})')\n",
    "            print(f\"Added {len(stages)} stage fields\")\n",
    "        else:\n",
    "            missing.append('stages')\n",
    "            print(\"WARNING: No stage fields found for Sankey\")\n",
    "        \n",
    "        # Process Y-axis (value field) - the measure for flow weight\n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            original_field = axis_config['y'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                print(f\"Value field: original={original_field}, col_name={col_name}, base={base_field}, sql_alias={sql_alias}\")\n",
    "                \n",
    "                encodings['value'] = {\n",
    "                    \"fieldName\": sql_alias\n",
    "                }\n",
    "                available.append('value')\n",
    "            else:\n",
    "                print(f\"WARNING: Could not find matching column for value field {original_field}\")\n",
    "                missing.append('value')\n",
    "        else:\n",
    "            missing.append('value')\n",
    "        \n",
    "        print(f\"Final Sankey encodings: stages={[s['fieldName'] for s in encodings.get('stages', [])]}, value={encodings.get('value', {}).get('fieldName')}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"sankey\",\n",
    "            \"version\": 1,  # Changed to version 1 to match your example\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "\n",
    "    def _get_pivot_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate pivot table specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: List to track optional unmapped properties\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Pivot table specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  # Continue with empty mapping\n",
    "        \n",
    "        # Get ordered columns\n",
    "        ordered_cols = answer.get('table', {}).get('ordered_column_ids', [])\n",
    "        if not ordered_cols:\n",
    "            ordered_cols = [c.get('name') for c in answer.get('answer_columns', []) if c.get('name')]\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        axis_config = chart.get('axis_configs', [{}])[0] if chart.get('axis_configs') else {}\n",
    "        \n",
    "        encodings = {}\n",
    "        \n",
    "        # Process Rows (x-axis)\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            rows = []\n",
    "            for original_field in axis_config['x']:\n",
    "                # Find matching answer column\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    rows.append({\"fieldName\": sql_alias, \"displayName\": base_field})\n",
    "                else:\n",
    "                    # Fallback if no match found\n",
    "                    base_field = clean_field_name(original_field)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    rows.append({\"fieldName\": sql_alias, \"displayName\": base_field})\n",
    "            \n",
    "            encodings['rows'] = rows\n",
    "            available.append('rows')\n",
    "        elif ordered_cols:\n",
    "            original_field = ordered_cols[0]\n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                encodings['rows'] = [{\"fieldName\": sql_alias, \"displayName\": base_field}]\n",
    "            else:\n",
    "                base_field = clean_field_name(original_field)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                encodings['rows'] = [{\"fieldName\": sql_alias, \"displayName\": base_field}]\n",
    "            available.append('rows (inferred)')\n",
    "        else:\n",
    "            missing.append('rows')\n",
    "        \n",
    "        # Process Columns (color axis) - optional for pivot\n",
    "        if axis_config.get('color') and len(axis_config['color']) > 0:\n",
    "            columns = []\n",
    "            for original_field in axis_config['color']:\n",
    "                # Find matching answer column\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    columns.append({\"fieldName\": sql_alias, \"displayName\": base_field})\n",
    "                else:\n",
    "                    # Fallback if no match found\n",
    "                    base_field = clean_field_name(original_field)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    columns.append({\"fieldName\": sql_alias, \"displayName\": base_field})\n",
    "            \n",
    "            encodings['columns'] = columns\n",
    "            available.append('columns')\n",
    "        else:\n",
    "            unmapped_props.append('columns (pivot)')\n",
    "        \n",
    "        # Process Values (y-axis) - Convert to cell structure for pivot\n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            fields = []\n",
    "            for original_field in axis_config['y']:\n",
    "                # Find matching answer column\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    # Use the field name directly without wrapping in aggregation\n",
    "                    fields.append({\n",
    "                        \"fieldName\": sql_alias,  # \n",
    "                        \"cellType\": \"text\"\n",
    "                    })\n",
    "                else:\n",
    "                    # Fallback if no match found\n",
    "                    base_field = clean_field_name(original_field)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    fields.append({\n",
    "                        \"fieldName\": sql_alias,  \n",
    "                        \"cellType\": \"text\"\n",
    "                    })\n",
    "            \n",
    "            encodings['cell'] = {\n",
    "                \"type\": \"multi-cell\",\n",
    "                \"fields\": fields\n",
    "            }\n",
    "            available.append('cell')\n",
    "        elif len(ordered_cols) > 1:\n",
    "            fields = []\n",
    "            for original_field in ordered_cols[1:]:\n",
    "                # Find matching answer column\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    fields.append({\n",
    "                        \"fieldName\": sql_alias,  # ✅ Just use sql_alias directly\n",
    "                        \"cellType\": \"text\"\n",
    "                    })\n",
    "                else:\n",
    "                    base_field = clean_field_name(original_field)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    fields.append({\n",
    "                        \"fieldName\": sql_alias, \n",
    "                        \"cellType\": \"text\"\n",
    "                    })\n",
    "            \n",
    "            encodings['cell'] = {\n",
    "                \"type\": \"multi-cell\",\n",
    "                \"fields\": fields\n",
    "            }\n",
    "            available.append('cell (inferred)')\n",
    "        else:\n",
    "            missing.append('cell')\n",
    "        return {\n",
    "        \"widgetType\": \"pivot\",\n",
    "        \"version\": 3,\n",
    "        \"frame\": self._get_base_frame(answer),\n",
    "        \"encodings\": encodings\n",
    "        }\n",
    "    def _get_combo_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate combo chart specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: List to track optional unmapped properties\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Combo chart specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  # Continue with empty mapping\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Process X-axis\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            original_field = axis_config['x'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['x'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": infer_scale_type(base_field)},\n",
    "                    \"axis\": {\"title\": base_field}\n",
    "                }\n",
    "                available.append('x-axis')\n",
    "            else:\n",
    "                missing.append('x-axis')\n",
    "        else:\n",
    "            missing.append('x-axis')\n",
    "        \n",
    "        # Process Y-axis (can contain multiple fields for combo charts)\n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            y_fields = axis_config['y']\n",
    "            transformed_fields = []\n",
    "            \n",
    "            for original_field in y_fields:\n",
    "                # Find matching answer column\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    transformed_fields.append(sql_alias)\n",
    "                else:\n",
    "                    # Fallback if no match found\n",
    "                    base_field = clean_field_name(original_field)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    transformed_fields.append(sql_alias)\n",
    "            \n",
    "            encodings['y'] = {\n",
    "                \"fieldName\": transformed_fields,\n",
    "                \"displayName\": \"Value\",\n",
    "                \"scale\": {\"type\": \"quantitative\"},\n",
    "                \"axis\": {\"title\": \"Value\"}\n",
    "            }\n",
    "            available.append('y-axis')\n",
    "        else:\n",
    "            missing.append('y-axis')\n",
    "        \n",
    "        # Process Color (series) - optional\n",
    "        if axis_config.get('color') and len(axis_config['color']) > 0:\n",
    "            original_field = axis_config['color'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                available.append('color (series)')\n",
    "        \n",
    "        unmapped_props.append('series chart type mapping (combo charts partially supported)')\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"combo\",\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "    def _create_layout(self, layout_data, widgets):\n",
    "        \"\"\"\n",
    "        Create dashboard layout with widget positions using database mappings.\n",
    "        Handles trend charts by placing them directly below their parent counters.\n",
    "        \"\"\"\n",
    "        layout = []\n",
    "        GRID_WIDTH = 6\n",
    "        \n",
    "        # Separate regular widgets from trend charts\n",
    "        regular_widgets = [w for w in widgets if not w.get('is_trend_chart')]\n",
    "        trend_widgets = {w.get('parent_viz_id'): w for w in widgets if w.get('is_trend_chart')}\n",
    "        \n",
    "        tiles = layout_data.get('tiles', []) if layout_data else []\n",
    "        \n",
    "        if tiles:\n",
    "            # Use TML tile positions\n",
    "            x = 0\n",
    "            y = 0\n",
    "            row_height = 0\n",
    "            \n",
    "            for tile in tiles:\n",
    "                viz_id = tile.get('visualization_id')\n",
    "                widget_obj = next((w for w in regular_widgets if w.get('viz_id') == viz_id), None)\n",
    "                \n",
    "                if not widget_obj:\n",
    "                    print(f\"WARNING: Widget not found for viz_id {viz_id}, skipping tile\")\n",
    "                    continue\n",
    "                \n",
    "                if not widget_obj.get('widget'):\n",
    "                    print(f\"WARNING: Widget object missing 'widget' key for viz_id {viz_id}\")\n",
    "                    continue\n",
    "                \n",
    "                # Get size from tile or default\n",
    "                tile_size = tile.get('size')\n",
    "                if not tile_size:\n",
    "                    widget_type = widget_obj.get('widget', {}).get('spec', {}).get('widgetType')\n",
    "                    tile_size = self._get_default_size_for_widget_type(widget_type)\n",
    "                \n",
    "                size = self._get_widget_size_from_db(tile_size)\n",
    "                if not size:\n",
    "                    print(f\"WARNING: No size configuration found for widget {viz_id}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Handle grid wrapping\n",
    "                if x + size['width'] > GRID_WIDTH:\n",
    "                    x = 0\n",
    "                    y += row_height\n",
    "                    row_height = 0\n",
    "                \n",
    "                # Add main widget\n",
    "                layout.append({\n",
    "                    \"widget\": widget_obj['widget'],\n",
    "                    \"position\": {\n",
    "                        \"x\": x, \n",
    "                        \"y\": y, \n",
    "                        \"width\": size['width'], \n",
    "                        \"height\": size['height']\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                current_x = x\n",
    "                current_y = y\n",
    "                current_height = size['height']\n",
    "                \n",
    "                # Check if this widget has a trend chart\n",
    "                if viz_id in trend_widgets:\n",
    "                    trend_widget = trend_widgets[viz_id]\n",
    "                    \n",
    "                    # Get size for trend chart (use LARGE for better visibility)\n",
    "                    trend_size = self._get_widget_size_from_db('LARGE')\n",
    "                    if not trend_size:\n",
    "                        print(f\"WARNING: Could not get size for trend chart of {viz_id}\")\n",
    "                    else:\n",
    "                        # Place trend chart directly below the counter\n",
    "                        trend_y = current_y + current_height\n",
    "                        \n",
    "                        layout.append({\n",
    "                            \"widget\": trend_widget['widget'],\n",
    "                            \"position\": {\n",
    "                                \"x\": current_x,  # Same X position as counter\n",
    "                                \"y\": trend_y,     # Below the counter\n",
    "                                \"width\": trend_size['width'], \n",
    "                                \"height\": trend_size['height']\n",
    "                            }\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"  ✓ Added trend chart for {viz_id} at position (x={current_x}, y={trend_y})\")\n",
    "                        \n",
    "                        # Update row height to account for trend chart\n",
    "                        row_height = max(row_height, current_height + trend_size['height'])\n",
    "                \n",
    "                x += size['width']\n",
    "                row_height = max(row_height, current_height)\n",
    "        \n",
    "        else:\n",
    "            # No tiles - use default layout\n",
    "            x = 0\n",
    "            y = 0\n",
    "            \n",
    "            for widget_obj in regular_widgets:\n",
    "                if not widget_obj or not widget_obj.get('widget'):\n",
    "                    continue\n",
    "                \n",
    "                widget_type = widget_obj.get('widget', {}).get('spec', {}).get('widgetType')\n",
    "                if not widget_type:\n",
    "                    continue\n",
    "                \n",
    "                default_size_category = self._get_default_size_for_widget_type(widget_type)\n",
    "                size = self._get_widget_size_from_db(default_size_category)\n",
    "                if not size:\n",
    "                    continue\n",
    "                \n",
    "                # Handle grid wrapping\n",
    "                if x + size['width'] > GRID_WIDTH:\n",
    "                    x = 0\n",
    "                    y += size['height']\n",
    "                \n",
    "                # Add main widget\n",
    "                layout.append({\n",
    "                    \"widget\": widget_obj['widget'],\n",
    "                    \"position\": {\n",
    "                        \"x\": x, \n",
    "                        \"y\": y, \n",
    "                        \"width\": size['width'], \n",
    "                        \"height\": size['height']\n",
    "                    }\n",
    "                })\n",
    "                \n",
    "                current_x = x\n",
    "                current_y = y\n",
    "                current_height = size['height']\n",
    "                \n",
    "                # Check if this widget has a trend chart\n",
    "                viz_id = widget_obj.get('viz_id')\n",
    "                if viz_id in trend_widgets:\n",
    "                    trend_widget = trend_widgets[viz_id]\n",
    "                    trend_size = self._get_widget_size_from_db('LARGE')\n",
    "                    \n",
    "                    if trend_size:\n",
    "                        trend_y = current_y + current_height\n",
    "                        \n",
    "                        layout.append({\n",
    "                            \"widget\": trend_widget['widget'],\n",
    "                            \"position\": {\n",
    "                                \"x\": current_x,\n",
    "                                \"y\": trend_y,\n",
    "                                \"width\": trend_size['width'],\n",
    "                                \"height\": trend_size['height']\n",
    "                            }\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"  ✓ Added trend chart for {viz_id} at position (x={current_x}, y={trend_y})\")\n",
    "                \n",
    "                x += size['width']\n",
    "        \n",
    "        return layout\n",
    "\n",
    "\n",
    "    def _get_widget_size_from_db(self, size_category):\n",
    "        \"\"\"\n",
    "        Fetch widget dimensions from pre-loaded WIDGET_SIZE_MAP.\n",
    "        \n",
    "        Args:\n",
    "            size_category: Size category (e.g., 'SMALL', 'MEDIUM', 'LARGE')\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with 'width' and 'height' keys, or None if not found\n",
    "        \"\"\"\n",
    "        if not size_category:\n",
    "            print(f\"WARNING: _get_widget_size_from_db called with no size category\")\n",
    "            return None\n",
    "        \n",
    "        # Use the pre-loaded WIDGET_SIZE_MAP from global scope\n",
    "        size_config = WIDGET_SIZE_MAP.get(size_category)\n",
    "        \n",
    "        if size_config:\n",
    "            print(f\"INFO: Found size config for '{size_category}': width={size_config['width']}, height={size_config['height']}\")\n",
    "            return size_config\n",
    "        \n",
    "        print(f\"WARNING: No size configuration found for category '{size_category}' in WIDGET_SIZE_MAP\")\n",
    "        print(f\"Available size categories: {list(WIDGET_SIZE_MAP.keys())}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    def _get_default_size_for_widget_type(self, widget_type):\n",
    "        \"\"\"\n",
    "        Fetch default size category for a widget type from pre-loaded TML_TO_LVDASH_MAPPING.\n",
    "        \n",
    "        Args:\n",
    "            widget_type: Widget type (e.g., 'bar', 'table', 'counter')\n",
    "            \n",
    "        Returns:\n",
    "            str: Size category (e.g., 'MEDIUM'), or None if not found\n",
    "        \"\"\"\n",
    "        normalized_widget_type = widget_type.replace(' ', '_')\n",
    "        if not widget_type:\n",
    "            print(f\"WARNING: _get_default_size_for_widget_type called with no widget type\")\n",
    "            return None\n",
    "        \n",
    "        # Query the chart_type_mappings table for default_size\n",
    "        try:\n",
    "            from pyspark.sql import SparkSession\n",
    "            spark = SparkSession.builder.getOrCreate()\n",
    "            \n",
    "            result = spark.sql(f\"\"\"\n",
    "                SELECT default_size \n",
    "                FROM {CHART_TYPE_MAPPING_TABLE}\n",
    "                WHERE widget_type = '{normalized_widget_type}'\n",
    "                LIMIT 1\n",
    "            \"\"\").collect()\n",
    "            \n",
    "            if result and len(result) > 0:\n",
    "                default_size = result[0]['default_size']\n",
    "                if default_size:\n",
    "                    print(f\"INFO: Found default size '{default_size}' for widget type '{normalized_widget_type}'\")\n",
    "                    return default_size\n",
    "            \n",
    "            print(f\"WARNING: No default size found for widget type '{normalized_widget_type}'\")\n",
    "            print(f\"Hint: Check if '{normalized_widget_type}' exists in {CHART_TYPE_MAPPING_TABLE}\")\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to fetch default size for widget type '{normalized_widget_type}': {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def convert_all_tml_files():\n",
    "    setup_environment()\n",
    "    \n",
    "    # ===== NEW: Load liveboard configuration =====\n",
    "    CONFIG_TABLE = f\"{CATALOG}.{SCHEMA}.liveboard_migration_config\"\n",
    "    \n",
    "    print(\"\\n--- Loading Liveboard Configuration ---\")\n",
    "    try:\n",
    "        config_df = spark.table(CONFIG_TABLE).toPandas()\n",
    "        enabled_liveboards = config_df[config_df['process_flag'] == 'Y']\n",
    "        \n",
    "        if len(enabled_liveboards) == 0:\n",
    "            print(f\"WARNING: No liveboards enabled (process_flag='Y') in {CONFIG_TABLE}\")\n",
    "            print(\"Please enable at least one liveboard:\")\n",
    "            print(f\"UPDATE {CONFIG_TABLE} SET process_flag = 'Y' WHERE name = 'Your Dashboard Name';\")\n",
    "            return\n",
    "        \n",
    "        enabled_names = set(enabled_liveboards['name'].tolist())\n",
    "        enabled_guids = set(enabled_liveboards['guid'].tolist())\n",
    "        \n",
    "        print(f\"Found {len(enabled_liveboards)} enabled liveboards:\")\n",
    "        for _, row in enabled_liveboards.iterrows():\n",
    "            print(f\"  ✓ {row['name']} ({row['guid']})\")\n",
    "        print(\"---\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load configuration table {CONFIG_TABLE}: {e}\")\n",
    "        print(\"Proceeding to convert ALL liveboards without filtering...\")\n",
    "        enabled_names = None\n",
    "        enabled_guids = None\n",
    "    # ===== END NEW SECTION =====\n",
    "    \n",
    "    if MAPPING_DATA is not None and len(MAPPING_DATA) > 0:\n",
    "        total_mappings = len(MAPPING_DATA)\n",
    "        mapped_count = len(MAPPING_DATA[\n",
    "            (MAPPING_DATA['databricks_table_name_ToBeFilled'].notna()) & \n",
    "            (MAPPING_DATA['databricks_table_name_ToBeFilled'] != '')\n",
    "        ])\n",
    "        print(f\"\\n--- Mapping Status ---\")\n",
    "        print(f\"Total visualizations: {total_mappings}\")\n",
    "        print(f\"Mapped to Databricks tables: {mapped_count}\")\n",
    "        print(f\"Unmapped (will use TML table names): {total_mappings - mapped_count}\")\n",
    "        print(\"---\\n\")\n",
    "    \n",
    "    try:\n",
    "        tml_files = [f.path for f in dbutils.fs.ls(TML_INPUT_PATH) if f.path.endswith(('.tml', '.yaml', '.json'))]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Cannot list files in '{TML_INPUT_PATH}'. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    if not tml_files:\n",
    "        print(f\"No TML files found in {TML_INPUT_PATH}\")\n",
    "        return\n",
    "    \n",
    "    # ===== NEW: Filter TML files based on enabled liveboards =====\n",
    "    if enabled_names or enabled_guids:\n",
    "        filtered_files = []\n",
    "        skipped_files = []\n",
    "        \n",
    "        for file_path in tml_files:\n",
    "            filename = Path(file_path).name\n",
    "            \n",
    "            try:\n",
    "                tml_data = parse_tml_file(file_path)\n",
    "                liveboard = tml_data.get('liveboard', {})\n",
    "                lb_name = liveboard.get('name', '')\n",
    "                lb_guid = tml_data.get('guid', '')\n",
    "                \n",
    "                if lb_name in enabled_names or lb_guid in enabled_guids:\n",
    "                    filtered_files.append(file_path)\n",
    "                    print(f\"  ✓ Including: {filename} ({lb_name})\")\n",
    "                else:\n",
    "                    skipped_files.append(filename)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  WARNING: Could not parse {filename} for filtering: {e}\")\n",
    "                filtered_files.append(file_path)\n",
    "        \n",
    "        tml_files = filtered_files\n",
    "        \n",
    "        print(f\"\\n--- File Filtering Results ---\")\n",
    "        print(f\"Total files found: {len(tml_files) + len(skipped_files)}\")\n",
    "        print(f\"Files to process: {len(tml_files)}\")\n",
    "        print(f\"Files skipped: {len(skipped_files)}\")\n",
    "        if skipped_files and len(skipped_files) <= 10:\n",
    "            print(f\"Skipped files: {', '.join(skipped_files)}\")\n",
    "        elif len(skipped_files) > 10:\n",
    "            print(f\"Skipped files: {', '.join(skipped_files[:10])}... and {len(skipped_files)-10} more\")\n",
    "        print(\"---\\n\")\n",
    "    \n",
    "    if not tml_files:\n",
    "        print(\"No enabled TML files to process after filtering.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(tml_files)} enabled liveboard(s)...\\n\")\n",
    "    # ===== END NEW SECTION =====\n",
    "    \n",
    "    tracker = ConversionTracker()\n",
    "    summary_results = []\n",
    "    failure_records = []\n",
    "    \n",
    "    for tml_file_path in tml_files:\n",
    "        filename = Path(tml_file_path).name\n",
    "        status = 'ERROR'\n",
    "        lvdash_data = {}\n",
    "        output_filename = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n--- Processing: {filename} ---\")\n",
    "            \n",
    "            try:\n",
    "                tml_data = parse_tml_file(tml_file_path)\n",
    "            except Exception as parse_error:\n",
    "                raise ValueError(f\"Failed to parse TML file: {str(parse_error)}\")\n",
    "            \n",
    "            converter = TMLToLVDASHConverter(tml_data, filename, tracker, MAPPING_DATA)\n",
    "            lvdash_data = converter.convert()\n",
    "            \n",
    "            output_filename = filename.replace('.tml', '').replace('.yaml', '').replace('.json', '') + '.lvdash.json'\n",
    "            output_path = os.path.join(LVDASH_OUTPUT_PATH, output_filename)\n",
    "            dbutils.fs.put(output_path, json.dumps(lvdash_data, indent=2), overwrite=True)\n",
    "            status = 'SUCCESS'\n",
    "            print(f\"  Saved to {output_filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  FAILED: {e}\")\n",
    "            import traceback\n",
    "            error_trace = traceback.format_exc()\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            failure_records.append({\n",
    "                'tml_file': filename,\n",
    "                'error_type': type(e).__name__,\n",
    "                'error_message': str(e)[:1000],\n",
    "                'stack_trace': error_trace[:2000],\n",
    "                'failure_timestamp': datetime.now()\n",
    "            })\n",
    "        \n",
    "        summary_results.append({\n",
    "            'tml_file': filename, \n",
    "            'lvdash_file': output_filename, \n",
    "            'status': status,\n",
    "            'num_datasets': len(lvdash_data.get('datasets', [])),\n",
    "            'num_pages': len(lvdash_data.get('pages', [])),\n",
    "            'num_widgets': sum(len(p.get('layout', [])) for p in lvdash_data.get('pages', [])),\n",
    "            'conversion_timestamp': datetime.now()\n",
    "        })\n",
    "\n",
    "    print(\"\\n--- Saving logs ---\")\n",
    "    if tracker.records:\n",
    "        df = pd.DataFrame(tracker.records)\n",
    "        df['conversion_timestamp'] = pd.to_datetime(df['conversion_timestamp'])\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(TRACKER_TABLE)\n",
    "        print(f\"Saved {len(tracker.records)} widget records to {TRACKER_TABLE}\")\n",
    "\n",
    "    if summary_results:\n",
    "        df = pd.DataFrame(summary_results)\n",
    "        df['num_datasets'] = df['num_datasets'].astype('int32')\n",
    "        df['num_pages'] = df['num_pages'].astype('int32')\n",
    "        df['num_widgets'] = df['num_widgets'].astype('int32')\n",
    "        df['conversion_timestamp'] = pd.to_datetime(df['conversion_timestamp'])\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(SUMMARY_TABLE)\n",
    "        print(f\"Saved {len(summary_results)} file summaries to {SUMMARY_TABLE}\")\n",
    "    \n",
    "    if failure_records:\n",
    "        df = pd.DataFrame(failure_records)\n",
    "        df['failure_timestamp'] = pd.to_datetime(df['failure_timestamp'])\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(FAILURE_TABLE)\n",
    "        print(f\"Saved {len(failure_records)} failures to {FAILURE_TABLE}\")\n",
    "    \n",
    "    print(\"\\n--- Conversion complete! ---\")\n",
    "\n",
    "convert_all_tml_files()\n",
    "\n",
    "print(\"--- Overall Conversion Summary ---\")\n",
    "try:\n",
    "    display(spark.table(SUMMARY_TABLE).orderBy(\"conversion_timestamp\", ascending=False))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display summary table. Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Detailed Widget Conversion Tracker ---\")\n",
    "try:\n",
    "    display(spark.table(TRACKER_TABLE).orderBy(\"conversion_timestamp\", ascending=False))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display tracker table. Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Conversion Failures ---\")\n",
    "try:\n",
    "    fail_df = spark.table(FAILURE_TABLE)\n",
    "    fail_count = fail_df.count()\n",
    "    if fail_count > 0:\n",
    "        print(f\"Found {fail_count} failed conversions:\")\n",
    "        display(fail_df.orderBy(\"failure_timestamp\", ascending=False))\n",
    "    else:\n",
    "        print(\"No failures!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not display failure table. Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Chart Type Mapping Statistics ---\")\n",
    "try:\n",
    "    stats_query = f\"\"\"\n",
    "    SELECT \n",
    "        tml_type,\n",
    "        lvdash_type,\n",
    "        status,\n",
    "        COUNT(*) as count,\n",
    "        COUNT(CASE WHEN unmapped_properties != '' THEN 1 END) as has_unmapped_props\n",
    "    FROM {TRACKER_TABLE}\n",
    "    GROUP BY tml_type, lvdash_type, status\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    "    display(spark.sql(stats_query))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display statistics. Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Unmapped Properties Report ---\")\n",
    "try:\n",
    "    unmapped_query = f\"\"\"\n",
    "    SELECT \n",
    "        tml_file,\n",
    "        widget_name,\n",
    "        tml_type,\n",
    "        lvdash_type,\n",
    "        unmapped_properties,\n",
    "        notes\n",
    "    FROM {TRACKER_TABLE}\n",
    "    WHERE unmapped_properties != '' OR notes != ''\n",
    "    ORDER BY tml_file, widget_name\n",
    "    \"\"\"\n",
    "    display(spark.sql(unmapped_query))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display unmapped properties. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Visualization_Mapping_AllViz",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
