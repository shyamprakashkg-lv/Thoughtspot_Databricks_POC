{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63d38463-bf22-4217-9327-e00322730f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualization Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dbb3dea-6797-4189-bdaa-202ec5a280c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "# Configuration\n",
    "CATALOG = \"dbx_migration_poc\"\n",
    "SCHEMA = \"dbx_migration_ts\"\n",
    "TML_VOLUME = \"lv_dashfiles_ak\"\n",
    "LVDASH_VOLUME = \"lvdash_files_ak_out\"\n",
    "\n",
    "dbutils.widgets.text(\"tml_file\", \"\")\n",
    "tml_file = dbutils.widgets.get(\"tml_file\")\n",
    "\n",
    "TML_INPUT_PATH = f\"/Volumes/dbx_migration_poc/dbx_migration_ts/lv_dashfiles_ak/liveboard/{tml_file}\"\n",
    "LVDASH_OUTPUT_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{LVDASH_VOLUME}/\"\n",
    "\n",
    "TRACKER_TABLE = f\"{CATALOG}.{SCHEMA}.tml_conversion_tracker\"\n",
    "SUMMARY_TABLE = f\"{CATALOG}.{SCHEMA}.tml_conversion_summary\"\n",
    "FAILURE_TABLE = f\"{CATALOG}.{SCHEMA}.tml_conversion_failures\"\n",
    "MAPPING_TABLE = f\"{CATALOG}.{SCHEMA}.tml_dbx_metadata_mapping\"\n",
    "\n",
    "LIVEBOARD_FILE = f\"{TML_INPUT_PATH}\"\n",
    "\n",
    "raw_name = os.path.basename(LIVEBOARD_FILE).split('.')[0]\n",
    "asset_name = re.sub(r'[\\s\\-]+', '_', raw_name)\n",
    "\n",
    "# Configuration tables\n",
    "CHART_TYPE_MAPPING_TABLE = f\"{CATALOG}.{SCHEMA}.chart_type_mappings\"\n",
    "WIDGET_SIZE_CONFIG_TABLE = f\"{CATALOG}.{SCHEMA}.widget_size_config\"\n",
    "EXPRESSION_TRANSFORM_TABLE = f\"{CATALOG}.{SCHEMA}.expression_transformations\"\n",
    "SCALE_TYPE_DETECTION_TABLE = f\"{CATALOG}.{SCHEMA}.scale_type_detection\"\n",
    "COLUMN_DETAILS_TABLE = f\"{CATALOG}.{SCHEMA}.{asset_name}_support_viz_column_details\"\n",
    "FILTER_MAPPING_TABLE = f\"{CATALOG}.{SCHEMA}.{asset_name}_filter_details\"\n",
    "VIZ_FILTER_TABLE = f\"{CATALOG}.{SCHEMA}.{asset_name}_support_viz_filter_metadata\"\n",
    "\n",
    "def validate_configuration():\n",
    "    print(\"--- Validating configuration ---\")\n",
    "    error_found = False\n",
    "    for var_name, var_value in [(\"CATALOG\", CATALOG), (\"SCHEMA\", SCHEMA), (\"TML_VOLUME\", TML_VOLUME), (\"LVDASH_VOLUME\", LVDASH_VOLUME)]:\n",
    "        if \"/\" in var_value or \"\\\\\" in var_value:\n",
    "            print(f\"ERROR: '{var_name}' contains a slash. Must be a single name.\")\n",
    "            error_found = True\n",
    "    \n",
    "    if not TML_INPUT_PATH.startswith(\"/Volumes/\"):\n",
    "        print(f\"WARNING: 'TML_INPUT_PATH' does not look like a Volume path.\")\n",
    "        error_found = True\n",
    "    \n",
    "    if error_found:\n",
    "        raise ValueError(\"Invalid configuration. Please review errors above.\")\n",
    "    else:\n",
    "        print(\"Configuration looks good.\")\n",
    "\n",
    "validate_configuration()\n",
    "\n",
    "def load_viz_filter_metadata():\n",
    "    try:\n",
    "        if not spark.catalog.tableExists(VIZ_FILTER_TABLE): return pd.DataFrame()\n",
    "        df = spark.table(VIZ_FILTER_TABLE).toPandas()\n",
    "        print(f\"✓ Loaded {len(df)} viz filter records from {VIZ_FILTER_TABLE}\")\n",
    "        return df\n",
    "    except: return pd.DataFrame()\n",
    "\n",
    "VIZ_FILTER_DATA = load_viz_filter_metadata()\n",
    "\n",
    "def load_chart_type_mappings():\n",
    "    \"\"\"Load chart type mappings from configuration table - REQUIRED\"\"\"\n",
    "    try:\n",
    "        df = spark.table(CHART_TYPE_MAPPING_TABLE).toPandas()\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(f\"Configuration table {CHART_TYPE_MAPPING_TABLE} is empty. Please run the configuration setup notebook first.\")\n",
    "        \n",
    "        mapping = {}\n",
    "        for _, row in df.iterrows():\n",
    "            tml_type = row['tml_chart_type']\n",
    "            widget_type = row['widget_type']\n",
    "            mapping[tml_type] = widget_type if pd.notna(widget_type) else None\n",
    "        \n",
    "        print(f\"✓ Loaded {len(mapping)} chart type mappings from {CHART_TYPE_MAPPING_TABLE}\")\n",
    "        return mapping\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\"\"\n",
    "ERROR: Failed to load required configuration table: {CHART_TYPE_MAPPING_TABLE}\n",
    "\n",
    "Details: {str(e)}\n",
    "\n",
    "SOLUTION: You must run the configuration setup notebook to create the required tables:\n",
    "1. Open the 'Configuration Tables Setup' notebook\n",
    "2. Run all cells to create: chart_type_mappings, widget_size_config, expression_transformations, scale_type_detection\n",
    "3. Then re-run this converter\n",
    "\n",
    "Configuration tables are REQUIRED and no fallback mappings are available.\n",
    "\"\"\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(f\"Required configuration table {CHART_TYPE_MAPPING_TABLE} not found or cannot be loaded. Run configuration setup first.\") from e\n",
    "\n",
    "def load_expression_transformations():\n",
    "    \"\"\"Load expression transformation rules from configuration table - REQUIRED\"\"\"\n",
    "    try:\n",
    "        df = spark.table(EXPRESSION_TRANSFORM_TABLE).toPandas()\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(f\"Configuration table {EXPRESSION_TRANSFORM_TABLE} is empty. Please run the configuration setup notebook first.\")\n",
    "        \n",
    "        transformations = []\n",
    "        for _, row in df.iterrows():\n",
    "            transformations.append({\n",
    "                'pattern': row['tml_pattern'],\n",
    "                'target': row['target_expression']\n",
    "            })\n",
    "        print(f\"✓ Loaded {len(transformations)} expression transformations from {EXPRESSION_TRANSFORM_TABLE}\")\n",
    "        return transformations\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\"\"\n",
    "ERROR: Failed to load required configuration table: {EXPRESSION_TRANSFORM_TABLE}\n",
    "\n",
    "Details: {str(e)}\n",
    "\n",
    "SOLUTION: Run the configuration setup notebook to create required tables.\n",
    "\"\"\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(f\"Required configuration table {EXPRESSION_TRANSFORM_TABLE} not found. Run configuration setup first.\") from e\n",
    "\n",
    "def load_column_details():\n",
    "    try:\n",
    "        df = spark.table(COLUMN_DETAILS_TABLE).toPandas()\n",
    "        if len(df) == 0:\n",
    "            print(f\"WARNING: Column details table {COLUMN_DETAILS_TABLE} is empty.\")\n",
    "            return pd.DataFrame()\n",
    "        print(f\"✓ Loaded {len(df)} column detail records from {COLUMN_DETAILS_TABLE}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not load column details table: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_scale_type_rules():\n",
    "    \"\"\"Load scale type detection rules from configuration table - REQUIRED\"\"\"\n",
    "    try:\n",
    "        df = spark.table(SCALE_TYPE_DETECTION_TABLE).toPandas()\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(f\"Configuration table {SCALE_TYPE_DETECTION_TABLE} is empty. Please run the configuration setup notebook first.\")\n",
    "        \n",
    "        rules = []\n",
    "        for _, row in df.iterrows():\n",
    "            rules.append({\n",
    "                'pattern': row['field_pattern'],\n",
    "                'scale_type': row['scale_type']\n",
    "            })\n",
    "        print(f\"✓ Loaded {len(rules)} scale type rules from {SCALE_TYPE_DETECTION_TABLE}\")\n",
    "        return rules\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\"\"\n",
    "ERROR: Failed to load required configuration table: {SCALE_TYPE_DETECTION_TABLE}\n",
    "\n",
    "Details: {str(e)}\n",
    "\n",
    "SOLUTION: Run the configuration setup notebook to create required tables.\n",
    "\"\"\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(f\"Required configuration table {SCALE_TYPE_DETECTION_TABLE} not found. Run configuration setup first.\") from e\n",
    "\n",
    "def load_widget_size_config():\n",
    "    \"\"\"Load widget size configuration from database table - REQUIRED\"\"\"\n",
    "    try:\n",
    "        df = spark.table(WIDGET_SIZE_CONFIG_TABLE).toPandas()\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            raise ValueError(f\"Configuration table {WIDGET_SIZE_CONFIG_TABLE} is empty. Please run the configuration setup notebook first.\")\n",
    "        \n",
    "        size_map = {}\n",
    "        for _, row in df.iterrows():\n",
    "            size_map[row['size_category']] = {\n",
    "                'width': row['width'],\n",
    "                'height': row['height']\n",
    "            }\n",
    "        print(f\"✓ Loaded {len(size_map)} widget size configurations from {WIDGET_SIZE_CONFIG_TABLE}\")\n",
    "        return size_map\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\"\"\n",
    "ERROR: Failed to load required configuration table: {WIDGET_SIZE_CONFIG_TABLE}\n",
    "\n",
    "Details: {str(e)}\n",
    "\n",
    "SOLUTION: Run the configuration setup notebook to create required tables.\n",
    "\"\"\"\n",
    "        print(error_msg)\n",
    "        raise RuntimeError(f\"Required configuration table {WIDGET_SIZE_CONFIG_TABLE} not found. Run configuration setup first.\") from e\n",
    "\n",
    "def load_filter_mappings():\n",
    "    \"\"\"Load filter definitions from configuration table - REQUIRED\"\"\"\n",
    "    try:\n",
    "        # Check if table exists first to avoid hard crash on spark.table()\n",
    "        if not spark.catalog.tableExists(FILTER_MAPPING_TABLE):\n",
    "             print(f\"WARNING: Filter table {FILTER_MAPPING_TABLE} not found. Filters will be skipped.\")\n",
    "             return pd.DataFrame()\n",
    "        df = spark.table(FILTER_MAPPING_TABLE).toPandas()\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(f\"WARNING: Configuration table {FILTER_MAPPING_TABLE} is empty. No filters will be generated.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # We return the DataFrame directly because the filter logic needs to query it\n",
    "        # dynamically based on the filename later in the script.\n",
    "        print(f\"✓ Loaded {len(df)} filter mappings from {FILTER_MAPPING_TABLE}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        error_msg = f\"\"\"\n",
    "ERROR: Failed to load configuration table: {FILTER_MAPPING_TABLE}\n",
    "Details: {str(e)}\n",
    "SOLUTION: Run the configuration setup notebook to create required tables.\n",
    "\"\"\"\n",
    "        print(error_msg)\n",
    "        # If filters are strictly required, uncomment the next line.\n",
    "        # Otherwise, returning an empty DF allows the script to continue without filters.\n",
    "        # raise RuntimeError(f\"Required configuration table {FILTER_MAPPING_TABLE} not found.\") from e\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load all configuration data\n",
    "TML_TO_LVDASH_MAPPING = load_chart_type_mappings()\n",
    "WIDGET_SIZE_MAP = load_widget_size_config()\n",
    "EXPRESSION_TRANSFORMATIONS = load_expression_transformations()\n",
    "SCALE_TYPE_RULES = load_scale_type_rules()\n",
    "COLUMN_DETAILS_DATA = load_column_details()\n",
    "FILTER_DATA = load_filter_mappings()\n",
    "\n",
    "\n",
    "def load_mapping_data():\n",
    "    try:\n",
    "        mapping_df = spark.table(MAPPING_TABLE).toPandas()\n",
    "        print(f\"Loaded {len(mapping_df)} mappings from {MAPPING_TABLE}\")\n",
    "        \n",
    "        unmapped = mapping_df[\n",
    "            (mapping_df['databricks_table_name_ToBeFilled'].isna()) | \n",
    "            (mapping_df['databricks_table_name_ToBeFilled'] == '')\n",
    "        ]\n",
    "        \n",
    "        if len(unmapped) > 0:\n",
    "            print(f\"\\nWARNING: {len(unmapped)} visualizations are not mapped to Databricks tables:\")\n",
    "            for _, row in unmapped.head(10).iterrows():\n",
    "                print(f\"  - {row['tml_file']}: {row['visualization_name']}\")\n",
    "            if len(unmapped) > 10:\n",
    "                print(f\"  ... and {len(unmapped) - 10} more\")\n",
    "            print(\"\\nThese visualizations will use TML table names directly.\")\n",
    "        \n",
    "        return mapping_df\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not load mapping table: {e}\")\n",
    "        print(\"Conversion will proceed using TML table names directly.\")\n",
    "        return None\n",
    "\n",
    "MAPPING_DATA = load_mapping_data()\n",
    "\n",
    "def setup_environment():\n",
    "    def ensure_schema_exists(catalog, schema):\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog}`.`{schema}`\")\n",
    "    \n",
    "    def ensure_volume_exists(catalog, schema, volume):\n",
    "        try:\n",
    "            spark.sql(f\"DESCRIBE VOLUME `{catalog}`.`{schema}`.`{volume}`\")\n",
    "        except Exception:\n",
    "            print(f\"  Creating volume {volume}...\")\n",
    "            spark.sql(f\"CREATE VOLUME IF NOT EXISTS `{catalog}`.`{schema}`.`{volume}`\")\n",
    "    \n",
    "    def ensure_table_exists(full_table_name, table_type):\n",
    "        parts = full_table_name.split('.')\n",
    "        catalog = parts[0]\n",
    "        schema = parts[1]\n",
    "        table_name = parts[2]\n",
    "        \n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS `{catalog}`.`{schema}`.`{table_name}`\")\n",
    "            print(f\"  Dropped existing table: {table_name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        if table_type == \"tracker\":\n",
    "            create_sql = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE `{catalog}`.`{schema}`.`{table_name}` (\n",
    "                    tml_file STRING, \n",
    "                    widget_name STRING, \n",
    "                    tml_type STRING, \n",
    "                    lvdash_type STRING, \n",
    "                    status STRING, \n",
    "                    available_fields STRING, \n",
    "                    missing_fields STRING, \n",
    "                    unmapped_properties STRING, \n",
    "                    notes STRING, \n",
    "                    conversion_timestamp TIMESTAMP\n",
    "                ) USING DELTA\n",
    "            \"\"\"\n",
    "        elif table_type == \"failure\":\n",
    "            create_sql = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE `{catalog}`.`{schema}`.`{table_name}` (\n",
    "                    tml_file STRING,\n",
    "                    error_type STRING,\n",
    "                    error_message STRING,\n",
    "                    stack_trace STRING,\n",
    "                    failure_timestamp TIMESTAMP\n",
    "                ) USING DELTA\n",
    "            \"\"\"\n",
    "        else:\n",
    "            create_sql = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE `{catalog}`.`{schema}`.`{table_name}` (\n",
    "                    tml_file STRING, \n",
    "                    lvdash_file STRING, \n",
    "                    status STRING, \n",
    "                    num_datasets INT, \n",
    "                    num_pages INT, \n",
    "                    num_widgets INT, \n",
    "                    conversion_timestamp TIMESTAMP\n",
    "                ) USING DELTA\n",
    "            \"\"\"\n",
    "        \n",
    "        spark.sql(create_sql)\n",
    "        print(f\"  Created table: {table_name}\")\n",
    "    \n",
    "    print(\"\\n--- Setting up environment ---\")\n",
    "    ensure_schema_exists(CATALOG, SCHEMA)\n",
    "    ensure_volume_exists(CATALOG, SCHEMA, TML_VOLUME)\n",
    "    ensure_volume_exists(CATALOG, SCHEMA, LVDASH_VOLUME)\n",
    "    ensure_table_exists(TRACKER_TABLE, \"tracker\")\n",
    "    ensure_table_exists(SUMMARY_TABLE, \"summary\")\n",
    "    ensure_table_exists(FAILURE_TABLE, \"failure\")\n",
    "    print(\"--- Environment ready ---\\n\")\n",
    "\n",
    "def generate_unique_id(length=8):\n",
    "    return uuid.uuid4().hex[:length]\n",
    "\n",
    "def parse_tml_file(file_path):\n",
    "    try:\n",
    "        content = dbutils.fs.head(file_path, 10 * 1024 * 1024)\n",
    "        try:\n",
    "            return yaml.safe_load(content)\n",
    "        except yaml.YAMLError:\n",
    "            return json.loads(content)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to parse TML file: {str(e)}\")\n",
    "\n",
    "def extract_colors_from_tml(chart_data):\n",
    "    try:\n",
    "        client_state = chart_data.get('client_state_v2', '{}')\n",
    "        if isinstance(client_state, str):\n",
    "            client_state = json.loads(client_state)\n",
    "        \n",
    "        system_colors = client_state.get('systemSeriesColors', [])\n",
    "        if system_colors:\n",
    "            colors = []\n",
    "            for item in system_colors:\n",
    "                if item.get('color') and item['color'] not in colors:\n",
    "                    colors.append(item['color'])\n",
    "            if colors:\n",
    "                return colors\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return [\"#2E75F0\", \"#06BF7F\", \"#FCC838\", \"#48D1E0\", \"#71A1F4\", \"#8C62F5\"]\n",
    "\n",
    "def infer_scale_type(column_name):\n",
    "    \"\"\"Infer scale type using configuration rules\"\"\"\n",
    "    name = column_name.lower()\n",
    "    \n",
    "    # Use configuration rules if available\n",
    "    if SCALE_TYPE_RULES:\n",
    "        for rule in SCALE_TYPE_RULES:\n",
    "            pattern = rule['pattern'].lower()\n",
    "            # Remove wildcards and check if pattern is in the column name\n",
    "            pattern_parts = pattern.replace('*_', '').replace('_*', '').split(',')\n",
    "            for part in pattern_parts:\n",
    "                part = part.strip()\n",
    "                if part in name:\n",
    "                    return rule['scale_type']\n",
    "    \n",
    "    # Fallback to default logic\n",
    "    if any(t in name for t in ['date', 'time', 'year', 'month', 'day', 'timestamp', 'week']):\n",
    "        return 'temporal'\n",
    "    \n",
    "    if any(t in name for t in ['total', 'sum', 'count', 'avg', 'average', 'min', 'max', 'revenue', 'price', 'quantity', 'amount', 'sales', 'number']):\n",
    "        return 'quantitative'\n",
    "    \n",
    "    return 'categorical'\n",
    "\n",
    "def clean_field_name(field_name):\n",
    "    if not field_name:\n",
    "        return \"\"\n",
    "    cleaned = re.sub(\n",
    "        r'^(Total |sum\\(|count\\(|avg\\(|min\\(|max\\(|Unique Number of )',\n",
    "        '',\n",
    "        field_name,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    cleaned = re.sub(r'\\)$', '', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "def sanitize_alias_name(alias_name):\n",
    "  if not alias_name:\n",
    "    return \"\"\n",
    "  safe_name = re.sub(r'\\W+', '_', alias_name)\n",
    "  sanitized_alias = safe_name.strip('_')\n",
    "  return sanitized_alias if sanitized_alias else \"_\"\n",
    "\n",
    "class ConversionTracker:\n",
    "    def __init__(self):\n",
    "        self.records = []\n",
    "    \n",
    "    def add_record(self, tml_file, widget_name, tml_type, lvdash_type, status, available, missing, unmapped_props, notes=\"\"):\n",
    "        self.records.append({\n",
    "            'tml_file': tml_file, \n",
    "            'widget_name': widget_name, \n",
    "            'tml_type': tml_type,\n",
    "            'lvdash_type': lvdash_type, \n",
    "            'status': status,\n",
    "            'available_fields': ', '.join(available), \n",
    "            'missing_fields': ', '.join(missing),\n",
    "            'unmapped_properties': ', '.join(unmapped_props),\n",
    "            'notes': notes, \n",
    "            'conversion_timestamp': datetime.now()\n",
    "        })\n",
    "\n",
    "class TMLToLVDASHConverter:\n",
    "    # Update __init__ arguments\n",
    "    def __init__(self, tml_data, tml_filename, tracker, mapping_data=None, column_details_data=None, viz_filter_data=None):\n",
    "        self.tml_data = tml_data\n",
    "        self.tml_filename = tml_filename\n",
    "        self.tracker = tracker\n",
    "        self.mapping_data = mapping_data\n",
    "        self.column_details_data = column_details_data if column_details_data is not None else pd.DataFrame()\n",
    "        # NEW: Store viz filter data\n",
    "        self.viz_filter_data = viz_filter_data if viz_filter_data is not None else pd.DataFrame()\n",
    "        \n",
    "    def _get_mapping_for_viz(self, viz_id):\n",
    "        \"\"\"Get mapping for a specific visualization ID, returns Series or None\"\"\"\n",
    "        if self.mapping_data is None or len(self.mapping_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            mapping = self.mapping_data[\n",
    "                (self.mapping_data['tml_file'] == self.tml_filename) &\n",
    "                (self.mapping_data['visualization_id'] == viz_id)\n",
    "            ]\n",
    "            \n",
    "            # Always return a Series (single row) or None\n",
    "            if not mapping.empty:\n",
    "                return mapping.iloc[0]  \n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Error getting mapping for viz {viz_id}: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def _get_column_details_for_viz(self, viz_id):\n",
    "        if self.column_details_data is None or self.column_details_data.empty:\n",
    "            return {}\n",
    "        try:\n",
    "            # 1. Filter column details for this Viz\n",
    "            viz_columns = self.column_details_data[self.column_details_data['VizID'] == viz_id]\n",
    "            if viz_columns.empty:\n",
    "                return {}\n",
    "            \n",
    "            # 2. Filter metadata for this Viz (NEW)\n",
    "            viz_filters = pd.DataFrame()\n",
    "            if hasattr(self, 'viz_filter_data') and not self.viz_filter_data.empty:\n",
    "                 viz_filters = self.viz_filter_data[self.viz_filter_data['viz_id'] == viz_id]\n",
    "\n",
    "            column_dict = {}\n",
    "            for _, row in viz_columns.iterrows():\n",
    "                column_name = row['ColumnName']\n",
    "                sanitized_name = row['Santized_Column']\n",
    "                \n",
    "                # 3. Lookup Filter Details (NEW)\n",
    "                filter_val = None\n",
    "                if not viz_filters.empty:\n",
    "                    match = viz_filters[viz_filters['Sanitized_Column'] == sanitized_name]\n",
    "                    if not match.empty:\n",
    "                        filter_val = match.iloc[0]['Filter_Details']\n",
    "\n",
    "                column_dict[column_name] = {\n",
    "                    'model_base_column': row['ModelBaseColumn'],\n",
    "                    'aggregation': row['Aggregation'] if pd.notna(row['Aggregation']) else None,\n",
    "                    'expression': row['Expression'] if pd.notna(row['Expression']) else None,\n",
    "                    'sanitized': sanitized_name,\n",
    "                    'filter_condition': filter_val # Store the filter like .false or .weekly\n",
    "                }\n",
    "            return column_dict\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Error getting column details for viz {viz_id}: {e}\")\n",
    "            return {}\n",
    "    def _detect_filter_widget_type(self, column_name, is_single_value=False):\n",
    "        \"\"\"\n",
    "        Determines the correct Databricks widget type based on column name and single-value flag.\n",
    "        \"\"\"\n",
    "        if not column_name: return \"filter-multi-select\"\n",
    "        col_lower = column_name.lower()\n",
    "        \n",
    "        # 1. Check for Date/Time\n",
    "        is_date = any(x in col_lower for x in ['date', 'time', 'created', 'closed', 'timestamp'])\n",
    "        \n",
    "        if is_date:\n",
    "            if is_single_value:\n",
    "                return \"filter-date-picker\"       # Single Date\n",
    "            else:\n",
    "                return \"filter-date-range-picker\" # Date Range (Default)\n",
    "        \n",
    "        # 2. Check for Non-Date Single/Multi\n",
    "        if is_single_value:\n",
    "            return \"filter-single-select\"\n",
    "            \n",
    "        return \"filter-multi-select\"\n",
    "    def _create_filter_widgets(self, datasets_map):\n",
    "        \"\"\"\n",
    "        Creates filter widgets with SQL Scanning, Deduplication, and Default Values.\n",
    "        \"\"\"\n",
    "        if FILTER_DATA is None or FILTER_DATA.empty: return []\n",
    "        \n",
    "        # 1. Fuzzy match filename\n",
    "        clean_filename = self.tml_filename.replace('.tml', '').replace('.json', '').strip()\n",
    "        if 'tml_file' in FILTER_DATA.columns:\n",
    "             file_filters = FILTER_DATA[FILTER_DATA['tml_file'].astype(str).str.contains(clean_filename, case=False, regex=False)]\n",
    "        else:\n",
    "             file_filters = FILTER_DATA\n",
    "            \n",
    "        if file_filters.empty: return []\n",
    "        if not datasets_map: return []\n",
    "        \n",
    "        # 2. Get the Unified Dataset & SQL\n",
    "        target_dataset_obj = next(iter(datasets_map.values()))\n",
    "        target_dataset_id = target_dataset_obj['name']\n",
    "        dataset_sql = \" \".join(target_dataset_obj.get('queryLines', [])).lower()\n",
    "        \n",
    "        filter_widgets = []\n",
    "        seen_aliases = set() # Deduplication check\n",
    "        \n",
    "        print(f\"\\n--- Creating Filters (Linked to: {target_dataset_id}) ---\")\n",
    "        \n",
    "        # --- Helper: Find Alias in SQL ---\n",
    "        def find_alias_in_sql(col_name):\n",
    "            import re\n",
    "            clean_col = col_name.lower().replace('`', '').strip()\n",
    "            if \"::\" in clean_col: clean_col = clean_col.split(\"::\")[-1].strip()\n",
    "            \n",
    "            # 1. Exact Match: \"Column Name\" AS Alias\n",
    "            pattern = r\"['`\\\"]?\" + re.escape(clean_col) + r\"['`\\\"]?\\s+as\\s+['`\\\"]?([\\w_]+)['`\\\"]?\"\n",
    "            match = re.search(pattern, dataset_sql)\n",
    "            if match: return match.group(1)\n",
    "            \n",
    "            # 2. Base Match: \"Name\" AS Alias (ignores prefixes)\n",
    "            short_col = clean_col.split('tbl_')[-1] if 'tbl_' in clean_col else clean_col\n",
    "            pattern_short = r\"['`\\\"]?\" + re.escape(short_col) + r\"['`\\\"]?\\s+as\\s+['`\\\"]?([\\w_]+)['`\\\"]?\"\n",
    "            match_short = re.search(pattern_short, dataset_sql)\n",
    "            if match_short: return match_short.group(1)\n",
    "            \n",
    "            return sanitize_alias_name(clean_col)\n",
    "        # ---------------------------------\n",
    "\n",
    "        for index, row in file_filters.iterrows():\n",
    "            try:\n",
    "                display_name = row.get('display_name', 'Filter')\n",
    "                \n",
    "                # Get Raw Column\n",
    "                if 'Filter_Column' in row and pd.notna(row['Filter_Column']):\n",
    "                    raw_col_name = row['Filter_Column']\n",
    "                else:\n",
    "                    phys_id = row.get('Physical_Column_ID', '')\n",
    "                    raw_col_name = phys_id.split('::')[-1] if '::' in str(phys_id) else phys_id\n",
    "                \n",
    "                raw_col_name = raw_col_name.replace('[', '').replace(']', '').strip()\n",
    "                \n",
    "                # [CRITICAL FIX] Find Actual SQL Alias\n",
    "                sql_alias = find_alias_in_sql(raw_col_name)\n",
    "                \n",
    "                # ... (Existing code matches lines 606-607)\n",
    "                if sql_alias in seen_aliases: continue\n",
    "                seen_aliases.add(sql_alias)\n",
    "\n",
    "                # --- NEW LOGIC START: Process Default Values ---\n",
    "                default_selection_spec = {}\n",
    "                raw_values = str(row.get('Values', '')).strip()\n",
    "\n",
    "                # Check if values exist and are not null/nan\n",
    "                if raw_values and raw_values.lower() not in ['nan', 'none', '', 'null']:\n",
    "                    # Regex to extract values inside single quotes: ('Value') -> Value\n",
    "                    # This handles the user requirement: \"('4C Portal...')\" -> \"4C Portal...\"\n",
    "                    extracted_values = re.findall(r\"'([^']*)'\", raw_values)\n",
    "\n",
    "                    # Fallback: If no quotes found but text exists (e.g. numeric (1, 2)), strip parens and split\n",
    "                    if not extracted_values:\n",
    "                        cleaned_raw = raw_values.replace('(', '').replace(')', '')\n",
    "                        extracted_values = [x.strip() for x in cleaned_raw.split(',') if x.strip()]\n",
    "\n",
    "                    if extracted_values:\n",
    "                        default_selection_spec = {\n",
    "                            \"defaultSelection\": {\n",
    "                                \"values\": {\n",
    "                                    \"dataType\": \"STRING\", # Defaulting to STRING for categorical filters\n",
    "                                    \"values\": [{\"value\": v} for v in extracted_values]\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                # --- NEW LOGIC END ---\n",
    "\n",
    "                # Determine Type\n",
    "                is_single = str(row.get('is_single_value', 'false')).lower() in ['true', 'yes', '1']\n",
    "                widget_type = self._detect_filter_widget_type(raw_col_name, is_single)\n",
    "                \n",
    "                widget_id = f\"filter_{generate_unique_id()}\"\n",
    "                query_id = f\"query_{generate_unique_id()}\"\n",
    "                \n",
    "                # Build JSON\n",
    "                filter_spec = {\n",
    "                    \"version\": 2,\n",
    "                    \"widgetType\": widget_type,\n",
    "                    \"encodings\": {\n",
    "                        \"fields\": [\n",
    "                            {\"fieldName\": sql_alias, \"queryName\": query_id}\n",
    "                        ]\n",
    "                    },\n",
    "                    \"frame\": {\n",
    "                        \"showTitle\": True,\n",
    "                        \"title\": display_name\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                # Apply default selection if found\n",
    "                if default_selection_spec:\n",
    "                    filter_spec[\"selection\"] = default_selection_spec\n",
    "\n",
    "                filter_widget = {\n",
    "                    \"viz_id\": f\"filter_{index}\",\n",
    "                    \"is_filter\": True,\n",
    "                    \"widget\": {\n",
    "                        \"name\": widget_id,\n",
    "                        \"queries\": [\n",
    "                            {\n",
    "                                \"name\": query_id,\n",
    "                                \"query\": {\n",
    "                                    \"datasetName\": target_dataset_id,\n",
    "                                    \"fields\": [\n",
    "                                        {\"name\": sql_alias, \"expression\": f\"`{sql_alias}`\"},\n",
    "                                        {\"name\": f\"{sql_alias}_associativity\", \"expression\": \"COUNT_IF(`associative_filter_predicate_group`)\"}\n",
    "                                    ],\n",
    "                                    \"disaggregated\": False\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                        \"spec\": filter_spec\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                filter_widgets.append(filter_widget)\n",
    "                # ... (Rest of the function remains the same)\n",
    "                print(f\"  ✓ Created {widget_type}: {display_name} -> Alias: {sql_alias}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR creating filter row {index}: {e}\")\n",
    "                \n",
    "        return filter_widgets\n",
    "    # ----------------------------------------\n",
    "\n",
    "    def convert(self):\n",
    "        liveboard = self.tml_data.get('liveboard')\n",
    "        if not liveboard:\n",
    "            raise ValueError(\"TML file missing 'liveboard' root key.\")\n",
    "\n",
    "        visualizations = liveboard.get('visualizations', [])\n",
    "        if not visualizations:\n",
    "            raise ValueError(\"No visualizations found in liveboard.\")\n",
    "        \n",
    "        # Track unique datasets by their name (supports reuse)\n",
    "        datasets_map = {}  \n",
    "        dataset_usage = {}  \n",
    "        widgets = []\n",
    "\n",
    "        for viz in visualizations:\n",
    "            try:\n",
    "                viz_id = viz.get('id')\n",
    "                viz_name = viz.get('answer', {}).get('name', 'Unknown')\n",
    "                \n",
    "                # Check if this viz should use a common/shared dataset\n",
    "                mapping = self._get_mapping_for_viz(viz_id)\n",
    "                column_details = self._get_column_details_for_viz(viz_id)\n",
    "                print(\"Check1\", mapping)\n",
    "                \n",
    "                if mapping is not None:\n",
    "                    common_ds = mapping.get('common_dataset_name')\n",
    "                    if pd.notna(common_ds) and common_ds and str(common_ds).strip() != '' and str(common_ds).lower() != 'null':\n",
    "                        # USE SHARED DATASET from mapping table\n",
    "                        dataset_name = common_ds\n",
    "                        if dataset_name not in datasets_map:\n",
    "                            dataset = self._create_shared_dataset(viz, mapping)\n",
    "                            datasets_map[dataset_name] = dataset\n",
    "                            dataset_usage[dataset_name] = []\n",
    "                            print(f\"  INFO: Created shared dataset '{dataset_name}' for visualization '{viz_name}'\")\n",
    "                        else:\n",
    "                            print(f\"  INFO: Reusing shared dataset '{dataset_name}' for visualization '{viz_name}'\")\n",
    "                        \n",
    "                        dataset_usage[dataset_name].append(viz_name)\n",
    "                    else:\n",
    "                        # CREATE UNIQUE DATASET per visualization\n",
    "                        dataset = self._create_dataset(viz)\n",
    "                        dataset_name = dataset['name']\n",
    "                        datasets_map[dataset_name] = dataset\n",
    "                        dataset_usage[dataset_name] = [viz_name]\n",
    "                        print(f\"  INFO: Created unique dataset '{dataset_name}' for visualization '{viz_name}'\")\n",
    "                else:\n",
    "                    # CREATE UNIQUE DATASET per visualization (no mapping found)\n",
    "                    dataset = self._create_dataset(viz)\n",
    "                    dataset_name = dataset['name']\n",
    "                    datasets_map[dataset_name] = dataset\n",
    "                    dataset_usage[dataset_name] = [viz_name]\n",
    "                    print(f\"  INFO: Created unique dataset '{dataset_name}' for visualization '{viz_name}'\")\n",
    "                        \n",
    "                # Create widget using the appropriate dataset name\n",
    "                widget_data = self._create_widget(viz, dataset_name)\n",
    "                if isinstance(widget_data, list):\n",
    "                    for w in widget_data:\n",
    "                        if isinstance(w, dict):\n",
    "                            widgets.append(w)\n",
    "                        else:\n",
    "                            print(f\"  WARNING: Skipping invalid widget object of type {type(w)}\")\n",
    "                elif isinstance(widget_data, dict):\n",
    "                    widgets.append(widget_data)\n",
    "                else:\n",
    "                    print(f\"  WARNING: _create_widget returned unexpected type {type(widget_data)} for viz {viz_id}\")\n",
    "                \n",
    "            except Exception as viz_error:\n",
    "                viz_name = viz.get('answer', {}).get('name', 'Unknown')\n",
    "                print(f\"  WARNING: Failed to convert visualization '{viz_name}': {viz_error}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                self.tracker.add_record(\n",
    "                    self.tml_filename,\n",
    "                    viz_name,\n",
    "                    viz.get('answer', {}).get('chart', {}).get('type', 'UNKNOWN'),\n",
    "                    'ERROR',\n",
    "                    'ERROR',\n",
    "                    [],\n",
    "                    [],\n",
    "                    [],\n",
    "                    f\"Conversion failed: {str(viz_error)[:500]}\"\n",
    "                )\n",
    "                continue\n",
    "        try:\n",
    "            filter_widgets = self._create_filter_widgets(datasets_map)\n",
    "            if filter_widgets:\n",
    "                widgets.extend(filter_widgets)\n",
    "                print(f\"  INFO: Added {len(filter_widgets)} filter widgets.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR generating filters: {e}\")\n",
    "\n",
    "        if not widgets:\n",
    "            raise ValueError(\"No widgets could be converted successfully.\")\n",
    "\n",
    "        # Log dataset reuse statistics\n",
    "        print(f\"\\n--- Dataset Reuse Summary ---\")\n",
    "        for ds_name, viz_list in dataset_usage.items():\n",
    "            if len(viz_list) > 1:\n",
    "                print(f\"  ✓ Shared dataset '{ds_name}' used by {len(viz_list)} visualizations: {', '.join(viz_list[:3])}{'...' if len(viz_list) > 3 else ''}\")\n",
    "        print(f\"Total datasets created: {len(datasets_map)}\")\n",
    "        print(f\"Total widgets created: {len(widgets)}\")\n",
    "        \n",
    "        # Check widget structure\n",
    "        print(f\"\\n--- Widget Structure Check ---\")\n",
    "        for i, w in enumerate(widgets):\n",
    "            if not isinstance(w, dict):\n",
    "                print(f\"  ERROR: Widget at index {i} is type {type(w)}, not dict!\")\n",
    "            elif 'widget' not in w:\n",
    "                print(f\"  WARNING: Widget at index {i} missing 'widget' key\")\n",
    "            else:\n",
    "                print(f\"  ✓ Widget {i}: viz_id={w.get('viz_id')}, type={w.get('widget', {}).get('spec', {}).get('widgetType')}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "        return {\n",
    "            \"datasets\": list(datasets_map.values()),\n",
    "            \"pages\": [{\n",
    "                \"name\": generate_unique_id(),\n",
    "                \"displayName\": liveboard.get('name', 'Converted Dashboard'),\n",
    "                \"layout\": self._create_layout(liveboard.get('layout'), widgets),\n",
    "                \"pageType\": \"PAGE_TYPE_CANVAS\"\n",
    "            }]\n",
    "        }\n",
    "\n",
    "    def _get_sql_filter(self, col_expression, filter_str):\n",
    "        \"\"\"\n",
    "        Converts TML shorthand into SQL conditions.\n",
    "        Applies lower() to string comparisons to ensure case-insensitive matching\n",
    "        (e.g. matching 'work in progress' to 'Work in Progress').\n",
    "        \"\"\"\n",
    "        if not filter_str or pd.isna(filter_str) or str(filter_str).lower() in ['nan', 'null', 'none']:\n",
    "            return None\n",
    "        s = str(filter_str).strip()\n",
    "\n",
    "        # 1. Handle Null Checks\n",
    "        if \"!= '{null}'\" in s: return f\"{col_expression} IS NOT NULL\"\n",
    "        if \"= '{null}'\" in s: return f\"{col_expression} IS NULL\"\n",
    "\n",
    "        # 2. Handle Lists (comma separated) -> IN clause\n",
    "        # Example: .on-hold, .Work in Progress -> lower(Col) IN ('on-hold', 'work in progress')\n",
    "        if \",\" in s:\n",
    "            parts = []\n",
    "            has_strings = False\n",
    "            for p in s.split(','):\n",
    "                val = p.strip()\n",
    "                if val.startswith('.'): val = val[1:] # Remove dot\n",
    "                \n",
    "                # Check types\n",
    "                if val.lower() in ['true', 'false']:\n",
    "                    parts.append(val.upper()) # TRUE/FALSE (No quotes)\n",
    "                elif re.match(r'^\\d+(\\.\\d+)?$', val):\n",
    "                    parts.append(val) # Number (No quotes)\n",
    "                else:\n",
    "                    # String: Mark as string list and lowercase the value\n",
    "                    has_strings = True\n",
    "                    parts.append(f\"'{val.lower()}'\")\n",
    "            \n",
    "            if has_strings:\n",
    "                # Apply lower() to the column for case-insensitive match\n",
    "                return f\"lower({col_expression}) IN ({', '.join(parts)})\"\n",
    "            else:\n",
    "                return f\"{col_expression} IN ({', '.join(parts)})\"\n",
    "\n",
    "        # 3. Handle Single Dot Values\n",
    "        if s.startswith('.'):\n",
    "            val = s[1:]\n",
    "            # Booleans\n",
    "            if val.lower() == 'true': return f\"{col_expression} = true\"\n",
    "            if val.lower() == 'false': return f\"{col_expression} = false\"\n",
    "            # Numbers\n",
    "            if re.match(r'^\\d+(\\.\\d+)?$', val): return f\"{col_expression} = {val}\"\n",
    "            \n",
    "            # Strings -> Case Insensitive Equality\n",
    "            return f\"lower({col_expression}) = '{val.lower()}'\"\n",
    "\n",
    "        # 4. Handle Standard Operators (>, =, !=)\n",
    "        # Regex to separate Operator from Value\n",
    "        match = re.match(r'^(=|!=|<>|>|<|>=|<=)\\s*(.*)', s)\n",
    "        if match:\n",
    "            op = match.group(1)\n",
    "            val = match.group(2).strip()\n",
    "            \n",
    "            # If value is a string (quoted or text), handle case insensitivity\n",
    "            if val.startswith(\"'\") or val.startswith('\"'):\n",
    "                clean_val = val.strip(\"'\\\"\")\n",
    "                return f\"lower({col_expression}) {op} '{clean_val.lower()}'\"\n",
    "            elif not re.match(r'^\\d+(\\.\\d+)?$', val) and val.lower() not in ['true', 'false', 'null']:\n",
    "                 # Unquoted string value found\n",
    "                 return f\"lower({col_expression}) {op} '{val.lower()}'\"\n",
    "                 \n",
    "            # Boolean/Numeric operators return as is\n",
    "            return f\"{col_expression} {s}\"\n",
    "            \n",
    "        return None\n",
    "\n",
    "    def _translate_search_query_to_sql(self, viz):\n",
    "        answer = viz.get('answer', {})\n",
    "        query = answer.get('search_query', '')\n",
    "        viz_id = viz.get('id')\n",
    "        print(\"Viz_id\", viz_id)\n",
    "        print(\"Query\", query)\n",
    "        print(\"Answer\", answer)\n",
    "\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id)\n",
    "        \n",
    "        # --- START: TABLE NAME LOGIC ---\n",
    "      \n",
    "        if mapping is not None and pd.notna(mapping.get('common_sql_query')) and mapping.get('common_sql_query'):\n",
    "            # STRATEGY 1: Use the pre-built \"common_sql_query\".\n",
    "            print(f\"  INFO: Using 'common_sql_query' from mapping for viz {viz_id}\")\n",
    "            return mapping['common_sql_query'].strip().split('\\n')\n",
    "\n",
    "        if mapping is not None and pd.notna(mapping.get('search_query_final')) and mapping.get('search_query_final'):\n",
    "            # STRATEGY 2: Use the pre-built \"search_query_final\".\n",
    "            print(f\"  INFO: Using 'search_query_final' from mapping for viz {viz_id}\")\n",
    "            return mapping['search_query_final'].strip().split('\\n')\n",
    "        \n",
    "        # --- STRATEGY 3: Auto-generate SQL (The Fallback) ---\n",
    "        print(f\"  WARNING: No common/final SQL found. Auto-generating SQL for viz {viz_id}.\")\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_table_name_ToBeFilled'):\n",
    "            table_name = mapping['databricks_table_name_ToBeFilled']\n",
    "            print(f\"  INFO: Using mapped table name: {table_name}\")\n",
    "        else:\n",
    "            table_name = answer.get('tables', [{}])[0].get('name', 'your_source_table')\n",
    "            print(f\"  INFO: Using TML table name (fallback): {table_name}\")\n",
    "        \n",
    "\n",
    "\n",
    "        column_mapping = {}\n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                column_mapping = {}\n",
    "        \n",
    "        answer_cols = [col.get('name') for col in answer.get('answer_columns', []) if col.get('name')]\n",
    "        \n",
    "        select_clauses = []\n",
    "        group_by_cols = []\n",
    "        where_clauses = []\n",
    "        \n",
    "        for i, col_name in enumerate(answer_cols, 1):\n",
    "            if not col_name:\n",
    "                continue\n",
    "            \n",
    "            base_field = clean_field_name(col_name)\n",
    "            mapped_field = column_mapping.get(base_field, base_field)\n",
    "            mapped_col_name = column_mapping.get(col_name, col_name)\n",
    "            sanitized_alias = sanitize_alias_name(mapped_col_name)\n",
    "            sanitized_base_alias = sanitize_alias_name(base_field)\n",
    "            \n",
    "            # --- NEW: Apply Filters from Metadata Table ---\n",
    "            if not self.viz_filter_data.empty:\n",
    "                filter_row = self.viz_filter_data[\n",
    "                    (self.viz_filter_data['viz_id'] == viz_id) & \n",
    "                    (self.viz_filter_data['Sanitized_Column'] == sanitized_alias)\n",
    "                ]\n",
    "                \n",
    "                if not filter_row.empty:\n",
    "                    filter_val = filter_row.iloc[0]['Filter_Details']\n",
    "                    col_expr = f\"`{sanitized_alias}`\"\n",
    "                    sql_cond = self._get_sql_filter(col_expr, filter_val)\n",
    "                    if sql_cond:\n",
    "                        where_clauses.append(sql_cond)\n",
    "                        print(f\"  INFO: Added filter for {sanitized_alias}: {sql_cond}\")\n",
    "            # ---------------------------------------------\n",
    "\n",
    "            expression_found = False\n",
    "            for transform in EXPRESSION_TRANSFORMATIONS:\n",
    "                pattern_str = transform['pattern'] \n",
    "                target_sql = transform['target'] \n",
    "                regex_pattern = re.escape(pattern_str).replace(r'\\{field\\}', r'(.+)')\n",
    "                \n",
    "                match = re.match(regex_pattern, col_name, re.IGNORECASE)\n",
    "                \n",
    "                if match:\n",
    "                    field_inside_func = match.group(1)\n",
    "                    mapped_field_inside = column_mapping.get(field_inside_func, field_inside_func)\n",
    "                    sql_expression = target_sql.format(field=mapped_field_inside)\n",
    "                    sql_alias = sanitize_alias_name(column_mapping.get(col_name, col_name))                    \n",
    "                    select_clauses.append(f\"  {sql_expression} AS {sql_alias}\")\n",
    "                    group_by_cols.append(str(i))\n",
    "                    expression_found = True\n",
    "                    print(f\"  INFO: Transformed '{col_name}' -> '{sql_expression} AS {sql_alias}'\")\n",
    "                    break \n",
    "            \n",
    "            if not expression_found:\n",
    "                # No transformation matched. This is the fallback logic.\n",
    "                if 'Unique Number of' in col_name or 'unique number of' in col_name.lower():\n",
    "                     match_inner = re.match(r\"Unique Number of \\((.+)\\)\", col_name, re.IGNORECASE)\n",
    "                     if match_inner:\n",
    "                         field_inside = match_inner.group(1)\n",
    "                         mapped_field_inside = column_mapping.get(field_inside, field_inside)\n",
    "                         select_clauses.append(f\"  COUNT(DISTINCT {mapped_field_inside}) AS {sanitized_alias}\")\n",
    "                     else:\n",
    "                         select_clauses.append(f\"  COUNT(DISTINCT {mapped_field}) AS {sanitized_alias}\")\n",
    "                \n",
    "                elif any(p in col_name for p in ['Total ', 'sum(', 'count(', 'Sum(', 'Count(']):\n",
    "                    agg = 'COUNT' if 'count' in col_name.lower() else 'SUM'\n",
    "                    select_clauses.append(f\"  {agg}({mapped_field}) AS {sanitized_base_alias}\")\n",
    "                \n",
    "                else:\n",
    "                    select_clauses.append(f\"  {mapped_field} AS {sanitized_base_alias}\")\n",
    "                    group_by_cols.append(str(i))\n",
    "\n",
    "        if not select_clauses:\n",
    "            return [f\"SELECT '' AS placeholder FROM {table_name}\"]\n",
    "\n",
    "        if \". 'this month'\" in query:\n",
    "            where_clauses.append(\"DATE >= DATE_TRUNC('MONTH', CURRENT_TIMESTAMP())\")\n",
    "        if \". 'last 30 days'\" in query:\n",
    "            where_clauses.append(\"DATE >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)\")\n",
    "\n",
    "        order_by = \"\"\n",
    "        limit = \"\"\n",
    "        if 'top 10' in query.lower():\n",
    "            measure_idx = next((i for i, col in enumerate(answer_cols, 1) if any(p in col for p in ['Total', 'Unique', 'sum(', 'count('])), 1)\n",
    "            order_by = f\" ORDER BY {measure_idx} DESC\"\n",
    "            limit = \" LIMIT 10\"\n",
    "\n",
    "        sql_parts = [\"SELECT\", \",\\n\".join(select_clauses), f\" FROM {table_name} \"]\n",
    "        \n",
    "        if where_clauses:\n",
    "            sql_parts.append(f\" WHERE {' AND '.join(where_clauses)} \")\n",
    "        \n",
    "        has_agg = any(agg in ' '.join(select_clauses).upper() for agg in ['SUM(', 'COUNT(', 'AVG('])\n",
    "        if group_by_cols and has_agg:\n",
    "            sql_parts.append(f\" GROUP BY {', '.join(group_by_cols)} \")\n",
    "        \n",
    "        if order_by:\n",
    "            sql_parts.append(order_by)\n",
    "        if limit:\n",
    "            sql_parts.append(limit)\n",
    "        \n",
    "        return '\\n'.join(sql_parts).split('\\n')\n",
    "\n",
    "    def _create_dataset(self, viz):\n",
    "        answer = viz.get('answer', {})\n",
    "        print(\"create dataset inside\", f\"ds_{viz.get('id', generate_unique_id())}\" , \" \", answer.get('name', 'Untitled Dataset'),\" \", self._translate_search_query_to_sql(viz)  )\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"name\": f\"ds_{viz.get('id', generate_unique_id())}\",\n",
    "            \"displayName\": answer.get('name', 'Untitled Dataset'),\n",
    "            \"queryLines\": self._translate_search_query_to_sql(viz)\n",
    "        }\n",
    "\n",
    "    def _extract_fields_from_answer(self, answer, viz_id, widget_type=None):\n",
    "        print(f\"\\n--- Extracting Fields for Viz_ID: {viz_id} ---\")\n",
    "        column_details = self._get_column_details_for_viz(viz_id)\n",
    "        \n",
    "        if not column_details:\n",
    "            print(f\"  WARNING: No column details found for viz {viz_id}. Using fallback.\")\n",
    "            return self._extract_fields_fallback(answer, viz_id, widget_type)\n",
    "            \n",
    "        fields = []\n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        for col in answer_cols:\n",
    "            original_col_name = col.get('name')\n",
    "            if not original_col_name:\n",
    "                continue\n",
    "                \n",
    "            if original_col_name in column_details:\n",
    "                detail = column_details[original_col_name]\n",
    "                \n",
    "                # 1. Get Base Name & Expression\n",
    "                field_name = detail['sanitized']\n",
    "                \n",
    "                # Use expression from table or default to agg(base)\n",
    "                if detail['expression'] and str(detail['expression']).strip() != '':\n",
    "                    field_expression = detail['expression']\n",
    "                else:\n",
    "                    agg = detail['aggregation']\n",
    "                    base = detail['model_base_column']\n",
    "                    field_expression = f\"{agg}(`{base}`)\" if agg else f\"`{base}`\"\n",
    "\n",
    "                fields.append({\n",
    "                    \"name\": field_name,           \n",
    "                    \"expression\": field_expression \n",
    "                })\n",
    "            else:\n",
    "                # Fallback\n",
    "                base_field = clean_field_name(original_col_name)\n",
    "                sanitized_name = sanitize_alias_name(base_field)\n",
    "                fields.append({\n",
    "                    \"name\": sanitized_name, \n",
    "                    \"expression\": f\"`{sanitized_name}`\"\n",
    "                })\n",
    "                \n",
    "        return fields\n",
    "    \n",
    "    def _extract_filters_for_query(self, viz_id):\n",
    "        \"\"\"\n",
    "        Generates the 'filters' list for the Databricks query object \n",
    "        by looking up the viz_id in the loaded VIZ_FILTER_DATA table.\n",
    "        \"\"\"\n",
    "        filters = []\n",
    "        \n",
    "        # Safety check: Ensure the table data was loaded\n",
    "        if not hasattr(self, 'viz_filter_data') or self.viz_filter_data.empty:\n",
    "            return []\n",
    "\n",
    "        # 1. Filter the global table down to THIS specific viz_id\n",
    "        current_viz_filters = self.viz_filter_data[\n",
    "            self.viz_filter_data['viz_id'].astype(str) == str(viz_id)\n",
    "        ]\n",
    "        \n",
    "        if current_viz_filters.empty:\n",
    "            return []\n",
    "\n",
    "        # 2. Iterate through valid filter rows for this viz\n",
    "        for _, row in current_viz_filters.iterrows():\n",
    "            filter_str = row.get('Filter_Details')\n",
    "            col_name = row.get('Sanitized_Column')\n",
    "            \n",
    "            # Skip empty filters\n",
    "            if not filter_str or pd.isna(filter_str) or str(filter_str).lower() in ['null', 'nan', 'none', '']:\n",
    "                continue\n",
    "                \n",
    "            if not col_name:\n",
    "                continue\n",
    "\n",
    "            # 3. Generate SQL Condition\n",
    "            # Use the sanitized column name from the table as the reference\n",
    "            col_expr = f\"`{col_name}`\"\n",
    "            sql_condition = self._get_sql_filter(col_expr, filter_str)\n",
    "            \n",
    "            if sql_condition:\n",
    "                filters.append({\n",
    "                    \"expression\": sql_condition\n",
    "                })\n",
    "                print(f\"  ✓ Applied Filter to {col_name}: {sql_condition}\")\n",
    "        \n",
    "        return filters\n",
    "\n",
    "    def _extract_fields_fallback(self, answer, viz_id, widget_type=None):\n",
    "        print(f\"  INFO: Using fallback field extraction for viz {viz_id}\")\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id)\n",
    "        column_mapping = {}\n",
    "        if mapping is not None:\n",
    "            mapping_str_to_use = None\n",
    "            common_mapping_str = mapping.get('common_column_mapping')\n",
    "            specific_mapping_str = mapping.get('databricks_column_mapping_ToBeFilled')\n",
    "            if pd.notna(common_mapping_str) and common_mapping_str and common_mapping_str.strip() not in ['{}', '']:\n",
    "                mapping_str_to_use = common_mapping_str\n",
    "            elif pd.notna(specific_mapping_str) and specific_mapping_str and specific_mapping_str.strip() != '{}':\n",
    "                mapping_str_to_use = specific_mapping_str\n",
    "            if mapping_str_to_use:\n",
    "                try:\n",
    "                    column_mapping = json.loads(mapping_str_to_use)\n",
    "                except Exception as e:\n",
    "                    print(f\"  WARNING: Could not parse mapping: {e}\")\n",
    "        fields = []\n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        for col in answer_cols:\n",
    "            original_col_name = col.get('name')\n",
    "            if not original_col_name:\n",
    "                continue\n",
    "            sql_alias = column_mapping.get(original_col_name)\n",
    "            if not sql_alias:\n",
    "                base_field = clean_field_name(original_col_name)\n",
    "                sql_alias = column_mapping.get(base_field)\n",
    "            if not sql_alias:\n",
    "                base_field = clean_field_name(original_col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "            fields.append({\"name\": sql_alias, \"expression\": f\"`{sql_alias}`\"})\n",
    "        return fields\n",
    "\n",
    "\n",
    "    def _apply_expression_transformation(self, col_name, sql_alias, base_field, column_mapping):\n",
    "        \"\"\"\n",
    "        Apply expression transformation using configuration from EXPRESSION_TRANSFORMATIONS.\n",
    "        \n",
    "        Args:\n",
    "            col_name: Original column name from TML (e.g., \"Day(Event Date)\", \"Total Revenue\")\n",
    "            sql_alias: Sanitized SQL alias (e.g., \"Event_Date\", \"Revenue\")\n",
    "            base_field: Cleaned field name\n",
    "            column_mapping: Dictionary of column mappings\n",
    "            \n",
    "        Returns:\n",
    "            str: Transformed expression (e.g., \"DATE_TRUNC('DAY', Event_Date)\", \"SUM(Revenue)\")\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Try to match against configured expression patterns\n",
    "        for transform in EXPRESSION_TRANSFORMATIONS:\n",
    "            pattern = transform['pattern']\n",
    "            target_expr = transform['target']\n",
    "            \n",
    "            # Handle date functions: Day(field), Month(field), etc.\n",
    "            if pattern.startswith(('Day(', 'Week(', 'Month(', 'Year(')):\n",
    "                func_match = re.match(r\"(Day|Week|Month|Year)\\((.*?)\\)\", col_name, re.IGNORECASE)\n",
    "                if func_match:\n",
    "                    func_name, field_name = func_match.groups()\n",
    "                    db_field = column_mapping.get(field_name, field_name)\n",
    "                    sanitized_field = sanitize_alias_name(base_field)\n",
    "                    expression = target_expr.replace('field', sanitized_field)\n",
    "                    expression = expression.replace('DAY', func_name.upper())\n",
    "                    expression = expression.replace('WEEK', func_name.upper())\n",
    "                    expression = expression.replace('MONTH', func_name.upper())\n",
    "                    expression = expression.replace('YEAR', func_name.upper())\n",
    "                    \n",
    "                    return expression\n",
    "            \n",
    "\n",
    "            elif pattern.startswith(('sum(', 'count(', 'avg(', 'min(', 'max(')):\n",
    "                continue\n",
    "        \n",
    "        return f'`{sql_alias}`'\n",
    "        \n",
    "    def _create_shared_dataset(self, viz, mapping):\n",
    "        \"\"\"\n",
    "        Create a shared dataset using common SQL and column mapping from the mapping table.\n",
    "        \n",
    "        Args:\n",
    "            viz: Visualization object (used for fallback display name)\n",
    "            mapping: Mapping row with common_dataset_name, common_sql_query, common_column_mapping\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dataset object with name, displayName, and queryLines\n",
    "        \"\"\"\n",
    "        dataset_name = mapping['common_dataset_name']\n",
    "        \n",
    "        # Get SQL query from mapping table\n",
    "        if mapping.get('common_sql_query'):\n",
    "            sql_query = mapping['common_sql_query']\n",
    "            query_lines = sql_query.strip().split('\\n')\n",
    "        else:\n",
    "            # Fallback: generate SQL from viz if common_sql_query is empty\n",
    "            print(f\"  WARNING: No common_sql_query found for shared dataset '{dataset_name}', generating from viz\")\n",
    "            query_lines = self._translate_search_query_to_sql(viz)\n",
    "        \n",
    "\n",
    "        display_name = mapping.get('common_dataset_name', viz.get('answer', {}).get('name', 'Shared Dataset'))\n",
    "        \n",
    "        print(f\"  INFO: Shared dataset '{dataset_name}' created with {len(query_lines)} query lines\")\n",
    "        \n",
    "        return {\n",
    "            \"name\": dataset_name,\n",
    "            \"displayName\": display_name,\n",
    "            \"queryLines\": query_lines\n",
    "        }\n",
    "    def _get_base_frame(self, answer):\n",
    "        return {\n",
    "            \"title\": answer.get('name', 'Untitled'),\n",
    "            \"description\": answer.get('description', ''),\n",
    "            \"showTitle\": True,\n",
    "            \"showDescription\": bool(answer.get('description'))\n",
    "        }\n",
    "\n",
    "    def _get_bar_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"Generates bar/column chart spec, using original TML names for display.\"\"\"\n",
    "\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        colors = extract_colors_from_tml(chart) \n",
    "        mark = {}\n",
    "\n",
    "        tml_type = chart.get('type', '')\n",
    "        is_column = 'COLUMN' in tml_type.upper()\n",
    "        is_stacked = 'STACKED' in tml_type.upper()\n",
    "\n",
    "        print(f\"\\n=== {'COLUMN' if is_column else 'BAR'} CHART DEBUG for '{widget_name}' (viz_id: {viz_id}) ===\")\n",
    "        print(f\"TML Type: {tml_type}, Is Column: {is_column}, Is Stacked: {is_stacked}\")\n",
    "\n",
    "        # Extract Custom Axis Names ---\n",
    "        custom_x_axis_title = None\n",
    "        custom_y_axis_title = None\n",
    "        try:\n",
    "            client_state = json.loads(chart.get('client_state_v2', '{}'))\n",
    "            axis_props = client_state.get('axisProperties', [])\n",
    "            for prop in axis_props:\n",
    "                prop_props = prop.get('properties', {})\n",
    "                if prop_props.get('axisType') == 'X':\n",
    "                    custom_x_axis_title = prop_props.get('name')\n",
    "                elif prop_props.get('axisType') == 'Y':\n",
    "                    custom_y_axis_title = prop_props.get('name')\n",
    "            if custom_x_axis_title: print(f\"  Found custom X-Axis Title: '{custom_x_axis_title}'\")\n",
    "            if custom_y_axis_title: print(f\"  Found custom Y-Axis Title: '{custom_y_axis_title}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Could not parse client_state_v2 for custom axis names. {e}\")\n",
    "        \n",
    "        # --- Get Column Mapping ---\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id)\n",
    "        column_mapping = {} # Holds the JSON mapping dict if found\n",
    "        if mapping is not None:\n",
    "            # Logic to load either common or specific mapping JSON\n",
    "            common_mapping_str = mapping.get('common_column_mapping')\n",
    "            specific_mapping_str = mapping.get('databricks_column_mapping_ToBeFilled')\n",
    "            mapping_str_to_use = None\n",
    "            if pd.notna(common_mapping_str) and common_mapping_str and common_mapping_str.strip() != '{}':\n",
    "                 mapping_str_to_use = common_mapping_str\n",
    "                 print(\"  Using common_column_mapping for lookups.\")\n",
    "            elif pd.notna(specific_mapping_str) and specific_mapping_str and specific_mapping_str.strip() != '{}':\n",
    "                 mapping_str_to_use = specific_mapping_str\n",
    "                 print(\"  Using databricks_column_mapping_ToBeFilled for lookups.\")\n",
    "            if mapping_str_to_use:\n",
    "                try: \n",
    "                    column_mapping = json.loads(mapping_str_to_use)\n",
    "                    print(f\"  Column mapping loaded: {column_mapping}\")\n",
    "                except (json.JSONDecodeError, TypeError) as e: \n",
    "                    print(f\"  WARNING: Failed to parse column mapping JSON: {e}\")\n",
    "        else: \n",
    "            print(\"  WARNING: No mapping data found for this visualization.\")\n",
    "\n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        print(f\"Answer columns from TML: {[col.get('name') for col in answer_cols]}\")\n",
    "\n",
    "        # --- Determine Axis Roles ---\n",
    "        if is_column: category_axis_key, measure_axis_key = 'x', 'y'\n",
    "        else: category_axis_key, measure_axis_key = 'y', 'x'\n",
    "\n",
    "        tml_category_config = axis_config.get('x', [])\n",
    "        tml_measure_config = axis_config.get('y', [])\n",
    "        tml_color_config = axis_config.get('color', [])\n",
    "\n",
    "        print(f\"Category Axis Key: {category_axis_key}, Measure Axis Key: {measure_axis_key}\")\n",
    "        print(f\"TML Category Config: {tml_category_config}\")\n",
    "        print(f\"TML Measure Config: {tml_measure_config}\")\n",
    "        print(f\"TML Color Config: {tml_color_config}\")\n",
    "\n",
    "        # --- Process Category Axis (Dimension) ---\n",
    "        if tml_category_config:\n",
    "            original_field_cat = tml_category_config[0] # << CAPTURE ORIGINAL NAME FROM TML CONFIG\n",
    "            matching_cat_col = next((col for col in answer_cols if col.get('name') == original_field_cat or clean_field_name(col.get('name')) == clean_field_name(original_field_cat)), None)\n",
    "\n",
    "            if matching_cat_col:\n",
    "                cat_col_name_from_answer = matching_cat_col.get('name')\n",
    "                cat_base_field = clean_field_name(cat_col_name_from_answer)\n",
    "                cat_sql_alias = column_mapping.get(original_field_cat, sanitize_alias_name(cat_base_field))\n",
    "                print(f\"Category axis ({category_axis_key}): TML={original_field_cat}, FoundInAnswer={cat_col_name_from_answer}, MappedSQLAlias={cat_sql_alias}\")\n",
    "\n",
    "                encodings[category_axis_key] = {\n",
    "                    \"fieldName\": cat_sql_alias,                \n",
    "                    \"displayName\": original_field_cat,         \n",
    "                    \"scale\": {\"type\": infer_scale_type(cat_col_name_from_answer)},\n",
    "                    \"axis\": {\"title\": custom_x_axis_title or original_field_cat} \n",
    "                }\n",
    "                available.append(f'{category_axis_key}-axis (category)')\n",
    "            else:\n",
    "                print(f\"Category axis WARNING: No match in answer_cols for TML field '{original_field_cat}'\")\n",
    "                missing.append(f'{category_axis_key}-axis (category)')\n",
    "        else:\n",
    "             print(f\"Category axis WARNING: No TML config found.\")\n",
    "             missing.append(f'{category_axis_key}-axis (category)')\n",
    "\n",
    "        # --- Process Measure Axis ---\n",
    "        measure_fields_processed = [] \n",
    "        original_measure_names = []  \n",
    "        measure_axis_title = \"Value\"  \n",
    "\n",
    "        if tml_measure_config:\n",
    "            print(f\"Processing {len(tml_measure_config)} measure field(s)...\")\n",
    "            for original_field_measure in tml_measure_config: \n",
    "                original_measure_names.append(original_field_measure)\n",
    "                matching_measure_col = next((col for col in answer_cols if col.get('name') == original_field_measure or clean_field_name(col.get('name')) == clean_field_name(original_field_measure)), None)\n",
    "\n",
    "                if matching_measure_col:\n",
    "                    measure_col_name_from_answer = matching_measure_col.get('name')\n",
    "                    measure_base_field = clean_field_name(measure_col_name_from_answer)\n",
    "                    measure_sql_alias = column_mapping.get(original_field_measure, sanitize_alias_name(measure_base_field))\n",
    "                    print(f\"  - Measure field: TML={original_field_measure}, FoundInAnswer={measure_col_name_from_answer}, MappedSQLAlias={measure_sql_alias}\")\n",
    "                    measure_fields_processed.append({\"fieldName\": measure_sql_alias, \"originalName\": original_field_measure})\n",
    "                else:\n",
    "                    print(f\"Measure axis WARNING: No match in answer_cols for TML field '{original_field_measure}'\")\n",
    "                    missing.append(f'{measure_axis_key}-axis measure: {original_field_measure}')\n",
    "\n",
    "            # --- Construct Measure Axis Encoding ---\n",
    "            if len(measure_fields_processed) == 1:\n",
    "                measure_info = measure_fields_processed[0]\n",
    "                measure_axis_title = custom_y_axis_title or measure_info[\"originalName\"] \n",
    "                encodings[measure_axis_key] = {\n",
    "                    \"fieldName\": measure_info[\"fieldName\"],    \n",
    "                    \"displayName\": measure_info[\"originalName\"], \n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": measure_axis_title}      \n",
    "                }\n",
    "                available.append(f'{measure_axis_key}-axis (single measure)')\n",
    "                print(f\"Measure axis ({measure_axis_key}): Single field = {measure_info['fieldName']}\")\n",
    "            elif len(measure_fields_processed) > 1:\n",
    "                measure_axis_title = custom_y_axis_title or \", \".join(original_measure_names[:3]) + (\"...\" if len(original_measure_names) > 3 else \"\")\n",
    "                encodings[measure_axis_key] = {\n",
    "                    \"fields\": [{\"fieldName\": m[\"fieldName\"]} for m in measure_fields_processed], \n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": measure_axis_title} \n",
    "                }\n",
    "                available.append(f'{measure_axis_key}-axis ({len(measure_fields_processed)} measures)')\n",
    "                print(f\"Measure axis ({measure_axis_key}): Multiple fields = {[m['fieldName'] for m in measure_fields_processed]}\")\n",
    "            else: \n",
    "                 if not any(m.startswith(f'{measure_axis_key}-axis measure:') for m in missing):\n",
    "                    missing.append(f'{measure_axis_key}-axis (measure)')\n",
    "        else:\n",
    "             print(f\"Measure axis WARNING: No TML config found.\")\n",
    "             missing.append(f'{measure_axis_key}-axis (measure)')\n",
    "\n",
    "        # --- Process Color ---\n",
    "        color_dimension_present = False\n",
    "        if tml_color_config:\n",
    "            original_field_color = tml_color_config[0]\n",
    "            matching_color_col = next((col for col in answer_cols if col.get('name') == original_field_color or clean_field_name(col.get('name')) == clean_field_name(original_field_color)), None)\n",
    "\n",
    "            if matching_color_col:\n",
    "                color_col_name_from_answer = matching_color_col.get('name')\n",
    "                color_base_field = clean_field_name(color_col_name_from_answer)\n",
    "                color_sql_alias = column_mapping.get(original_field_color, sanitize_alias_name(color_base_field))\n",
    "                print(f\"Color axis: TML={original_field_color}, FoundInAnswer={color_col_name_from_answer}, MappedSQLAlias={color_sql_alias}\")\n",
    "\n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": color_sql_alias,        \n",
    "                    \"displayName\": original_field_color, \n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                \n",
    "                color_dimension_present = True\n",
    "                available.append('color')\n",
    "            else:\n",
    "                 print(f\"Color axis WARNING: No match in answer_cols for TML field '{original_field_color}'\")\n",
    "                 missing.append(f'color axis: {original_field_color}')\n",
    "        else:\n",
    "             print(\"Color axis: No color dimension specified in TML.\")\n",
    "\n",
    "        # --- Determine Stacking/Grouping in `mark` ---\n",
    "        if is_stacked:\n",
    "            if color_dimension_present: \n",
    "                mark = {\"stackType\": \"stacked\"}\n",
    "                available.append('stacking (by dimension)')\n",
    "                print(\"Mark: Stacking by color dimension\")\n",
    "            elif len(measure_fields_processed) > 1: \n",
    "                mark = {\"stackType\": \"stacked\"}\n",
    "                available.append('stacking (multiple measures)')\n",
    "                print(\"Mark: Stacking by multiple measures\")\n",
    "            else: \n",
    "                print(\"Mark: Single measure, no color dimension - No stacking needed.\")\n",
    "        else: \n",
    "            if color_dimension_present: \n",
    "                mark = {\"stackType\": \"grouped\"}\n",
    "                available.append('grouping (by dimension)')\n",
    "                print(\"Mark: Grouping by color dimension\")\n",
    "\n",
    "        if not mark and not color_dimension_present and colors:\n",
    "             print(f\"Mark: Applying first TML color (fallback): {colors[0]}\")\n",
    "             mark = {\"colors\": colors[:1]}\n",
    "\n",
    "        print(f\"Final encodings: {encodings}\")\n",
    "        print(f\"Final Mark: {mark}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "        return { \"widgetType\": \"bar\", \"version\": 3, \"frame\": self._get_base_frame(answer), \"encodings\": encodings, \"mark\": mark }\n",
    "\n",
    "    def _get_line_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"Generates line chart spec, using original TML names for display.\"\"\"\n",
    "\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        mark = {} \n",
    "        tml_type = chart.get('type', '')\n",
    "        is_stacked_area = 'STACKED_AREA' in tml_type.upper()\n",
    "\n",
    "        print(f\"\\n=== LINE/AREA CHART DEBUG for '{widget_name}' (viz_id: {viz_id}) ===\")\n",
    "        print(f\"TML Type: {tml_type}\")\n",
    "\n",
    "        # ---Extract Custom Axis Names ---\n",
    "        custom_x_axis_title = None\n",
    "        custom_y_axis_title = None\n",
    "        try:\n",
    "            client_state = json.loads(chart.get('client_state_v2', '{}'))\n",
    "            axis_props = client_state.get('axisProperties', [])\n",
    "            for prop in axis_props:\n",
    "                prop_props = prop.get('properties', {})\n",
    "                if prop_props.get('axisType') == 'X':\n",
    "                    custom_x_axis_title = prop_props.get('name')\n",
    "                elif prop_props.get('axisType') == 'Y':\n",
    "                    custom_y_axis_title = prop_props.get('name')\n",
    "            if custom_x_axis_title: print(f\"  Found custom X-Axis Title: '{custom_x_axis_title}'\")\n",
    "            if custom_y_axis_title: print(f\"  Found custom Y-Axis Title: '{custom_y_axis_title}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Could not parse client_state_v2 for custom axis names. {e}\")\n",
    "\n",
    "\n",
    "        # --- Get Column Mapping ---\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id)\n",
    "        column_mapping = {}\n",
    "        if mapping is not None:\n",
    "            common_mapping_str = mapping.get('common_column_mapping')\n",
    "            specific_mapping_str = mapping.get('databricks_column_mapping_ToBeFilled')\n",
    "            mapping_str_to_use = None\n",
    "            if pd.notna(common_mapping_str) and common_mapping_str and common_mapping_str.strip() != '{}':\n",
    "                 mapping_str_to_use = common_mapping_str\n",
    "                 print(\"  Using common_column_mapping for lookups.\")\n",
    "            elif pd.notna(specific_mapping_str) and specific_mapping_str and specific_mapping_str.strip() != '{}':\n",
    "                 mapping_str_to_use = specific_mapping_str\n",
    "                 print(\"  Using databricks_column_mapping_ToBeFilled for lookups.\")\n",
    "            if mapping_str_to_use:\n",
    "                try: column_mapping = json.loads(mapping_str_to_use)\n",
    "                except (json.JSONDecodeError, TypeError) as e: print(f\"  WARNING: Failed to parse column mapping JSON: {e}\")\n",
    "        else: print(\"  WARNING: No mapping data found for this visualization.\")\n",
    "\n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        print(f\"Answer columns from TML: {[col.get('name') for col in answer_cols]}\")\n",
    "\n",
    "        # --- TML Config Extraction ---\n",
    "        tml_x_config = axis_config.get('x', [])\n",
    "        tml_y_config = axis_config.get('y', [])\n",
    "        tml_color_config = axis_config.get('color', [])\n",
    "\n",
    "        print(f\"TML X Config: {tml_x_config}\")\n",
    "        print(f\"TML Y Config: {tml_y_config}\")\n",
    "        print(f\"TML Color Config: {tml_color_config}\")\n",
    "\n",
    "        # --- Process X-axis (Dimension - usually temporal) ---\n",
    "        if tml_x_config:\n",
    "            original_field_x = tml_x_config[0] \n",
    "            matching_x_col = next((col for col in answer_cols if col.get('name') == original_field_x or clean_field_name(col.get('name')) == clean_field_name(original_field_x)), None)\n",
    "\n",
    "            if matching_x_col:\n",
    "                x_col_name_from_answer = matching_x_col.get('name')\n",
    "                x_base_field = clean_field_name(x_col_name_from_answer)\n",
    "                x_sql_alias = column_mapping.get(original_field_x, sanitize_alias_name(x_base_field))\n",
    "                print(f\"X-axis: TML={original_field_x}, FoundInAnswer={x_col_name_from_answer}, MappedSQLAlias={x_sql_alias}\")\n",
    "\n",
    "                encodings['x'] = {\n",
    "                    \"fieldName\": x_sql_alias,\n",
    "                    \"displayName\": original_field_x,\n",
    "                    \"scale\": {\"type\": infer_scale_type(x_col_name_from_answer)},\n",
    "                    \"axis\": {\"title\": custom_x_axis_title or original_field_x}\n",
    "                }\n",
    "                available.append('x-axis')\n",
    "            else:\n",
    "                print(f\"X-axis WARNING: No match in answer_cols for TML field '{original_field_x}'\")\n",
    "                missing.append('x-axis')\n",
    "        else:\n",
    "            print(f\"X-axis WARNING: No TML config found.\")\n",
    "            missing.append('x-axis')\n",
    "\n",
    "        # --- Process Y-axis (One or More Measures) ---\n",
    "        measure_fields_processed = [] \n",
    "        original_y_names = []         \n",
    "        y_axis_title = \"Value\"        \n",
    "\n",
    "        if tml_y_config:\n",
    "            print(f\"Processing {len(tml_y_config)} Y-axis measure field(s)...\")\n",
    "            for original_field_y in tml_y_config: \n",
    "                original_y_names.append(original_field_y)\n",
    "                matching_y_col = next((col for col in answer_cols if col.get('name') == original_field_y or clean_field_name(col.get('name')) == clean_field_name(original_field_y)), None)\n",
    "\n",
    "                if matching_y_col:\n",
    "                    y_col_name_from_answer = matching_y_col.get('name')\n",
    "                    y_base_field = clean_field_name(y_col_name_from_answer)\n",
    "                    y_sql_alias = column_mapping.get(original_field_y, sanitize_alias_name(y_base_field))\n",
    "                    print(f\"  - Y-axis measure: TML={original_field_y}, FoundInAnswer={y_col_name_from_answer}, MappedSQLAlias={y_sql_alias}\")\n",
    "                    measure_fields_processed.append({\"fieldName\": y_sql_alias, \"originalName\": original_field_y})\n",
    "                else:\n",
    "                    print(f\"Y-axis WARNING: No match in answer_cols for TML field '{original_field_y}'\")\n",
    "                    missing.append(f'y-axis measure: {original_field_y}')\n",
    "\n",
    "            # --- Construct Y-axis Encoding ---\n",
    "            if len(measure_fields_processed) == 1:\n",
    "                measure_info = measure_fields_processed[0]\n",
    "                y_axis_title = custom_y_axis_title or measure_info[\"originalName\"] \n",
    "                encodings['y'] = {\n",
    "                    \"fieldName\": measure_info[\"fieldName\"],    \n",
    "                    \"displayName\": measure_info[\"originalName\"], \n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": y_axis_title}          \n",
    "                }\n",
    "                available.append('y-axis (single measure)')\n",
    "                print(f\"Y-axis: Single field = {measure_info['fieldName']}\")\n",
    "            elif len(measure_fields_processed) > 1:\n",
    "                y_axis_title = custom_y_axis_title or \", \".join(original_y_names[:3]) + (\"...\" if len(original_y_names) > 3 else \"\") \n",
    "                encodings['y'] = {\n",
    "                    \"fields\": [{\"fieldName\": m[\"fieldName\"]} for m in measure_fields_processed], \n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": y_axis_title} \n",
    "                }\n",
    "                available.append(f'y-axis ({len(measure_fields_processed)} measures)')\n",
    "                print(f\"Y-axis: Multiple fields = {[m['fieldName'] for m in measure_fields_processed]}\")\n",
    "            else: \n",
    "                 if not any(m.startswith('y-axis measure:') for m in missing):\n",
    "                    missing.append('y-axis (measure)')\n",
    "        else:\n",
    "             print(f\"Y-axis WARNING: No TML config found.\")\n",
    "             missing.append('y-axis (measure)')\n",
    "\n",
    "        # --- Process Color (Dimension for Multiple Lines) ---\n",
    "        color_dimension_present = False\n",
    "        if tml_color_config:\n",
    "            original_field_color = tml_color_config[0] \n",
    "            matching_color_col = next((col for col in answer_cols if col.get('name') == original_field_color or clean_field_name(col.get('name')) == clean_field_name(original_field_color)), None)\n",
    "\n",
    "            if matching_color_col:\n",
    "                color_col_name_from_answer = matching_color_col.get('name')\n",
    "                color_base_field = clean_field_name(color_col_name_from_answer)\n",
    "                color_sql_alias = column_mapping.get(original_field_color, sanitize_alias_name(color_base_field))\n",
    "                print(f\"Color axis: TML={original_field_color}, FoundInAnswer={color_col_name_from_answer}, MappedSQLAlias={color_sql_alias}\")\n",
    "\n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": color_sql_alias,\n",
    "                    \"displayName\": original_field_color, \n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                color_dimension_present = True\n",
    "                available.append('color (series)')\n",
    "            else:\n",
    "                 print(f\"Color axis WARNING: No match in answer_cols for TML field '{original_field_color}'\")\n",
    "                 missing.append(f'color axis: {original_field_color}')\n",
    "        else:\n",
    "             if len(measure_fields_processed) > 1:\n",
    "                 print(\"Color axis: No TML config, Databricks will use multiple Y-fields for color.\")\n",
    "                 available.append('color (implicit from measures)')\n",
    "             else:\n",
    "                 print(\"Color axis: No color dimension specified in TML.\")\n",
    "        \n",
    "        # Handle Stacked Area\n",
    "        if is_stacked_area:\n",
    "             if color_dimension_present or len(measure_fields_processed) > 1:\n",
    "                 mark = {\"stackType\": \"stacked\"}\n",
    "                 available.append('stacking (area)')\n",
    "                 print(\"Mark: Stacking enabled for area chart\")\n",
    "\n",
    "        print(f\"Final encodings: {encodings}\")\n",
    "        print(f\"Final Mark: {mark}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "        widget_type = \"area\" if \"AREA\" in tml_type.upper() else \"line\"\n",
    "\n",
    "        return {\n",
    "            \"widgetType\": widget_type,\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings,\n",
    "            \"mark\": mark \n",
    "        }\n",
    "\n",
    "    def _create_widget(self, viz, dataset_name):\n",
    "        answer = viz.get('answer', {})\n",
    "        chart = answer.get('chart', {})\n",
    "        viz_id = viz.get('id')\n",
    "        \n",
    "        display_mode = answer.get('display_mode', '')\n",
    "        tml_type = chart.get('type', 'TABLE_MODE' if display_mode == 'TABLE_MODE' else 'UNKNOWN')\n",
    "        \n",
    "        lvdash_type = TML_TO_LVDASH_MAPPING.get(tml_type)\n",
    "        if lvdash_type is None:\n",
    "            lvdash_type = 'table'\n",
    "            status_note = f\"Chart type '{tml_type}' has no direct mapping, using fallback\"\n",
    "        else:\n",
    "            status_note = \"\"\n",
    "        \n",
    "        available = []\n",
    "        missing = []\n",
    "        unmapped_props = []\n",
    "        \n",
    "        # Get spec builder\n",
    "        spec_builder = getattr(self, f'_get_{lvdash_type}_spec', self._get_table_spec)\n",
    "        spec_result = spec_builder(answer, chart, available, missing, unmapped_props, viz.get('id'))\n",
    "        \n",
    "        # 1. Extract Fields\n",
    "        fields = self._extract_fields_from_answer(answer, viz.get('id'))\n",
    "        \n",
    "        # 2. Extract Filters (NEW CALL)\n",
    "        query_filters = self._extract_filters_for_query(viz.get('id'))\n",
    "        \n",
    "        # Check for Tuple (Counter + Trend)\n",
    "        if isinstance(spec_result, tuple) and len(spec_result) == 2:\n",
    "            counter_spec, line_spec = spec_result\n",
    "            \n",
    "            self.tracker.add_record(self.tml_filename, answer.get('name', 'Unnamed'), tml_type, 'counter', 'COMPLETE', available, missing, unmapped_props, status_note + \" (with trend)\")\n",
    "            \n",
    "            counter_widget = {\n",
    "                \"viz_id\": viz_id,\n",
    "                \"viz_guid\": viz.get('viz_guid'),\n",
    "                \"widget\": {\n",
    "                    \"name\": f\"widget_{viz_id}_{generate_unique_id()}\",\n",
    "                    \"queries\": [{\n",
    "                        \"name\": \"main_query\",\n",
    "                        \"query\": {\n",
    "                            \"datasetName\": dataset_name,\n",
    "                            \"fields\": fields,\n",
    "                            \"disaggregated\": False,\n",
    "                            \"filters\": query_filters # <--- INJECTED HERE\n",
    "                        }\n",
    "                    }],\n",
    "                    \"spec\": counter_spec\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            line_widget = {\n",
    "                \"viz_id\": f\"{viz_id}_trend\",\n",
    "                \"viz_guid\": viz.get('viz_guid'),\n",
    "                \"is_trend_chart\": True,\n",
    "                \"parent_viz_id\": viz_id,\n",
    "                \"widget\": {\n",
    "                    \"name\": f\"widget_{viz_id}_trend_{generate_unique_id()}\",\n",
    "                    \"queries\": [{\n",
    "                        \"name\": \"main_query\",\n",
    "                        \"query\": {\n",
    "                            \"datasetName\": dataset_name,\n",
    "                            \"fields\": fields,\n",
    "                            \"disaggregated\": False,\n",
    "                            \"filters\": query_filters # <--- INJECTED HERE\n",
    "                        }\n",
    "                    }],\n",
    "                    \"spec\": line_spec\n",
    "                }\n",
    "            }\n",
    "            return [counter_widget, line_widget]\n",
    "        \n",
    "        # Normal single widget handling\n",
    "        self.tracker.add_record(self.tml_filename, answer.get('name', 'Unnamed'), tml_type, lvdash_type, 'COMPLETE', available, missing, unmapped_props, status_note)\n",
    "        \n",
    "        return {\n",
    "            \"viz_id\": viz_id,\n",
    "            \"viz_guid\": viz.get('viz_guid'),\n",
    "            \"widget\": {\n",
    "                \"name\": f\"widget_{viz_id}_{generate_unique_id()}\",\n",
    "                \"queries\": [{\n",
    "                    \"name\": \"main_query\",\n",
    "                    \"query\": {\n",
    "                        \"datasetName\": dataset_name,\n",
    "                        \"fields\": fields,\n",
    "                        \"disaggregated\": False,\n",
    "                        \"filters\": query_filters # <--- INJECTED HERE\n",
    "                    }\n",
    "                }],\n",
    "                \"spec\": spec_result  \n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _get_pie_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate pie chart specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Pie chart specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0] \n",
    "        encodings = {}\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass \n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Process Y-axis (angle/value) \n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            original_field = axis_config['y'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['angle'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"quantitative\"}\n",
    "                }\n",
    "                available.append('angle')\n",
    "            else:\n",
    "                missing.append('angle')\n",
    "        else:\n",
    "            missing.append('angle')\n",
    "        \n",
    "        # Process X-axis (color/category) \n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            original_field = axis_config['x'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                available.append('color')\n",
    "            else:\n",
    "                missing.append('color')\n",
    "        else:\n",
    "            missing.append('color')\n",
    "        \n",
    "        encodings['label'] = {\"show\": True}\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"pie\", \n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "    def _get_area_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None): \n",
    "        \"\"\"Generates area/stacked area chart spec, using original TML names for display.\"\"\"\n",
    "\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        mark = {} \n",
    "        tml_type = chart.get('type', '')\n",
    "        is_stacked = 'STACKED' in tml_type.upper()\n",
    "\n",
    "        print(f\"\\n=== AREA CHART DEBUG for '{widget_name}' (viz_id: {viz_id}) ===\")\n",
    "        print(f\"TML Type: {tml_type}, Is Stacked: {is_stacked}\")\n",
    "\n",
    "        #Extract Custom Axis Names \n",
    "        custom_x_axis_title = None\n",
    "        custom_y_axis_title = None\n",
    "        try:\n",
    "            client_state = json.loads(chart.get('client_state_v2', '{}'))\n",
    "            axis_props = client_state.get('axisProperties', [])\n",
    "            for prop in axis_props:\n",
    "                prop_props = prop.get('properties', {})\n",
    "                if prop_props.get('axisType') == 'X':\n",
    "                    custom_x_axis_title = prop_props.get('name')\n",
    "                elif prop_props.get('axisType') == 'Y':\n",
    "                    custom_y_axis_title = prop_props.get('name')\n",
    "            if custom_x_axis_title: print(f\"  Found custom X-Axis Title: '{custom_x_axis_title}'\")\n",
    "            if custom_y_axis_title: print(f\"  Found custom Y-Axis Title: '{custom_y_axis_title}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"  WARNING: Could not parse client_state_v2 for custom axis names. {e}\")\n",
    "\n",
    "        # Get Column Mapping ---\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id)\n",
    "        column_mapping = {}\n",
    "        if mapping is not None:\n",
    "            common_mapping_str = mapping.get('common_column_mapping')\n",
    "            specific_mapping_str = mapping.get('databricks_column_mapping_ToBeFilled')\n",
    "            mapping_str_to_use = None\n",
    "            if pd.notna(common_mapping_str) and common_mapping_str and common_mapping_str.strip() != '{}':\n",
    "                 mapping_str_to_use = common_mapping_str\n",
    "                 print(\"  Using common_column_mapping for lookups.\")\n",
    "            elif pd.notna(specific_mapping_str) and specific_mapping_str and specific_mapping_str.strip() != '{}':\n",
    "                 mapping_str_to_use = specific_mapping_str\n",
    "                 print(\"  Using databricks_column_mapping_ToBeFilled for lookups.\")\n",
    "            if mapping_str_to_use:\n",
    "                try: column_mapping = json.loads(mapping_str_to_use)\n",
    "                except (json.JSONDecodeError, TypeError) as e: print(f\"  WARNING: Failed to parse column mapping JSON: {e}\")\n",
    "        else: print(\"  WARNING: No mapping data found for this visualization.\")\n",
    "\n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        print(f\"Answer columns from TML: {[col.get('name') for col in answer_cols]}\")\n",
    "\n",
    "        #TML Config Extraction\n",
    "        tml_x_config = axis_config.get('x', [])\n",
    "        tml_y_config = axis_config.get('y', [])\n",
    "        tml_color_config = axis_config.get('color', [])\n",
    "\n",
    "        print(f\"TML X Config: {tml_x_config}\")\n",
    "        print(f\"TML Y Config: {tml_y_config}\")\n",
    "        print(f\"TML Color Config: {tml_color_config}\")\n",
    "\n",
    "        #Process X-axis (Dimension)\n",
    "        if tml_x_config:\n",
    "            original_field_x = tml_x_config[0] \n",
    "            matching_x_col = next((col for col in answer_cols if col.get('name') == original_field_x or clean_field_name(col.get('name')) == clean_field_name(original_field_x)), None)\n",
    "\n",
    "            if matching_x_col:\n",
    "                x_col_name_from_answer = matching_x_col.get('name')\n",
    "                x_base_field = clean_field_name(x_col_name_from_answer)\n",
    "                x_sql_alias = column_mapping.get(original_field_x, sanitize_alias_name(x_base_field))\n",
    "                print(f\"X-axis: TML={original_field_x}, FoundInAnswer={x_col_name_from_answer}, MappedSQLAlias={x_sql_alias}\")\n",
    "\n",
    "                encodings['x'] = {\n",
    "                    \"fieldName\": x_sql_alias,\n",
    "                    \"displayName\": original_field_x, \n",
    "                    \"scale\": {\"type\": infer_scale_type(x_col_name_from_answer)},\n",
    "                    \"axis\": {\"title\": custom_x_axis_title or original_field_x} \n",
    "                }\n",
    "                available.append('x-axis')\n",
    "            else:\n",
    "                print(f\"X-axis WARNING: No match in answer_cols for TML field '{original_field_x}'\")\n",
    "                missing.append('x-axis')\n",
    "        else:\n",
    "            print(f\"X-axis WARNING: No TML config found.\")\n",
    "            missing.append('x-axis')\n",
    "\n",
    "        #Process Y-axis (One or More Measures)\n",
    "        measure_fields_processed = [] \n",
    "        original_y_names = []         \n",
    "        y_axis_title = \"Value\"        \n",
    "\n",
    "        if tml_y_config:\n",
    "            print(f\"Processing {len(tml_y_config)} Y-axis measure field(s)...\")\n",
    "            for original_field_y in tml_y_config: \n",
    "                original_y_names.append(original_field_y)\n",
    "                matching_y_col = next((col for col in answer_cols if col.get('name') == original_field_y or clean_field_name(col.get('name')) == clean_field_name(original_field_y)), None)\n",
    "\n",
    "                if matching_y_col:\n",
    "                    y_col_name_from_answer = matching_y_col.get('name')\n",
    "                    y_base_field = clean_field_name(y_col_name_from_answer)\n",
    "                    y_sql_alias = column_mapping.get(original_field_y, sanitize_alias_name(y_base_field))\n",
    "                    print(f\"  - Y-axis measure: TML={original_field_y}, FoundInAnswer={y_col_name_from_answer}, MappedSQLAlias={y_sql_alias}\")\n",
    "                    measure_fields_processed.append({\"fieldName\": y_sql_alias, \"originalName\": original_field_y})\n",
    "                else:\n",
    "                    print(f\"Y-axis WARNING: No match in answer_cols for TML field '{original_field_y}'\")\n",
    "                    missing.append(f'y-axis measure: {original_field_y}')\n",
    "\n",
    "            #Construct Y-axis Encoding\n",
    "            if len(measure_fields_processed) == 1:\n",
    "                measure_info = measure_fields_processed[0]\n",
    "                y_axis_title = custom_y_axis_title or measure_info[\"originalName\"] \n",
    "                encodings['y'] = {\n",
    "                    \"fieldName\": measure_info[\"fieldName\"],    \n",
    "                    \"displayName\": measure_info[\"originalName\"], \n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": y_axis_title}          \n",
    "                }\n",
    "                available.append('y-axis (single measure)')\n",
    "                print(f\"Y-axis: Single field = {measure_info['fieldName']}\")\n",
    "            elif len(measure_fields_processed) > 1:\n",
    "                y_axis_title = custom_y_axis_title or \", \".join(original_y_names[:3]) + (\"...\" if len(original_y_names) > 3 else \"\") \n",
    "                encodings['y'] = {\n",
    "                    \"fields\": [{\"fieldName\": m[\"fieldName\"]} for m in measure_fields_processed], \n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": y_axis_title} \n",
    "                }\n",
    "                available.append(f'y-axis ({len(measure_fields_processed)} measures)')\n",
    "                print(f\"Y-axis: Multiple fields = {[m['fieldName'] for m in measure_fields_processed]}\")\n",
    "            else: \n",
    "                 if not any(m.startswith('y-axis measure:') for m in missing):\n",
    "                    missing.append('y-axis (measure)')\n",
    "        else:\n",
    "             print(f\"Y-axis WARNING: No TML config found.\")\n",
    "             missing.append('y-axis (measure)')\n",
    "\n",
    "        #Process Color (Dimension for Stacking)\n",
    "        color_dimension_present = False\n",
    "        if tml_color_config:\n",
    "            original_field_color = tml_color_config[0] \n",
    "            matching_color_col = next((col for col in answer_cols if col.get('name') == original_field_color or clean_field_name(col.get('name')) == clean_field_name(original_field_color)), None)\n",
    "\n",
    "            if matching_color_col:\n",
    "                color_col_name_from_answer = matching_color_col.get('name')\n",
    "                color_base_field = clean_field_name(color_col_name_from_answer)\n",
    "                color_sql_alias = column_mapping.get(original_field_color, sanitize_alias_name(color_base_field))\n",
    "                print(f\"Color axis: TML={original_field_color}, FoundInAnswer={color_col_name_from_answer}, MappedSQLAlias={color_sql_alias}\")\n",
    "\n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": color_sql_alias,\n",
    "                    \"displayName\": original_field_color, \n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                color_dimension_present = True\n",
    "                available.append('color (series)')\n",
    "            else:\n",
    "                 print(f\"Color axis WARNING: No match in answer_cols for TML field '{original_field_color}'\")\n",
    "                 missing.append(f'color axis: {original_field_color}')\n",
    "        else:\n",
    "             if len(measure_fields_processed) > 1:\n",
    "                 print(\"Color axis: No TML config, Databricks will use multiple Y-fields for color.\")\n",
    "                 available.append('color (implicit from measures)')\n",
    "             else:\n",
    "                 print(\"Color axis: No color dimension specified in TML.\")\n",
    "        \n",
    "        # Handle Stacking\n",
    "        if is_stacked:\n",
    "             if color_dimension_present or len(measure_fields_processed) > 1:\n",
    "                 mark = {\"stackType\": \"stacked\"}\n",
    "                 available.append('stacking (area)')\n",
    "                 print(\"Mark: Stacking enabled for area chart\")\n",
    "\n",
    "        print(f\"Final encodings: {encodings}\")\n",
    "        print(f\"Final Mark: {mark}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "        return {\n",
    "            \"widgetType\": \"area\",\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings,\n",
    "            \"mark\": mark \n",
    "        }\n",
    "\n",
    "    def _get_scatter_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate scatter plot specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Scatter plot specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass \n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Process X-axis\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            original_field = axis_config['x'][0]\n",
    "            \n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['x'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": infer_scale_type(base_field)},\n",
    "                    \"axis\": {\"title\": base_field}\n",
    "                }\n",
    "                available.append('x-axis')\n",
    "            else:\n",
    "                missing.append('x-axis')\n",
    "        else:\n",
    "            missing.append('x-axis')\n",
    "        \n",
    "        # Process Y-axis\n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            original_field = axis_config['y'][0]\n",
    "            \n",
    "            \n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['y'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"quantitative\"},\n",
    "                    \"axis\": {\"title\": base_field}\n",
    "                }\n",
    "                available.append('y-axis')\n",
    "            else:\n",
    "                missing.append('y-axis')\n",
    "        else:\n",
    "            missing.append('y-axis')\n",
    "        \n",
    "        # Process Color \n",
    "        if axis_config.get('color') and len(axis_config['color']) > 0:\n",
    "            original_field = axis_config['color'][0]\n",
    "            \n",
    "            \n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                available.append('color')\n",
    "        \n",
    "        # Process Size \n",
    "        if axis_config.get('size') and len(axis_config['size']) > 0:\n",
    "            original_field = axis_config['size'][0]\n",
    "            \n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['size'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"quantitative\"}\n",
    "                }\n",
    "                available.append('size')\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"scatter\",\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "    def _get_table_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate table specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Table specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass \n",
    "        \n",
    "        # Get ordered columns from table config or answer columns\n",
    "        ordered_cols = answer.get('table', {}).get('ordered_column_ids', [])\n",
    "        if not ordered_cols:\n",
    "            ordered_cols = [c.get('name') for c in answer.get('answer_columns', []) if c.get('name')]\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        columns = []\n",
    "        \n",
    "        if ordered_cols:\n",
    "            available.append('columns')\n",
    "            for col_name in ordered_cols:\n",
    "                if not col_name:\n",
    "                    continue\n",
    "                \n",
    "                # Find matching answer column for proper type inference\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    answer_col_name = col.get('name')\n",
    "                    if answer_col_name == col_name or clean_field_name(answer_col_name) == clean_field_name(col_name):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    base_field = clean_field_name(matching_col.get('name'))\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    # Infer display type from column type\n",
    "                    col_type = matching_col.get('type', 'string').lower()\n",
    "                    display_as = \"string\"\n",
    "                    if col_type in ['int', 'integer', 'long', 'bigint', 'float', 'double', 'decimal', 'numeric']:\n",
    "                        display_as = \"number\"\n",
    "                    elif col_type in ['date', 'timestamp', 'datetime']:\n",
    "                        display_as = \"datetime\"\n",
    "                    elif col_type in ['boolean', 'bool']:\n",
    "                        display_as = \"boolean\"\n",
    "                    \n",
    "                    columns.append({\n",
    "                        \"fieldName\": sql_alias,\n",
    "                        \"title\": base_field,\n",
    "                        \"visible\": True,\n",
    "                        \"alignContent\": \"left\",\n",
    "                        \"allowHTML\": False,\n",
    "                        \"displayAs\": display_as,\n",
    "                        \"type\": col_type if col_type else \"string\"\n",
    "                    })\n",
    "                else:\n",
    "                    # Fallback if column not found in answer_columns\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    columns.append({\n",
    "                        \"fieldName\": sql_alias,\n",
    "                        \"title\": base_field,\n",
    "                        \"visible\": True,\n",
    "                        \"alignContent\": \"left\",\n",
    "                        \"allowHTML\": False,\n",
    "                        \"displayAs\": \"string\",\n",
    "                        \"type\": \"string\"\n",
    "                    })\n",
    "        else:\n",
    "            missing.append('columns')\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"table\",\n",
    "            \"version\": 1,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": {\"columns\": columns},\n",
    "            \"allowHTMLByDefault\": False,\n",
    "            \"itemsPerPage\": 25\n",
    "        }\n",
    "\n",
    "    def _get_counter_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate counter/KPI specification with encodings matching SQL aliases.\n",
    "        \n",
    "        When temporal data is present, returns a TUPLE of (counter_spec, line_spec)\n",
    "        to create both a KPI counter and a trend line chart below it.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict OR tuple: Counter spec, or (counter_spec, line_spec) if temporal data exists\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  \n",
    "        \n",
    "        # Get answer columns and axis config\n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        axis_config = chart.get('axis_configs', [{}])[0] if chart.get('axis_configs') else {}\n",
    "        \n",
    "       \n",
    "        has_temporal_field = False\n",
    "        temporal_field_name = None\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            x_field = axis_config['x'][0]\n",
    "            \n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == x_field or clean_field_name(col_name) == clean_field_name(x_field):\n",
    "                    if any(keyword in col_name for keyword in ['Month(', 'Day(', 'Week(', 'Year(', 'Date']):\n",
    "                        has_temporal_field = True\n",
    "                        temporal_field_name = col_name\n",
    "                        print(f\"  INFO: Detected temporal field '{temporal_field_name}' for KPI trend visualization\")\n",
    "                    break\n",
    "        \n",
    "        value_field_alias = None\n",
    "        value_field_display = None\n",
    "        \n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            y_field = axis_config['y'][0]\n",
    "            \n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == y_field or clean_field_name(col_name) == clean_field_name(y_field):\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    value_field_alias = sanitize_alias_name(base_field)\n",
    "                    value_field_display = base_field\n",
    "                    break\n",
    "        \n",
    "        # If no value found from y-axis, find first non-temporal column\n",
    "        if not value_field_alias and answer_cols:\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name:\n",
    "                    if not any(keyword in col_name for keyword in ['Month(', 'Day(', 'Week(', 'Year(', 'Date']):\n",
    "                        base_field = clean_field_name(col_name)\n",
    "                        value_field_alias = sanitize_alias_name(base_field)\n",
    "                        value_field_display = base_field\n",
    "                        break\n",
    "        \n",
    "        if not value_field_alias:\n",
    "            missing.append('value')\n",
    "            return {\n",
    "                \"widgetType\": \"counter\",\n",
    "                \"version\": 2,\n",
    "                \"frame\": self._get_base_frame(answer),\n",
    "                \"encodings\": {}\n",
    "            }\n",
    "        \n",
    "        # BUILD COUNTER SPEC \n",
    "        counter_encodings = {\n",
    "            \"value\": {\n",
    "                \"fieldName\": value_field_alias,\n",
    "                \"displayName\": value_field_display or value_field_alias\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        counter_spec = {\n",
    "            \"widgetType\": \"counter\",\n",
    "            \"version\": 2,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": counter_encodings\n",
    "        }\n",
    "        \n",
    "        available.append('value')\n",
    "        \n",
    "        # If we have temporal data, CREATE SEPARATE LINE CHART for trend\n",
    "        if has_temporal_field and temporal_field_name:\n",
    "            print(f\"  INFO: Creating separate line chart for KPI trend using _get_line_spec\")\n",
    "            \n",
    "            # Create separate lists for line chart tracking\n",
    "            line_available = []\n",
    "            line_missing = []\n",
    "            line_unmapped = []\n",
    "            \n",
    "            # Call the existing line chart spec builder\n",
    "            line_spec = self._get_line_spec(answer, chart, line_available, line_missing, line_unmapped, viz_id)\n",
    "            \n",
    "            # Customize the frame to hide title for trend chart\n",
    "            line_spec['frame']['title'] = f\"{answer.get('name', 'Trend')} - Trend\"\n",
    "            line_spec['frame']['showTitle'] = False  # Hide title for cleaner look\n",
    "            \n",
    "            available.extend(line_available)\n",
    "            missing.extend(line_missing)\n",
    "            unmapped_props.extend(line_unmapped)\n",
    "            \n",
    "            # Return TUPLE: (counter, line chart)\n",
    "            return (counter_spec, line_spec)\n",
    "        \n",
    "        # No temporal data - return simple counter only\n",
    "        return counter_spec\n",
    "\n",
    "    def _get_heatmap_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate a normalized Databricks heatmap specification from a ThoughtSpot Answer.\n",
    "        \n",
    "        This version uses the RAW TML name for the user-visible legend title and\n",
    "        axis display names, matching the TML exactly.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns.\n",
    "            chart: Chart configuration with axis settings.\n",
    "            available: List to track successfully mapped encodings.\n",
    "            missing: List to track missing/failed encodings.\n",
    "            unmapped_props: Unused parameter for consistency.\n",
    "            viz_id: Optional visualization ID.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Databricks heatmap specification.\n",
    "        \"\"\"\n",
    "        \n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        encodings = {}\n",
    "\n",
    "        # --- Helper function for list-based axes (X and Y) ---\n",
    "        def get_list_axis_details(tml_axis_key):\n",
    "            \"\"\"\n",
    "            Finds column details for TML list-based axes (x, y).\n",
    "            Returns (raw_display_name, field_name) or (None, None).\n",
    "            \"\"\"\n",
    "            if column_mapping.get(tml_axis_key):\n",
    "                field_name = column_mapping[tml_axis_key]\n",
    "                matching_col = next(\n",
    "                    (col for col in answer_cols if sanitize_alias_name(clean_field_name(col.get('name'))) == field_name), \n",
    "                    None\n",
    "                )\n",
    "                raw_display_name = matching_col.get('name') if matching_col else field_name\n",
    "                return raw_display_name, field_name\n",
    "\n",
    "            if axis_config.get(tml_axis_key) and len(axis_config[tml_axis_key]) > 0:\n",
    "                original_field = axis_config[tml_axis_key][0]\n",
    "                matching_col = next(\n",
    "                    (col for col in answer_cols if \n",
    "                    col.get('name') == original_field or \n",
    "                    clean_field_name(col.get('name')) == clean_field_name(original_field)), \n",
    "                    None\n",
    "                )\n",
    "                if matching_col:\n",
    "                    raw_display_name = matching_col.get('name')\n",
    "                    field_name = sanitize_alias_name(clean_field_name(raw_display_name))\n",
    "                    return raw_display_name, field_name\n",
    "                    \n",
    "            return None, None\n",
    "\n",
    "        # --- Load Column Mapping ---\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass \n",
    "\n",
    "        # -- Process X-Axis ---\n",
    "        x_display, x_field = get_list_axis_details(\"x\")\n",
    "        if x_field:\n",
    "            encodings['x'] = {\n",
    "                \"fieldName\": x_field,\n",
    "                \"displayName\": x_display, \n",
    "                \"scale\": {\n",
    "                    \"type\": infer_scale_type(x_display),\n",
    "                    \"sort\": {\"by\": \"natural-order\"}\n",
    "                }\n",
    "            }\n",
    "            available.append('x')\n",
    "        else:\n",
    "            missing.append('x')\n",
    "\n",
    "        # -- Process Y-Axis ---\n",
    "        y_display, y_field = get_list_axis_details(\"y\")\n",
    "        if y_field:\n",
    "            encodings['y'] = {\n",
    "                \"fieldName\": y_field,\n",
    "                \"displayName\": y_display, \n",
    "                \"scale\": {\n",
    "                    \"type\": infer_scale_type(y_display),\n",
    "                    \"sort\": {\"by\": \"natural-order-reversed\"}\n",
    "                }\n",
    "            }\n",
    "            available.append('y')\n",
    "        else:\n",
    "            missing.append('y')\n",
    "\n",
    "        # --- Process Color/Measure \n",
    "        original_field = None\n",
    "        measure_key = None\n",
    "        color_field = None\n",
    "        color_display = None\n",
    "        # Check for mapping override first\n",
    "        if column_mapping.get('color'):\n",
    "            color_field = column_mapping['color']\n",
    "            measure_key = 'color'\n",
    "            matching_col = next((col for col in answer_cols if sanitize_alias_name(clean_field_name(col.get('name'))) == color_field), None)\n",
    "            color_display = matching_col.get('name') if matching_col else color_field\n",
    "        \n",
    "        # If no override, parse TML\n",
    "        if not color_field:\n",
    "            if axis_config.get('color') and len(axis_config['color']) > 0:\n",
    "                original_field = axis_config['color'][0]\n",
    "                measure_key = 'color'\n",
    "            elif axis_config.get('size') and isinstance(axis_config['size'], str):\n",
    "                original_field = axis_config['size']\n",
    "                measure_key = 'size'\n",
    "\n",
    "            if original_field:\n",
    "                matching_col = next(\n",
    "                    (col for col in answer_cols if \n",
    "                    col.get('name') == original_field or \n",
    "                    clean_field_name(col.get('name')) == clean_field_name(original_field)), \n",
    "                    None\n",
    "                )\n",
    "                if matching_col:\n",
    "                    color_display = matching_col.get('name')\n",
    "                    color_field = sanitize_alias_name(clean_field_name(color_display))\n",
    "\n",
    "        #-- Add to encodings if successful\n",
    "        if color_field:\n",
    "            encodings['color'] = {\n",
    "                \"fieldName\": color_field, \n",
    "                \"scale\": {\"type\": \"quantitative\"},\n",
    "                \"legend\": {\n",
    "                    \"hideTitle\": False,\n",
    "                    \"title\": color_display,\n",
    "                    \"position\": \"bottom\",\n",
    "                    \"hide\": False\n",
    "                }\n",
    "            }\n",
    "            if measure_key:\n",
    "                available.append(measure_key)\n",
    "        else:\n",
    "            missing.append('measure (color/size)')\n",
    "\n",
    "        # -- Process Data Labels ---\n",
    "        try:\n",
    "            client_state_str = chart.get('client_state_v2', '{}')\n",
    "            client_state = json.loads(client_state_str)\n",
    "            chart_props = client_state.get('chartProperties', {})\n",
    "            \n",
    "            if chart_props.get('allLabels') is True:\n",
    "                encodings['label'] = {\"show\": True}\n",
    "                available.append('label')\n",
    "                \n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            pass \n",
    "\n",
    "        # -- Return the DB Spec ---\n",
    "        return {\n",
    "            \"widgetType\": \"heatmap\",\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "    def _get_choropleth_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate choropleth (geo map) specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Choropleth specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  # Continue with empty mapping\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Process X-axis (geographic field)\n",
    "        x_axis_config = axis_config.get('x')\n",
    "        if x_axis_config and len(x_axis_config) > 0:\n",
    "            original_field = x_axis_config[0]\n",
    "            \n",
    "            if original_field is None:\n",
    "                missing.append('geographic field')\n",
    "            else:\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name is None:\n",
    "                        continue\n",
    "                    \n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    sql_alias_lower = sql_alias.lower()\n",
    "                    \n",
    "                    if 'state' in sql_alias_lower or 'province' in sql_alias_lower:\n",
    "                        if 'code' in sql_alias_lower or 'abbr' in sql_alias_lower or 'iso' in sql_alias_lower:\n",
    "                            geo_role = \"admin1-iso-a2\"\n",
    "                        else:\n",
    "                            geo_role = \"admin1-name\"\n",
    "                        \n",
    "                        encodings['region'] = {\n",
    "                            \"regionType\": \"mapbox-v4-admin\",\n",
    "                            \"admin1\": {\n",
    "                                \"fieldName\": sql_alias,\n",
    "                                \"type\": \"field\",\n",
    "                                \"geographicRole\": geo_role\n",
    "                            }\n",
    "                        }\n",
    "                    else:\n",
    "                        if 'iso2' in sql_alias_lower or sql_alias_lower.endswith('_2'):\n",
    "                            geo_role = \"admin0-iso-a2\"\n",
    "                        elif 'iso3' in sql_alias_lower or 'code' in sql_alias_lower or sql_alias_lower.endswith('_3'):\n",
    "                            geo_role = \"admin0-iso-3166-1-alpha-3\"\n",
    "                        else:\n",
    "                            geo_role = \"admin0-name\"\n",
    "\n",
    "                        encodings['region'] = {\n",
    "                            \"regionType\": \"mapbox-v4-admin\",\n",
    "                            \"admin0\": {\n",
    "                                \"fieldName\": sql_alias,\n",
    "                                \"type\": \"field\",\n",
    "                                \"geographicRole\": geo_role\n",
    "                            }\n",
    "                        }\n",
    "                    \n",
    "                    available.append('geographic field')\n",
    "                else:\n",
    "                    missing.append('geographic field')\n",
    "        else:\n",
    "            missing.append('geographic field')\n",
    "        \n",
    "        # Process Y-axis (value field)\n",
    "        y_axis_config = axis_config.get('y')\n",
    "        if y_axis_config and len(y_axis_config) > 0:\n",
    "            original_field = y_axis_config[0]\n",
    "            \n",
    "            if original_field is None:\n",
    "                missing.append('value')\n",
    "            else:\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name is None:\n",
    "                        continue\n",
    "                    \n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    encodings['color'] = {\n",
    "                        \"fieldName\": sql_alias,\n",
    "                        \"scale\": {\n",
    "                            \"type\": \"quantitative\",\n",
    "                            \"colorRamp\": {\n",
    "                                \"mode\": \"scheme\",\n",
    "                                \"scheme\": \"blues\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                    available.append('value')\n",
    "                else:\n",
    "                    missing.append('value')\n",
    "        else:\n",
    "            missing.append('value')\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"choropleth-map\",\n",
    "            \"version\": 1,\n",
    "            \"frame\": {\n",
    "                \"title\": widget_name,\n",
    "                \"showTitle\": True\n",
    "            },\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "    def _get_funnel_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate funnel specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Funnel specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass  \n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Process X-axis \n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            original_field = axis_config['x'][0]\n",
    "            \n",
    "            # Find matching answer column\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['x'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                available.append('stage')\n",
    "            else:\n",
    "                missing.append('stage')\n",
    "        else:\n",
    "            missing.append('stage')\n",
    "        \n",
    "        # Process Y-axis\n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            original_field = axis_config['y'][0]\n",
    "            \n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['y'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"quantitative\"}\n",
    "                }\n",
    "                available.append('value')\n",
    "            else:\n",
    "                missing.append('value')\n",
    "        else:\n",
    "            missing.append('value')\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"funnel\",\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "    def _get_sankey_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate Sankey diagram specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Sankey charts require:\n",
    "        - stages: Array of categorical fields representing the flow nodes\n",
    "        - value: Quantitative field for flow thickness/weight\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: Unused parameter for consistency\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Sankey specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        \n",
    "        print(f\"\\n=== SANKEY CHART DEBUG for '{widget_name}' (viz_id: {viz_id}) ===\")\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "                print(f\"Column mapping loaded: {column_mapping}\")\n",
    "            except (json.JSONDecodeError, TypeError) as e:\n",
    "                print(f\"Failed to parse column mapping: {e}\")\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Collect stage fields (the flow nodes)\n",
    "        stages = []\n",
    "        \n",
    "        # Process X-axis fields (typically source and intermediate nodes)\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            for original_field in axis_config['x']:\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    print(f\"Stage field: original={original_field}, col_name={col_name}, base={base_field}, sql_alias={sql_alias}\")\n",
    "                    \n",
    "                    stages.append({\n",
    "                        \"fieldName\": sql_alias\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"WARNING: Could not find matching column for stage field {original_field}\")\n",
    "        \n",
    "        # Process Color axis fields \n",
    "        if axis_config.get('color') and len(axis_config['color']) > 0:\n",
    "            for original_field in axis_config['color']:\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    print(f\"Stage field (from color): original={original_field}, col_name={col_name}, base={base_field}, sql_alias={sql_alias}\")\n",
    "                    \n",
    "                    # Avoid duplicates\n",
    "                    if not any(s['fieldName'] == sql_alias for s in stages):\n",
    "                        stages.append({\n",
    "                            \"fieldName\": sql_alias\n",
    "                        })\n",
    "                else:\n",
    "                    print(f\"WARNING: Could not find matching column for stage field {original_field}\")\n",
    "        \n",
    "        if stages:\n",
    "            encodings['stages'] = stages\n",
    "            available.append(f'stages ({len(stages)})')\n",
    "            print(f\"Added {len(stages)} stage fields\")\n",
    "        else:\n",
    "            missing.append('stages')\n",
    "            print(\"WARNING: No stage fields found for Sankey\")\n",
    "        \n",
    "        # Process Y-axis\n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            original_field = axis_config['y'][0]\n",
    "            \n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                print(f\"Value field: original={original_field}, col_name={col_name}, base={base_field}, sql_alias={sql_alias}\")\n",
    "                \n",
    "                encodings['value'] = {\n",
    "                    \"fieldName\": sql_alias\n",
    "                }\n",
    "                available.append('value')\n",
    "            else:\n",
    "                print(f\"WARNING: Could not find matching column for value field {original_field}\")\n",
    "                missing.append('value')\n",
    "        else:\n",
    "            missing.append('value')\n",
    "        \n",
    "        print(f\"Final Sankey encodings: stages={[s['fieldName'] for s in encodings.get('stages', [])]}, value={encodings.get('value', {}).get('fieldName')}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"sankey\",\n",
    "            \"version\": 1, \n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "\n",
    "    def _get_pivot_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate pivot table specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: List to track optional unmapped properties\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Pivot table specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass \n",
    "        \n",
    "        # Get ordered columns\n",
    "        ordered_cols = answer.get('table', {}).get('ordered_column_ids', [])\n",
    "        if not ordered_cols:\n",
    "            ordered_cols = [c.get('name') for c in answer.get('answer_columns', []) if c.get('name')]\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        axis_config = chart.get('axis_configs', [{}])[0] if chart.get('axis_configs') else {}\n",
    "        \n",
    "        encodings = {}\n",
    "        \n",
    "        # Process Rows (x-axis)\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            rows = []\n",
    "            for original_field in axis_config['x']:\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    rows.append({\"fieldName\": sql_alias, \"displayName\": base_field})\n",
    "                else:\n",
    "                    # Fallback if no match found\n",
    "                    base_field = clean_field_name(original_field)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    rows.append({\"fieldName\": sql_alias, \"displayName\": base_field})\n",
    "            \n",
    "            encodings['rows'] = rows\n",
    "            available.append('rows')\n",
    "        elif ordered_cols:\n",
    "            original_field = ordered_cols[0]\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                encodings['rows'] = [{\"fieldName\": sql_alias, \"displayName\": base_field}]\n",
    "            else:\n",
    "                base_field = clean_field_name(original_field)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                encodings['rows'] = [{\"fieldName\": sql_alias, \"displayName\": base_field}]\n",
    "            available.append('rows (inferred)')\n",
    "        else:\n",
    "            missing.append('rows')\n",
    "        \n",
    "        # Process Columns\n",
    "        if axis_config.get('color') and len(axis_config['color']) > 0:\n",
    "            columns = []\n",
    "            for original_field in axis_config['color']:\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    columns.append({\"fieldName\": sql_alias, \"displayName\": base_field})\n",
    "                else:\n",
    "                    # Fallback if no match found\n",
    "                    base_field = clean_field_name(original_field)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    columns.append({\"fieldName\": sql_alias, \"displayName\": base_field})\n",
    "            \n",
    "            encodings['columns'] = columns\n",
    "            available.append('columns')\n",
    "        else:\n",
    "            unmapped_props.append('columns (pivot)')\n",
    "        \n",
    "        # Process Values (y-axis) \n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            fields = []\n",
    "            for original_field in axis_config['y']:\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    # Use the field name directly without wrapping in aggregation\n",
    "                    fields.append({\n",
    "                        \"fieldName\": sql_alias,  \n",
    "                        \"cellType\": \"text\"\n",
    "                    })\n",
    "                else:\n",
    "                    # Fallback if no match found\n",
    "                    base_field = clean_field_name(original_field)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    fields.append({\n",
    "                        \"fieldName\": sql_alias,  \n",
    "                        \"cellType\": \"text\"\n",
    "                    })\n",
    "            \n",
    "            encodings['cell'] = {\n",
    "                \"type\": \"multi-cell\",\n",
    "                \"fields\": fields\n",
    "            }\n",
    "            available.append('cell')\n",
    "        elif len(ordered_cols) > 1:\n",
    "            fields = []\n",
    "            for original_field in ordered_cols[1:]:\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    fields.append({\n",
    "                        \"fieldName\": sql_alias,  \n",
    "                        \"cellType\": \"text\"\n",
    "                    })\n",
    "                else:\n",
    "                    base_field = clean_field_name(original_field)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    \n",
    "                    fields.append({\n",
    "                        \"fieldName\": sql_alias, \n",
    "                        \"cellType\": \"text\"\n",
    "                    })\n",
    "            \n",
    "            encodings['cell'] = {\n",
    "                \"type\": \"multi-cell\",\n",
    "                \"fields\": fields\n",
    "            }\n",
    "            available.append('cell (inferred)')\n",
    "        else:\n",
    "            missing.append('cell')\n",
    "        return {\n",
    "        \"widgetType\": \"pivot\",\n",
    "        \"version\": 3,\n",
    "        \"frame\": self._get_base_frame(answer),\n",
    "        \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "\n",
    "    def _get_combo_spec(self, answer, chart, available, missing, unmapped_props, viz_id=None):\n",
    "        \"\"\"\n",
    "        Generate combo chart specification with encodings matching SQL aliases.\n",
    "        \n",
    "        Args:\n",
    "            answer: Answer object containing chart data and columns\n",
    "            chart: Chart configuration with axis settings\n",
    "            available: List to track successfully mapped encodings\n",
    "            missing: List to track missing/failed encodings\n",
    "            unmapped_props: List to track optional unmapped properties\n",
    "            viz_id: Optional visualization ID for column mapping lookup\n",
    "            \n",
    "        Returns:\n",
    "            dict: Combo chart specification with widgetType, version, frame, and encodings\n",
    "        \"\"\"\n",
    "        widget_name = answer.get('name', 'Unknown')\n",
    "        axis_config = chart.get('axis_configs', [{}])[0]\n",
    "        encodings = {}\n",
    "        \n",
    "        # Load column mapping if available\n",
    "        column_mapping = {}\n",
    "        mapping = self._get_mapping_for_viz(viz_id)\n",
    "        column_details = self._get_column_details_for_viz(viz_id) if viz_id else None\n",
    "        \n",
    "        if mapping is not None and mapping.get('databricks_column_mapping_ToBeFilled'):\n",
    "            try:\n",
    "                column_mapping = json.loads(mapping['databricks_column_mapping_ToBeFilled'])\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                pass\n",
    "        \n",
    "        answer_cols = answer.get('answer_columns', [])\n",
    "        \n",
    "        # Process X-axis\n",
    "        if axis_config.get('x') and len(axis_config['x']) > 0:\n",
    "            original_field = axis_config['x'][0]\n",
    "\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['x'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": infer_scale_type(base_field)},\n",
    "                    \"axis\": {\"title\": base_field}\n",
    "                }\n",
    "                available.append('x-axis')\n",
    "            else:\n",
    "                missing.append('x-axis')\n",
    "        else:\n",
    "            missing.append('x-axis')\n",
    "        \n",
    "        # Process Y-axis \n",
    "        if axis_config.get('y') and len(axis_config['y']) > 0:\n",
    "            y_fields = axis_config['y']\n",
    "            transformed_fields = []\n",
    "            \n",
    "            for original_field in y_fields:\n",
    "                matching_col = None\n",
    "                for col in answer_cols:\n",
    "                    col_name = col.get('name')\n",
    "                    if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                        matching_col = col\n",
    "                        break\n",
    "                \n",
    "                if matching_col:\n",
    "                    col_name = matching_col.get('name')\n",
    "                    base_field = clean_field_name(col_name)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    transformed_fields.append(sql_alias)\n",
    "                else:\n",
    "                    # Fallback if no match found\n",
    "                    base_field = clean_field_name(original_field)\n",
    "                    sql_alias = sanitize_alias_name(base_field)\n",
    "                    transformed_fields.append(sql_alias)\n",
    "            \n",
    "            encodings['y'] = {\n",
    "                \"fieldName\": transformed_fields,\n",
    "                \"displayName\": \"Value\",\n",
    "                \"scale\": {\"type\": \"quantitative\"},\n",
    "                \"axis\": {\"title\": \"Value\"}\n",
    "            }\n",
    "            available.append('y-axis')\n",
    "        else:\n",
    "            missing.append('y-axis')\n",
    "        \n",
    "        # Process Color \n",
    "        if axis_config.get('color') and len(axis_config['color']) > 0:\n",
    "            original_field = axis_config['color'][0]\n",
    "\n",
    "            matching_col = None\n",
    "            for col in answer_cols:\n",
    "                col_name = col.get('name')\n",
    "                if col_name == original_field or clean_field_name(col_name) == clean_field_name(original_field):\n",
    "                    matching_col = col\n",
    "                    break\n",
    "            \n",
    "            if matching_col:\n",
    "                col_name = matching_col.get('name')\n",
    "                base_field = clean_field_name(col_name)\n",
    "                sql_alias = sanitize_alias_name(base_field)\n",
    "                \n",
    "                encodings['color'] = {\n",
    "                    \"fieldName\": sql_alias,\n",
    "                    \"displayName\": base_field,\n",
    "                    \"scale\": {\"type\": \"categorical\"}\n",
    "                }\n",
    "                available.append('color (series)')\n",
    "        \n",
    "        unmapped_props.append('series chart type mapping (combo charts partially supported)')\n",
    "        \n",
    "        return {\n",
    "            \"widgetType\": \"combo\",\n",
    "            \"version\": 3,\n",
    "            \"frame\": self._get_base_frame(answer),\n",
    "            \"encodings\": encodings\n",
    "        }\n",
    "\n",
    "    def _get_sql_filter(self, col_expression, filter_str):\n",
    "        \"\"\"\n",
    "        Converts TML shorthand into SQL conditions.\n",
    "        Generic solution: Uses lower() for string comparisons to handle case mismatches.\n",
    "        \"\"\"\n",
    "        if not filter_str or pd.isna(filter_str) or str(filter_str).lower() in ['nan', 'null', 'none']:\n",
    "            return None\n",
    "        s = str(filter_str).strip()\n",
    "\n",
    "        # 1. Handle Null Checks\n",
    "        if \"!= '{null}'\" in s: return f\"{col_expression} IS NOT NULL\"\n",
    "        if \"= '{null}'\" in s: return f\"{col_expression} IS NULL\"\n",
    "\n",
    "        # --- Helper to process individual values ---\n",
    "        def process_val(val_chunk):\n",
    "            val = val_chunk.strip()\n",
    "            if val.startswith('.'): val = val[1:] # Remove leading dot\n",
    "            \n",
    "            # Remove existing quotes to check raw content\n",
    "            clean_val = val.strip(\"'\\\"\")\n",
    "            \n",
    "            # Check for Boolean/Numbers (Return raw value, IsString=False)\n",
    "            if clean_val.lower() == 'true': return 'TRUE', False\n",
    "            if clean_val.lower() == 'false': return 'FALSE', False\n",
    "            if re.match(r'^\\d+(\\.\\d+)?$', clean_val): return clean_val, False\n",
    "            \n",
    "            # It's a string: Lowercase it and quote it (Return value, IsString=True)\n",
    "            return f\"'{clean_val.lower()}'\", True\n",
    "        # -------------------------------------------\n",
    "\n",
    "        # 2. Handle Lists (comma separated) -> IN clause\n",
    "        if \",\" in s:\n",
    "            parts = []\n",
    "            has_strings = False\n",
    "            # Split by comma\n",
    "            raw_parts = s.split(',')\n",
    "            \n",
    "            for p in raw_parts:\n",
    "                p_val, is_str = process_val(p)\n",
    "                parts.append(p_val)\n",
    "                if is_str: has_strings = True\n",
    "            \n",
    "            joined_values = \", \".join(parts)\n",
    "            \n",
    "            # Generic logic: if any value is a string, lower() the column\n",
    "            if has_strings:\n",
    "                return f\"lower({col_expression}) IN ({joined_values})\"\n",
    "            else:\n",
    "                return f\"{col_expression} IN ({joined_values})\"\n",
    "\n",
    "        # 3. Handle Single Values (Dot notation or Operators)\n",
    "        \n",
    "        # Check standard operators first (>, <, =, !=)\n",
    "        match_op = re.match(r'^(=|!=|<>|>|<|>=|<=)\\s*(.*)', s)\n",
    "        if match_op:\n",
    "            op = match_op.group(1)\n",
    "            raw_val = match_op.group(2)\n",
    "            p_val, is_str = process_val(raw_val)\n",
    "            \n",
    "            if is_str:\n",
    "                return f\"lower({col_expression}) {op} {p_val}\"\n",
    "            else:\n",
    "                return f\"{col_expression} {op} {p_val}\"\n",
    "        \n",
    "        # Default: Dot notation (Implicit IN/Equality)\n",
    "        p_val, is_str = process_val(s)\n",
    "        if is_str:\n",
    "             return f\"lower({col_expression}) IN ({p_val})\"\n",
    "        else:\n",
    "             return f\"{col_expression} IN ({p_val})\"\n",
    "\n",
    "        def clean_and_format_value(val_str):\n",
    "            \"\"\"Cleans value and applies mapping or title casing.\"\"\"\n",
    "            raw = val_str.strip()\n",
    "            # Remove leading dot if present\n",
    "            if raw.startswith('.'): raw = raw[1:]\n",
    "            \n",
    "            # Remove existing quotes for checking\n",
    "            core_val = raw.strip(\"'\\\"\")\n",
    "            \n",
    "            # Check for Boolean/Numbers first (return as-is, uppercase boolean)\n",
    "            if core_val.lower() == 'true': return 'TRUE'\n",
    "            if core_val.lower() == 'false': return 'FALSE'\n",
    "            if re.match(r'^\\d+(\\.\\d+)?$', core_val): return core_val\n",
    "            \n",
    "            # Handle Strings\n",
    "            lower_val = core_val.lower()\n",
    "            if lower_val in VALUE_MAPPING:\n",
    "                return f\"'{VALUE_MAPPING[lower_val]}'\"\n",
    "            \n",
    "            # Fallback: Title Case (e.g. 'consumer' -> 'Consumer')\n",
    "            # Note: This might capitalize 'in' -> 'In', which is why specific mapping above is preferred\n",
    "            return f\"'{core_val.title()}'\"\n",
    "\n",
    "        # 2. Handle Lists (comma separated) -> IN clause\n",
    "        if \",\" in s:\n",
    "            parts = [clean_and_format_value(p) for p in s.split(',')]\n",
    "            return f\"{col_expression} IN ({', '.join(parts)})\"\n",
    "\n",
    "        # 3. Handle Single Dot Values -> IN clause\n",
    "        if s.startswith('.'):\n",
    "            formatted_val = clean_and_format_value(s)\n",
    "            return f\"{col_expression} IN ({formatted_val})\"\n",
    "\n",
    "        # 4. Handle Standard Operators (>, =, !=)\n",
    "        match = re.match(r'^(=|!=|<>|>|<|>=|<=)\\s*(.*)', s)\n",
    "        if match:\n",
    "            op = match.group(1)\n",
    "            raw_val = match.group(2)\n",
    "            formatted_val = clean_and_format_value(raw_val)\n",
    "            return f\"{col_expression} {op} {formatted_val}\"\n",
    "            \n",
    "        return None\n",
    "\n",
    "    def _create_layout(self, layout_data, widgets):\n",
    "        \"\"\"\n",
    "        Structured Hierarchy:\n",
    "        1. Filters (Top - Height 1)\n",
    "        2. KPI/Headline Cards (Middle - Height 2)\n",
    "        3. Graphs/Charts (Bottom - Height 4)\n",
    "        \"\"\"\n",
    "        layout = []\n",
    "        GRID_WIDTH = 6\n",
    "        \n",
    "        # --- 1. CLASSIFICATION ---\n",
    "        filters = [w for w in widgets if w.get('is_filter')]\n",
    "        \n",
    "        # Helper to identify Cards (KPIs) vs Graphs\n",
    "        def is_card(widget_obj):\n",
    "            if widget_obj.get('is_filter') or widget_obj.get('is_trend_chart'):\n",
    "                return False\n",
    "            spec = widget_obj.get('widget', {}).get('spec', {})\n",
    "            w_type = str(spec.get('widgetType', '')).lower()\n",
    "            return any(k in w_type for k in ['kpi', 'headline', 'counter'])\n",
    "    \n",
    "        cards = [w for w in widgets if is_card(w)]\n",
    "        graphs = [w for w in widgets if not w.get('is_filter') and not is_card(w)]\n",
    "    \n",
    "        # --- 2. DIMENSIONS ---\n",
    "        FILTER_W, FILTER_H = 2, 1  # 3 per row\n",
    "        CARD_W, CARD_H     = 2, 2  # 3 per row\n",
    "        GRAPH_W, GRAPH_H   = 3, 4  # 2 per row (Side-by-side)\n",
    "    \n",
    "        current_x = 0\n",
    "        current_y = 0\n",
    "    \n",
    "        # --- 3. LAYER 1: FILTERS ---\n",
    "        if filters:\n",
    "            for f in filters:\n",
    "                if current_x + FILTER_W > GRID_WIDTH:\n",
    "                    current_x = 0\n",
    "                    current_y += FILTER_H\n",
    "                \n",
    "                layout.append({\n",
    "                    \"widget\": f['widget'],\n",
    "                    \"position\": {\"x\": current_x, \"y\": current_y, \"width\": FILTER_W, \"height\": FILTER_H}\n",
    "                })\n",
    "                current_x += FILTER_W\n",
    "            \n",
    "            # Advance Y to next row and add a small spacer\n",
    "            current_y += FILTER_H\n",
    "            current_x = 0\n",
    "    \n",
    "        # --- 4. LAYER 2: KPI CARDS ---\n",
    "        if cards:\n",
    "            for card in cards:\n",
    "                if current_x + CARD_W > GRID_WIDTH:\n",
    "                    current_x = 0\n",
    "                    current_y += CARD_H\n",
    "                \n",
    "                layout.append({\n",
    "                    \"widget\": card['widget'],\n",
    "                    \"position\": {\"x\": current_x, \"y\": current_y, \"width\": CARD_W, \"height\": CARD_H}\n",
    "                })\n",
    "                current_x += CARD_W\n",
    "                \n",
    "            # Advance Y to next row for graphs\n",
    "            current_y += CARD_H\n",
    "            current_x = 0\n",
    "    \n",
    "        # --- 5. LAYER 3: GRAPHS ---\n",
    "        for graph in graphs:\n",
    "            if current_x + GRAPH_W > GRID_WIDTH:\n",
    "                current_x = 0\n",
    "                current_y += GRAPH_H\n",
    "            \n",
    "            layout.append({\n",
    "                \"widget\": graph['widget'],\n",
    "                \"position\": {\"x\": current_x, \"y\": current_y, \"width\": GRAPH_W, \"height\": GRAPH_H}\n",
    "            })\n",
    "            current_x += GRAPH_W\n",
    "    \n",
    "        print(f\"Layout Generated: {len(filters)} Filters, {len(cards)} Cards, {len(graphs)} Graphs.\")\n",
    "        return layout\n",
    "\n",
    "\n",
    "    def _get_widget_size_from_db(self, size_category):\n",
    "        \"\"\"\n",
    "        Fetch widget dimensions from pre-loaded WIDGET_SIZE_MAP.\n",
    "        \n",
    "        Args:\n",
    "            size_category: Size category (e.g., 'SMALL', 'MEDIUM', 'LARGE')\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with 'width' and 'height' keys, or None if not found\n",
    "        \"\"\"\n",
    "        if not size_category:\n",
    "            print(f\"WARNING: _get_widget_size_from_db called with no size category\")\n",
    "            return None\n",
    "        \n",
    "        # Use the pre-loaded WIDGET_SIZE_MAP from global scope\n",
    "        size_config = WIDGET_SIZE_MAP.get(size_category)\n",
    "        \n",
    "        if size_config:\n",
    "            print(f\"INFO: Found size config for '{size_category}': width={size_config['width']}, height={size_config['height']}\")\n",
    "            return size_config\n",
    "        \n",
    "        print(f\"WARNING: No size configuration found for category '{size_category}' in WIDGET_SIZE_MAP\")\n",
    "        print(f\"Available size categories: {list(WIDGET_SIZE_MAP.keys())}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    def _get_default_size_for_widget_type(self, widget_type):\n",
    "        \"\"\"\n",
    "        Fetch default size category for a widget type from pre-loaded TML_TO_LVDASH_MAPPING.\n",
    "        \n",
    "        Args:\n",
    "            widget_type: Widget type (e.g., 'bar', 'table', 'counter')\n",
    "            \n",
    "        Returns:\n",
    "            str: Size category (e.g., 'MEDIUM'), or None if not found\n",
    "        \"\"\"\n",
    "        normalized_widget_type = widget_type.replace(' ', '_')\n",
    "        if not widget_type:\n",
    "            print(f\"WARNING: _get_default_size_for_widget_type called with no widget type\")\n",
    "            return None\n",
    "        \n",
    "        # Query the chart_type_mappings table for default_size\n",
    "        try:\n",
    "            from pyspark.sql import SparkSession\n",
    "            spark = SparkSession.builder.getOrCreate()\n",
    "            \n",
    "            result = spark.sql(f\"\"\"\n",
    "                SELECT default_size \n",
    "                FROM {CHART_TYPE_MAPPING_TABLE}\n",
    "                WHERE widget_type = '{normalized_widget_type}'\n",
    "                LIMIT 1\n",
    "            \"\"\").collect()\n",
    "            \n",
    "            if result and len(result) > 0:\n",
    "                default_size = result[0]['default_size']\n",
    "                if default_size:\n",
    "                    print(f\"INFO: Found default size '{default_size}' for widget type '{normalized_widget_type}'\")\n",
    "                    return default_size\n",
    "            \n",
    "            print(f\"WARNING: No default size found for widget type '{normalized_widget_type}'\")\n",
    "            print(f\"Hint: Check if '{normalized_widget_type}' exists in {CHART_TYPE_MAPPING_TABLE}\")\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to fetch default size for widget type '{normalized_widget_type}': {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def convert_all_tml_files():\n",
    "    setup_environment()\n",
    "    \n",
    "    # ===== Load liveboard configuration =====\n",
    "    CONFIG_TABLE = f\"{CATALOG}.{SCHEMA}.liveboard_migration_config\"\n",
    "    \n",
    "    print(\"\\n--- Loading Liveboard Configuration ---\")\n",
    "    try:\n",
    "        config_df = spark.table(CONFIG_TABLE).toPandas()\n",
    "        enabled_liveboards = config_df[config_df['process_flag'] == 'Y']\n",
    "        \n",
    "        if len(enabled_liveboards) == 0:\n",
    "            print(f\"WARNING: No liveboards enabled (process_flag='Y') in {CONFIG_TABLE}\")\n",
    "            print(\"Please enable at least one liveboard:\")\n",
    "            print(f\"UPDATE {CONFIG_TABLE} SET process_flag = 'Y' WHERE name = 'Your Dashboard Name';\")\n",
    "            return\n",
    "        \n",
    "        enabled_names = set(enabled_liveboards['name'].tolist())\n",
    "        enabled_guids = set(enabled_liveboards['guid'].tolist())\n",
    "        \n",
    "        print(f\"Found {len(enabled_liveboards)} enabled liveboards:\")\n",
    "        for _, row in enabled_liveboards.iterrows():\n",
    "            print(f\"  ✓ {row['name']} ({row['guid']})\")\n",
    "        print(\"---\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load configuration table {CONFIG_TABLE}: {e}\")\n",
    "        print(\"Proceeding to convert ALL liveboards without filtering...\")\n",
    "        enabled_names = None\n",
    "        enabled_guids = None\n",
    "\n",
    "    \n",
    "    if MAPPING_DATA is not None and len(MAPPING_DATA) > 0:\n",
    "        total_mappings = len(MAPPING_DATA)\n",
    "        mapped_count = len(MAPPING_DATA[\n",
    "            (MAPPING_DATA['databricks_table_name_ToBeFilled'].notna()) & \n",
    "            (MAPPING_DATA['databricks_table_name_ToBeFilled'] != '')\n",
    "        ])\n",
    "        print(f\"\\n--- Mapping Status ---\")\n",
    "        print(f\"Total visualizations: {total_mappings}\")\n",
    "        print(f\"Mapped to Databricks tables: {mapped_count}\")\n",
    "        print(f\"Unmapped (will use TML table names): {total_mappings - mapped_count}\")\n",
    "        print(\"---\\n\")\n",
    "    \n",
    "    try:\n",
    "        tml_files = [f.path for f in dbutils.fs.ls(TML_INPUT_PATH) if f.path.endswith(('.tml', '.yaml', '.json'))]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Cannot list files in '{TML_INPUT_PATH}'. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    if not tml_files:\n",
    "        print(f\"No TML files found in {TML_INPUT_PATH}\")\n",
    "        return\n",
    "    \n",
    "    # ===== Filter TML files based on enabled liveboards =====\n",
    "    if enabled_names or enabled_guids:\n",
    "        filtered_files = []\n",
    "        skipped_files = []\n",
    "        \n",
    "        for file_path in tml_files:\n",
    "            filename = Path(file_path).name\n",
    "            \n",
    "            try:\n",
    "                tml_data = parse_tml_file(file_path)\n",
    "                liveboard = tml_data.get('liveboard', {})\n",
    "                lb_name = liveboard.get('name', '')\n",
    "                lb_guid = tml_data.get('guid', '')\n",
    "                \n",
    "                if lb_name in enabled_names or lb_guid in enabled_guids:\n",
    "                    filtered_files.append(file_path)\n",
    "                    print(f\"  ✓ Including: {filename} ({lb_name})\")\n",
    "                else:\n",
    "                    skipped_files.append(filename)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  WARNING: Could not parse {filename} for filtering: {e}\")\n",
    "                filtered_files.append(file_path)\n",
    "        \n",
    "        tml_files = filtered_files\n",
    "        \n",
    "        print(f\"\\n--- File Filtering Results ---\")\n",
    "        print(f\"Total files found: {len(tml_files) + len(skipped_files)}\")\n",
    "        print(f\"Files to process: {len(tml_files)}\")\n",
    "        print(f\"Files skipped: {len(skipped_files)}\")\n",
    "        if skipped_files and len(skipped_files) <= 10:\n",
    "            print(f\"Skipped files: {', '.join(skipped_files)}\")\n",
    "        elif len(skipped_files) > 10:\n",
    "            print(f\"Skipped files: {', '.join(skipped_files[:10])}... and {len(skipped_files)-10} more\")\n",
    "        print(\"---\\n\")\n",
    "    \n",
    "    if not tml_files:\n",
    "        print(\"No enabled TML files to process after filtering.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(tml_files)} enabled liveboard(s)...\\n\")\n",
    "\n",
    "    \n",
    "    tracker = ConversionTracker()\n",
    "    summary_results = []\n",
    "    failure_records = []\n",
    "    \n",
    "    for tml_file_path in tml_files:\n",
    "        filename = Path(tml_file_path).name\n",
    "        status = 'ERROR'\n",
    "        lvdash_data = {}\n",
    "        output_filename = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n--- Processing: {filename} ---\")\n",
    "            \n",
    "            try:\n",
    "                tml_data = parse_tml_file(tml_file_path)\n",
    "            except Exception as parse_error:\n",
    "                raise ValueError(f\"Failed to parse TML file: {str(parse_error)}\")\n",
    "            \n",
    "            converter = TMLToLVDASHConverter(\n",
    "                tml_data, \n",
    "                filename, \n",
    "                tracker, \n",
    "                MAPPING_DATA, \n",
    "                COLUMN_DETAILS_DATA,\n",
    "                VIZ_FILTER_DATA\n",
    "            )\n",
    "            lvdash_data = converter.convert()\n",
    "            \n",
    "            output_filename = filename.replace('.tml', '').replace('.yaml', '').replace('.json', '') + '.lvdash.json'\n",
    "            output_path = os.path.join(LVDASH_OUTPUT_PATH, output_filename)\n",
    "            dbutils.fs.put(output_path, json.dumps(lvdash_data, indent=2), overwrite=True)\n",
    "            status = 'SUCCESS'\n",
    "            print(f\"  Saved to {output_filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  FAILED: {e}\")\n",
    "            import traceback\n",
    "            error_trace = traceback.format_exc()\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            failure_records.append({\n",
    "                'tml_file': filename,\n",
    "                'error_type': type(e).__name__,\n",
    "                'error_message': str(e)[:1000],\n",
    "                'stack_trace': error_trace[:2000],\n",
    "                'failure_timestamp': datetime.now()\n",
    "            })\n",
    "        \n",
    "        summary_results.append({\n",
    "            'tml_file': filename, \n",
    "            'lvdash_file': output_filename, \n",
    "            'status': status,\n",
    "            'num_datasets': len(lvdash_data.get('datasets', [])),\n",
    "            'num_pages': len(lvdash_data.get('pages', [])),\n",
    "            'num_widgets': sum(len(p.get('layout', [])) for p in lvdash_data.get('pages', [])),\n",
    "            'conversion_timestamp': datetime.now()\n",
    "        })\n",
    "\n",
    "    print(\"\\n--- Saving logs ---\")\n",
    "    if tracker.records:\n",
    "        df = pd.DataFrame(tracker.records)\n",
    "        df['conversion_timestamp'] = pd.to_datetime(df['conversion_timestamp'])\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(TRACKER_TABLE)\n",
    "        print(f\"Saved {len(tracker.records)} widget records to {TRACKER_TABLE}\")\n",
    "\n",
    "    if summary_results:\n",
    "        df = pd.DataFrame(summary_results)\n",
    "        df['num_datasets'] = df['num_datasets'].astype('int32')\n",
    "        df['num_pages'] = df['num_pages'].astype('int32')\n",
    "        df['num_widgets'] = df['num_widgets'].astype('int32')\n",
    "        df['conversion_timestamp'] = pd.to_datetime(df['conversion_timestamp'])\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(SUMMARY_TABLE)\n",
    "        print(f\"Saved {len(summary_results)} file summaries to {SUMMARY_TABLE}\")\n",
    "    \n",
    "    if failure_records:\n",
    "        df = pd.DataFrame(failure_records)\n",
    "        df['failure_timestamp'] = pd.to_datetime(df['failure_timestamp'])\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(FAILURE_TABLE)\n",
    "        print(f\"Saved {len(failure_records)} failures to {FAILURE_TABLE}\")\n",
    "    \n",
    "    print(\"\\n--- Conversion complete! ---\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "convert_all_tml_files()\n",
    "\n",
    "print(\"--- Overall Conversion Summary ---\")\n",
    "try:\n",
    "    display(spark.table(SUMMARY_TABLE).orderBy(\"conversion_timestamp\", ascending=False))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display summary table. Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Detailed Widget Conversion Tracker ---\")\n",
    "try:\n",
    "    display(spark.table(TRACKER_TABLE).orderBy(\"conversion_timestamp\", ascending=False))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display tracker table. Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Conversion Failures ---\")\n",
    "try:\n",
    "    fail_df = spark.table(FAILURE_TABLE)\n",
    "    fail_count = fail_df.count()\n",
    "    if fail_count > 0:\n",
    "        print(f\"Found {fail_count} failed conversions:\")\n",
    "        display(fail_df.orderBy(\"failure_timestamp\", ascending=False))\n",
    "    else:\n",
    "        print(\"No failures!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not display failure table. Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Chart Type Mapping Statistics ---\")\n",
    "try:\n",
    "    stats_query = f\"\"\"\n",
    "    SELECT \n",
    "        tml_type,\n",
    "        lvdash_type,\n",
    "        status,\n",
    "        COUNT(*) as count,\n",
    "        COUNT(CASE WHEN unmapped_properties != '' THEN 1 END) as has_unmapped_props\n",
    "    FROM {TRACKER_TABLE}\n",
    "    GROUP BY tml_type, lvdash_type, status\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    "    display(spark.sql(stats_query))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display statistics. Error: {e}\")\n",
    "\n",
    "print(\"\\n--- Unmapped Properties Report ---\")\n",
    "try:\n",
    "    unmapped_query = f\"\"\"\n",
    "    SELECT \n",
    "        tml_file,\n",
    "        widget_name,\n",
    "        tml_type,\n",
    "        lvdash_type,\n",
    "        unmapped_properties,\n",
    "        notes\n",
    "    FROM {TRACKER_TABLE}\n",
    "    WHERE unmapped_properties != '' OR notes != ''\n",
    "    ORDER BY tml_file, widget_name\n",
    "    \"\"\"\n",
    "    display(spark.sql(unmapped_query))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display unmapped properties. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4880426491823361,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Visualization_Mapping_AllViz",
   "widgets": {
    "tml_file": {
     "currentValue": "4C - HPNow Support Tickets Tracker.liveboard.tml",
     "nuid": "d6192593-3d64-4490-8a70-dc8aef9d0e84",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "tml_file",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "tml_file",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
