{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571e1e3c-1ec3-455f-8f8b-2a6e114344c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"ds_training_1\"\n",
    "SCHEMA = \"thoughtspot_inventory_ak\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "CHART_TYPE_MAPPING_TABLE = f\"{CATALOG}.{SCHEMA}.chart_type_mappings\"\n",
    "WIDGET_SIZE_CONFIG_TABLE = f\"{CATALOG}.{SCHEMA}.widget_size_config\"\n",
    "EXPRESSION_TRANSFORM_TABLE = f\"{CATALOG}.{SCHEMA}.expression_transformations\"\n",
    "SCALE_TYPE_DETECTION_TABLE = f\"{CATALOG}.{SCHEMA}.scale_type_detection\"\n",
    "\n",
    "# Create chart_type_mappings\n",
    "spark.sql(f\"\"\"\n",
    "    DROP TABLE IF EXISTS {CHART_TYPE_MAPPING_TABLE}\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {CHART_TYPE_MAPPING_TABLE} (\n",
    "        tml_chart_type STRING,\n",
    "        widget_type STRING,\n",
    "        default_size STRING\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert data\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {CHART_TYPE_MAPPING_TABLE} VALUES\n",
    "    ('COLUMN', 'bar', 'MEDIUM'),\n",
    "    ('STACKED_COLUMN', 'bar', 'MEDIUM'),\n",
    "    ('LINE_COLUMN', 'line', 'LARGE'),\n",
    "    ('PIVOT_TABLE', 'pivot', 'LARGE'),\n",
    "    ('LINE', 'line', 'LARGE'),\n",
    "    ('KPI', 'counter', 'EXTRA_SMALL'),\n",
    "    ('DONUT', 'pie', 'MEDIUM'),\n",
    "    ('PIE', 'pie', 'MEDIUM'),\n",
    "    ('BAR', 'bar', 'MEDIUM'),\n",
    "    ('STACKED_BAR', 'bar', 'MEDIUM'),\n",
    "    ('AREA', 'area', 'LARGE'),\n",
    "    ('STACKED_AREA', 'area', 'LARGE'),\n",
    "    ('SCATTER', 'scatter', 'LARGE'),\n",
    "    ('WATERFALL', 'bar', 'MEDIUM'),\n",
    "    ('FUNNEL', 'funnel', 'MEDIUM'),\n",
    "    ('GEO_BUBBLE', 'choropleth map', 'LARGE'),\n",
    "    ('GEO_HEATMAP', 'choropleth map', 'LARGE'),\n",
    "    ('CHOROPLETH', 'choropleth map', 'LARGE'),\n",
    "    ('GEO_AREA', 'choropleth map', 'LARGE'),\n",
    "    ('RADAR', NULL, 'MEDIUM'),\n",
    "    ('PARETO', NULL, 'MEDIUM'),\n",
    "    ('COMBO_LINE_COLUMN', 'combo', 'LARGE'),\n",
    "    ('COMBO_LINE_STACKED_COLUMN', 'combo', 'LARGE'),\n",
    "    ('BUBBLE', 'scatter', 'LARGE'),\n",
    "    ('HEATMAP', 'heatmap', 'LARGE'),\n",
    "    ('TREEMAP', NULL, 'MEDIUM'),\n",
    "    ('SANKEY', 'sankey', 'LARGE'),\n",
    "    ('CANDLESTICK', NULL, 'LARGE'),\n",
    "    ('TABLE', 'table', 'LARGE'),\n",
    "    ('TABLE_MODE', 'table', 'LARGE'),\n",
    "    ('PIVOT_TABLE_CLASSIC', 'pivot', 'LARGE')\n",
    "\"\"\")\n",
    "\n",
    "# Create widget_size_config\n",
    "spark.sql(f\"\"\"\n",
    "    DROP TABLE IF EXISTS {WIDGET_SIZE_CONFIG_TABLE}\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {WIDGET_SIZE_CONFIG_TABLE} (\n",
    "        size_category STRING,\n",
    "        width INT,\n",
    "        height INT,\n",
    "        description STRING\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {WIDGET_SIZE_CONFIG_TABLE} VALUES\n",
    "    ('EXTRA_SMALL', 2, 2, 'Smallest widget size'),\n",
    "    ('SMALL', 2, 3, 'Small widget size'),\n",
    "    ('MEDIUM_SMALL', 3, 3, 'Medium-small widget size'),\n",
    "    ('MEDIUM', 3, 4, 'Medium widget size'),\n",
    "    ('LARGE_SMALL', 4, 4, 'Large-small widget size'),\n",
    "    ('LARGE', 4, 5, 'Large widget size'),\n",
    "    ('EXTRA_LARGE', 6, 6, 'Largest widget size')\n",
    "\"\"\")\n",
    "\n",
    "# Create expression_transformations\n",
    "spark.sql(f\"\"\"\n",
    "    DROP TABLE IF EXISTS {EXPRESSION_TRANSFORM_TABLE}\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {EXPRESSION_TRANSFORM_TABLE} (\n",
    "        tml_pattern STRING,\n",
    "        target_expression STRING,\n",
    "        expression_type STRING,\n",
    "        priority INT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {EXPRESSION_TRANSFORM_TABLE} VALUES\n",
    "    ('sum(field)', 'SUM(`field`)', 'AGGREGATION', 1),\n",
    "    ('count(distinct field)', 'COUNT(DISTINCT `field`)', 'AGGREGATION', 1),\n",
    "    ('count(field)', 'COUNT(`field`)', 'AGGREGATION', 2),\n",
    "    ('avg(field)', 'AVG(`field`)', 'AGGREGATION', 3),\n",
    "    ('min(field)', 'MIN(`field`)', 'AGGREGATION', 4),\n",
    "    ('max(field)', 'MAX(`field`)', 'AGGREGATION', 5),\n",
    "    ('Day(field)', \"DATE_TRUNC('DAY', `field`)\", 'DATE_FUNCTION', 5),\n",
    "    ('Week(field)', \"DATE_TRUNC('WEEK', `field`)\", 'DATE_FUNCTION', 6),\n",
    "    ('Month(field)', \"DATE_TRUNC('MONTH', `field`)\", 'DATE_FUNCTION', 7),\n",
    "    ('Year(field)', \"DATE_TRUNC('YEAR', `field`)\", 'DATE_FUNCTION', 8)\n",
    "\"\"\")\n",
    "\n",
    "# Create scale_type_detection\n",
    "spark.sql(f\"\"\"\n",
    "    DROP TABLE IF EXISTS {SCALE_TYPE_DETECTION_TABLE}\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {SCALE_TYPE_DETECTION_TABLE} (\n",
    "        field_pattern STRING,\n",
    "        scale_type STRING,\n",
    "        priority INT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {SCALE_TYPE_DETECTION_TABLE} VALUES\n",
    "    ('*_date, *_time', 'temporal', 1),\n",
    "    ('*year*, *month*, *day*', 'temporal', 2),\n",
    "    ('*timestamp*, *datetime*', 'temporal', 3),\n",
    "    ('*week*', 'temporal', 4),\n",
    "    ('sum_*, count_*, avg_*', 'quantitative', 10),\n",
    "    ('*total*, *revenue*, *price*', 'quantitative', 11),\n",
    "    ('*quantity*, *amount*, *sales*', 'quantitative', 12),\n",
    "    ('*number*, *num_*', 'quantitative', 13),\n",
    "    ('*min*, *max*', 'quantitative', 14),\n",
    "    ('*_category, *_name', 'categorical', 20),\n",
    "    ('*status*, *type*', 'categorical', 21),\n",
    "    ('*id, *_id', 'categorical', 22)\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ All configuration tables created successfully!\")\n",
    "print(\"You can now run the converter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da462da-24cd-403a-a1ff-9029eaaa0590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from ds_training_1.thoughtspot_inventory_ak.tml_metadata_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f013c2a-7f9a-408b-bd33-58a9e89505fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Dashboard Configuration Setup\n",
    "# MAGIC \n",
    "# MAGIC This notebook:\n",
    "# MAGIC 1. Reads TML files from a volume folder\n",
    "# MAGIC 2. Identifies all liveboards\n",
    "# MAGIC 3. Creates/updates a configuration table with dashboard name, GUID, and process flag (default: N)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Configuration\n",
    "CATALOG = \"ts_migration\"\n",
    "SCHEMA = \"thoughtspot_inventory_ak\"\n",
    "VOLUME = \"tml_files_ak/tml_hp\"\n",
    "FOLDER = \"liveboard\"\n",
    "\n",
    "# Full path to TML files\n",
    "TML_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/{FOLDER}/\"\n",
    "\n",
    "# Configuration table\n",
    "CONFIG_TABLE = f\"{CATALOG}.{SCHEMA}.liveboard_migration_config\"\n",
    "\n",
    "print(f\"Reading from: {TML_PATH}\")\n",
    "print(f\"Config table: {CONFIG_TABLE}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def parse_tml_file(file_path):\n",
    "    \"\"\"Parse TML file (YAML or JSON)\"\"\"\n",
    "    try:\n",
    "        content = dbutils.fs.head(file_path, 10 * 1024 * 1024)\n",
    "        try:\n",
    "            return yaml.safe_load(content)\n",
    "        except yaml.YAMLError:\n",
    "            return json.loads(content)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error parsing file: {e}\")\n",
    "        return None\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Read files from volume\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "try:\n",
    "    all_files = dbutils.fs.ls(TML_PATH)\n",
    "    tml_files = [f.path for f in all_files if f.path.endswith(('.tml', '.yaml', '.json'))]\n",
    "    print(f\"Found {len(tml_files)} TML files\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Cannot read from {TML_PATH}\")\n",
    "    print(f\"Error: {e}\")\n",
    "    raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Identify liveboards\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "liveboards = []\n",
    "\n",
    "for file_path in tml_files:\n",
    "    filename = Path(file_path).name\n",
    "    print(f\"Processing: {filename}\")\n",
    "    \n",
    "    tml_data = parse_tml_file(file_path)\n",
    "    \n",
    "    if tml_data and 'liveboard' in tml_data:\n",
    "        liveboard = tml_data['liveboard']\n",
    "        name = liveboard.get('name', filename.replace('.tml', '').replace('.yaml', '').replace('.json', ''))\n",
    "        guid = tml_data.get('guid', 'NO_GUID')\n",
    "        \n",
    "        liveboards.append({\n",
    "            'name': name,\n",
    "            'guid': guid,\n",
    "            'process_flag': 'N',\n",
    "            'last_updated': datetime.now()\n",
    "        })\n",
    "        \n",
    "        print(f\"  ✓ Found liveboard: {name} ({guid})\")\n",
    "    else:\n",
    "        print(f\"  ✗ Not a liveboard or parse error\")\n",
    "\n",
    "print(f\"\\nTotal liveboards found: {len(liveboards)}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Create table and insert entries\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create table if not exists\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {CONFIG_TABLE} (\n",
    "    name STRING,\n",
    "    guid STRING,\n",
    "    process_flag STRING,\n",
    "    last_updated TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_table_sql)\n",
    "print(f\"✓ Table {CONFIG_TABLE} ready\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get existing GUIDs\n",
    "try:\n",
    "    existing_df = spark.sql(f\"SELECT guid FROM {CONFIG_TABLE}\").toPandas()\n",
    "    existing_guids = set(existing_df['guid'].tolist())\n",
    "    print(f\"Found {len(existing_guids)} existing entries\")\n",
    "except:\n",
    "    existing_guids = set()\n",
    "    print(\"No existing entries\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Insert only new liveboards\n",
    "import pandas as pd\n",
    "\n",
    "new_liveboards = [lb for lb in liveboards if lb['guid'] not in existing_guids]\n",
    "\n",
    "if new_liveboards:\n",
    "    df = pd.DataFrame(new_liveboards)\n",
    "    df['last_updated'] = pd.to_datetime(df['last_updated'])\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    spark_df.write.mode(\"append\").saveAsTable(CONFIG_TABLE)\n",
    "    print(f\"✓ Added {len(new_liveboards)} new liveboards\")\n",
    "else:\n",
    "    print(\"✓ No new liveboards to add\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## View Configuration Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "display(spark.sql(f\"SELECT * FROM {CONFIG_TABLE} ORDER BY last_updated DESC\"))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "summary = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    process_flag,\n",
    "    COUNT(*) as count\n",
    "FROM {CONFIG_TABLE}\n",
    "GROUP BY process_flag\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== Summary ===\")\n",
    "display(summary)\n",
    "\n",
    "print(f\"\"\"\n",
    "Configuration table: {CONFIG_TABLE}\n",
    "\n",
    "To enable a dashboard:\n",
    "UPDATE {CONFIG_TABLE} \n",
    "SET process_flag = 'Y', last_updated = CURRENT_TIMESTAMP()\n",
    "WHERE name = 'Your Dashboard Name';\n",
    "\n",
    "To enable all:\n",
    "UPDATE {CONFIG_TABLE} \n",
    "SET process_flag = 'Y', last_updated = CURRENT_TIMESTAMP();\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cbe9d61-43d4-452a-a915-6d4707327066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE ts_migration.thoughtspot_inventory_ak.paas_tracking_card (\n",
    "    session_id STRING,\n",
    "    app_package_deployed_uuid STRING,\n",
    "    os_platform STRING,\n",
    "    app_name STRING,\n",
    "    app_package_id STRING,\n",
    "    app_version STRING,\n",
    "    session_start_date_time TIMESTAMP,\n",
    "    geo_country_code STRING,\n",
    "    is_hpid_signed_in BOOLEAN,\n",
    "    total_printer_count INT,\n",
    "    total_device_count INT,\n",
    "    total_accessory_count INT,\n",
    "    total_pc_count INT,\n",
    "    is_viewed_aip_tracking_card BOOLEAN,\n",
    "    is_viewed_aip_tracking_card_order_confirmed BOOLEAN,\n",
    "    is_viewed_aip_tracking_card_order_processing BOOLEAN,\n",
    "    is_viewed_aip_tracking_card_order_shipped BOOLEAN,\n",
    "    is_viewed_aip_tracking_card_order_delivered BOOLEAN,\n",
    "    is_clicked_aip_order_accordian BOOLEAN,\n",
    "    is_clicked_order_confirmation BOOLEAN,\n",
    "    is_clicked_order_processing BOOLEAN,\n",
    "    is_clicked_track_delivery BOOLEAN,\n",
    "    is_clicked_complete_setup BOOLEAN,\n",
    "    is_clicked_order_confirmation_pill BOOLEAN,\n",
    "    is_clicked_order_processing_pill BOOLEAN,\n",
    "    is_clicked_order_shipped_pill BOOLEAN,\n",
    "    is_clicked_order_delivered_pill BOOLEAN,\n",
    "    device_app_package_deployed_uuid STRING,\n",
    "    max_total_printer_count INT,\n",
    "    max_total_device_count INT,\n",
    "    max_total_accessory_count INT,\n",
    "    max_total_pc_count INT,\n",
    "    associated_device_session_id STRING,\n",
    "    aip_device_uuid STRING,\n",
    "    is_associated_device BOOLEAN,\n",
    "    is_clicked_aip_order_accordian_order_processing BOOLEAN,\n",
    "    is_clicked_aip_order_accord BOOLEAN,\n",
    "    is_aip_setup_start BOOLEAN,\n",
    "    is_clicked_aip_order_accordian_order_confirmed BOOLEAN,\n",
    "    is_ows_start BOOLEAN,\n",
    "    is_clicked_aip_order_accordian_order_shipped BOOLEAN,\n",
    "    is_oobe_complete BOOLEAN,\n",
    "    is_aip_setup_complete BOOLEAN,\n",
    "    is_clicked_support BOOLEAN,\n",
    "    is_oobe_support_session BOOLEAN\n",
    ")\n",
    "USING DELTA;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b8e845a-2f11-4312-9674-79646031e854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def generate_row(i):\n",
    "    return (\n",
    "        f\"sess_{i}\",  # session_id\n",
    "        f\"pkg_deployed_{random.randint(1000,2000)}\",  # app_package_deployed_uuid\n",
    "        random.choice([\"Windows\", \"MacOS\", \"Linux\"]),  # os_platform\n",
    "        random.choice([\"HP Smart\", \"Instant Ink\", \"HP App\"]),  # app_name\n",
    "        f\"app_{random.randint(1,500)}\",  # app_package_id\n",
    "        f\"v{random.randint(1,10)}.{random.randint(0,9)}\",  # app_version\n",
    "        datetime.now() - timedelta(days=random.randint(0, 120)),  # session_start_date_time\n",
    "        random.choice([\"US\", \"IN\", \"UK\", \"DE\", \"FR\", \"CN\"]),  # geo_country_code\n",
    "        random.choice([True, False]),  # is_hpid_signed_in\n",
    "        random.randint(0, 5),  # total_printer_count\n",
    "        random.randint(0, 10),  # total_device_count\n",
    "        random.randint(0, 3),  # total_accessory_count\n",
    "        random.randint(0, 7),  # total_pc_count\n",
    "        random.choice([True, False]),  # is_viewed_aip_tracking_card\n",
    "        random.choice([True, False]),  # is_viewed_aip_tracking_card_order_confirmed\n",
    "        random.choice([True, False]),  # is_viewed_aip_tracking_card_order_processing\n",
    "        random.choice([True, False]),  # is_viewed_aip_tracking_card_order_shipped\n",
    "        random.choice([True, False]),  # is_viewed_aip_tracking_card_order_delivered\n",
    "        random.choice([True, False]),  # is_clicked_aip_order_accordian\n",
    "        random.choice([True, False]),  # is_clicked_order_confirmation\n",
    "        random.choice([True, False]),  # is_clicked_order_processing\n",
    "        random.choice([True, False]),  # is_clicked_track_delivery\n",
    "        random.choice([True, False]),  # is_clicked_complete_setup\n",
    "        random.choice([True, False]),  # is_clicked_order_confirmation_pill\n",
    "        random.choice([True, False]),  # is_clicked_order_processing_pill\n",
    "        random.choice([True, False]),  # is_clicked_order_shipped_pill\n",
    "        random.choice([True, False]),  # is_clicked_order_delivered_pill\n",
    "        f\"device_pkg_{random.randint(100,999)}\",  # device_app_package_deployed_uuid\n",
    "        random.randint(0, 5),  # max_total_printer_count\n",
    "        random.randint(0, 10),  # max_total_device_count\n",
    "        random.randint(0, 3),  # max_total_accessory_count\n",
    "        random.randint(0, 7),  # max_total_pc_count\n",
    "        f\"assoc_session_{random.randint(100,999)}\",  # associated_device_session_id\n",
    "        f\"aip_uuid_{random.randint(10000,99999)}\",  # aip_device_uuid\n",
    "        random.choice([True, False]),  # is_associated_device\n",
    "        random.choice([True, False]),  # is_clicked_aip_order_accordian_order_processing\n",
    "        random.choice([True, False]),  # is_clicked_aip_order_accord\n",
    "        random.choice([True, False]),  # is_aip_setup_start\n",
    "        random.choice([True, False]),  # is_clicked_aip_order_accordian_order_confirmed\n",
    "        random.choice([True, False]),  # is_ows_start\n",
    "        random.choice([True, False]),  # is_clicked_aip_order_accordian_order_shipped\n",
    "        random.choice([True, False]),  # is_oobe_complete\n",
    "        random.choice([True, False]),  # is_aip_setup_complete\n",
    "        random.choice([True, False]),  # is_clicked_support\n",
    "        random.choice([True, False])   # is_oobe_support_session\n",
    "    )\n",
    "data = [generate_row(i) for i in range(1, 101)]\n",
    "\n",
    "schema = spark.table(\"ts_migration.thoughtspot_inventory_ak.paas_tracking_card\").schema\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "df.write.mode(\"append\").saveAsTable(\"ts_migration.thoughtspot_inventory_ak.paas_tracking_card\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5869069174448022,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Config_Table_Scripts",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
