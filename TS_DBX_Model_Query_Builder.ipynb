{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ecad597-0eac-49e9-928e-49c5dc20fd01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Identifying Column Lineage, Joins and Calculated Fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bacb983-1f01-40d9-ae35-72cc807a9289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# --- Configuration ---\n",
    "CATALOG = \"dbx_migration_poc\"\n",
    "SCHEMA = \"dbx_migration_ts\"\n",
    "\n",
    "dbutils.widgets.text(\"tml_file\", \"\")\n",
    "tml_file = dbutils.widgets.get(\"tml_file\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# CORE EXTRACTOR CLASS (Auto-Discovery Version)\n",
    "# -------------------------------------------------------------------------\n",
    "class TMLAnalyzer:\n",
    "    def __init__(self, liveboard_path: str, tml_base_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the analyzer by reading the Liveboard file and \n",
    "        automatically discovering related Model and Table files.\n",
    "        \"\"\"\n",
    "        self.liveboard_path = liveboard_path\n",
    "        self.tml_base_path = tml_base_path\n",
    "        \n",
    "        print(f\"1. Loading Liveboard: {Path(liveboard_path).name}\")\n",
    "        self.liveboard_tm = self._load_yaml(liveboard_path)\n",
    "        \n",
    "        # --- Auto-Discover Model ---\n",
    "        visualizations = self.liveboard_tm.get('liveboard', {}).get('visualizations', [])\n",
    "        model_name = None\n",
    "\n",
    "        if visualizations:\n",
    "            # Check the first visualization's answer block for the primary model/worksheet\n",
    "            first_viz_tables = visualizations[0].get('answer', {}).get('tables', [])\n",
    "            if first_viz_tables:\n",
    "                model_name = first_viz_tables[0].get('name')\n",
    "\n",
    "        if not model_name:\n",
    "            raise ValueError(f\"Could not identify a Model name from the first visualization in {Path(liveboard_path).name}\")\n",
    "            \n",
    "        print(f\"   > Found Base Model Reference: {model_name}\")\n",
    "\n",
    "        # --- Load Model File ---\n",
    "        model_file_path = f\"{tml_base_path}/model/{model_name}.model.tml\"\n",
    "        \n",
    "        if not os.path.exists(model_file_path):\n",
    "             model_file_path_ws = f\"{tml_base_path}/worksheet/{model_name}.worksheet.tml\"\n",
    "             if os.path.exists(model_file_path_ws):\n",
    "                 model_file_path = model_file_path_ws\n",
    "             else:\n",
    "                 model_file_path_flat = f\"{os.path.dirname(liveboard_path)}/{model_name}.model.tml\"\n",
    "                 if os.path.exists(model_file_path_flat):\n",
    "                    model_file_path = model_file_path_flat\n",
    "                 else:\n",
    "                    raise FileNotFoundError(f\"Model file not found at: {model_file_path}\")\n",
    "             \n",
    "        print(f\"2. Loading Model: {Path(model_file_path).name}\")\n",
    "        self.model_tm = self._load_yaml(model_file_path)\n",
    "        \n",
    "        # --- Auto-Discover Tables ---\n",
    "        self.tables_tm = {}\n",
    "        table_names = self._get_all_model_tables(self.model_tm)\n",
    "        \n",
    "        print(f\"3. Discovering {len(table_names)} Tables referenced in Model...\")\n",
    "        for t_name in table_names:\n",
    "            t_path = f\"{tml_base_path}/table/{t_name}.table.tml\"\n",
    "            \n",
    "            if not os.path.exists(t_path):\n",
    "                 t_path_flat = f\"{os.path.dirname(liveboard_path)}/{t_name}.table.tml\"\n",
    "                 if os.path.exists(t_path_flat):\n",
    "                     t_path = t_path_flat\n",
    "\n",
    "            if os.path.exists(t_path):\n",
    "                data = self._load_yaml(t_path)\n",
    "                self.tables_tm[t_name] = data.get('table', {})\n",
    "                print(f\"   > Loaded Table: {t_name}\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ Warning: Table file not found for '{t_name}' at {t_path}\")\n",
    "\n",
    "        # Pre-process Model Columns for fast lookup\n",
    "        self.model_col_map = self._build_model_column_map()\n",
    "\n",
    "    def _load_yaml(self, path: str) -> Dict[str, Any]:\n",
    "        with open(path, 'r') as f:\n",
    "            return yaml.safe_load(f)\n",
    "\n",
    "    def _get_all_model_tables(self, model_data: Dict[str, Any]) -> List[str]:\n",
    "        root = model_data.get('model') or model_data.get('worksheet') or {}\n",
    "        return [t['name'] for t in root.get('model_tables', []) if 'name' in t]\n",
    "\n",
    "    def _build_model_column_map(self) -> Dict[str, Dict]:\n",
    "        col_map = {}\n",
    "        root = self.model_tm.get('model') or self.model_tm.get('worksheet') or {}\n",
    "        \n",
    "        for col in root.get('columns', []):\n",
    "            agg = col.get('properties', {}).get('aggregation')\n",
    "            \n",
    "            if 'column_id' in col:\n",
    "                parts = col['column_id'].split('::')\n",
    "                if len(parts) == 2:\n",
    "                    entry = {\n",
    "                        'table_name': parts[0],\n",
    "                        'physical_col_id': parts[1],\n",
    "                        'full_physical_id': col['column_id'], # <--- Storing full reference\n",
    "                        'type': 'DIRECT',\n",
    "                        'aggregation': agg\n",
    "                    }\n",
    "                    col_map[col['name']] = entry\n",
    "            elif 'formula_id' in col:\n",
    "                col_map[col['name']] = {\n",
    "                    'type': 'FORMULA', \n",
    "                    'full_physical_id': col['formula_id'], # <--- Storing formula reference\n",
    "                    'expr': '', \n",
    "                    'aggregation': agg\n",
    "                }\n",
    "        \n",
    "        for form in root.get('formulas', []):\n",
    "            name = form.get('name')\n",
    "            expr = form.get('expr', '')\n",
    "            if name in col_map:\n",
    "                col_map[name]['expr'] = expr\n",
    "            else:\n",
    "                entry = {'type': 'FORMULA', 'expr': expr, 'aggregation': None, 'full_physical_id': form.get('id')}\n",
    "                col_map[name] = entry\n",
    "            if 'id' in form:\n",
    "                col_map[form['id']] = col_map[name]\n",
    "             \n",
    "        return col_map\n",
    "\n",
    "    def _clean_col_name(self, name: str) -> str:\n",
    "        name = re.sub(r'^(Total |Maximum |Minimum |Average |Unique Number of )\\s*', '', name, flags=re.IGNORECASE)\n",
    "        wrapper_keywords = r'Sum|Count|Avg|Min|Max|Unique Count|Monthly|Daily|Weekly|Quarterly|Yearly|Week|Month|Quarter|Year|Day'\n",
    "        while True:\n",
    "            match = re.match(r'^(' + wrapper_keywords + r')\\s*\\((.*)\\)$', name, flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                name = match.group(2).strip()\n",
    "            else:\n",
    "                break\n",
    "        return name\n",
    "\n",
    "    def _get_physical_info(self, logical_table, col_id):\n",
    "        t_def = self.tables_tm.get(logical_table, {})\n",
    "        db = t_def.get('db', 'UNK')\n",
    "        sch = t_def.get('schema', 'UNK')\n",
    "        tbl = t_def.get('db_table', logical_table)\n",
    "        clean_tbl = f\"`{tbl}`\" if ' ' in tbl or '-' in tbl else tbl\n",
    "        full_table = f\"{db}.{sch}.{clean_tbl}\"\n",
    "        \n",
    "        phy_col = col_id\n",
    "        for c in t_def.get('columns', []):\n",
    "            if c['name'] == col_id:\n",
    "                phy_col = c.get('db_column_name', col_id)\n",
    "                break\n",
    "        if ' ' in phy_col: phy_col = f\"`{phy_col}`\"\n",
    "        return full_table, tbl, phy_col\n",
    "\n",
    "    def _resolve_expr(self, expr, viz_formulas=None, depth=0):\n",
    "        if depth > 10: return \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\"\n",
    "        match = re.search(r'\\[([^\\]]+)::([^\\]]+)\\]', expr)\n",
    "        if match:\n",
    "            return self._get_physical_info(match.group(1), match.group(2))\n",
    "        matches = re.findall(r'\\[(.*?)\\]', expr)\n",
    "        best_resolution = (\"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\")\n",
    "        for ref in matches:\n",
    "            if '::' in ref: continue \n",
    "            current_resolution = (\"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\")\n",
    "            if viz_formulas and ref in viz_formulas:\n",
    "                current_resolution = self._resolve_expr(viz_formulas[ref], viz_formulas, depth+1)\n",
    "            if current_resolution[0] == \"UNKNOWN\":\n",
    "                m_info = self.model_col_map.get(ref)\n",
    "                if m_info:\n",
    "                    if m_info['type'] == 'DIRECT':\n",
    "                        current_resolution = self._get_physical_info(m_info['table_name'], m_info['physical_col_id'])\n",
    "                    elif m_info['type'] == 'FORMULA':\n",
    "                        current_resolution = self._resolve_expr(m_info['expr'], viz_formulas, depth+1)\n",
    "            if current_resolution[0] != \"UNKNOWN\":\n",
    "                best_resolution = current_resolution\n",
    "                break \n",
    "        return best_resolution\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # INTEGRATED REQUIREMENT: JOINING LIVEBOARD FILTERS WITH MODEL COLUMN IDs\n",
    "    # -------------------------------------------------------------------------\n",
    "    def generate_filter_details(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Parses Liveboard TML and joins with Model mapping for physical IDs.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        # Extract filters from the liveboard root \n",
    "        filters = self.liveboard_tm.get('liveboard', {}).get('filters', [])\n",
    "        \n",
    "        for fltr in filters:\n",
    "            # Get column alias from the Liveboard\n",
    "            columns = fltr.get('column', [])\n",
    "            col_name = columns[0] if columns else \"Unknown\"\n",
    "            \n",
    "            # --- Link with Model to get the actual physical ID ---\n",
    "            # We look up the alias in the pre-built model_col_map \n",
    "            m_info = self.model_col_map.get(col_name, {})\n",
    "            physical_id = m_info.get('full_physical_id', \"Not Found in Model\")\n",
    "            \n",
    "            # Get the operator (e.g., 'in') [cite: 10, 12, 13]\n",
    "            operator = fltr.get('oper', \"Unknown\")\n",
    "            # Get the operator (e.g., 'in') [cite: 10, 12, 13]\n",
    "            display_name = fltr.get('display_name', \"Unknown\")\n",
    "            # Get the operator (e.g., 'in') [cite: 10, 12, 13]\n",
    "            is_single_value = fltr.get('is_single_value', \"Unknown\")\n",
    "            # Get the operator (e.g., 'in') [cite: 10, 12, 13]\n",
    "            is_mandatory = fltr.get('is_mandatory', \"Unknown\")\n",
    "            \n",
    "            # Extract and format values: wrap each in '' and the group in () [cite: 10, 11]\n",
    "            raw_values = fltr.get('values', [])\n",
    "            if raw_values:\n",
    "                # Handle strings and numbers by forcing to string and adding quotes\n",
    "                formatted_list = \", \".join([f\"'{str(v)}'\" for v in raw_values])\n",
    "                values_str = f\"({formatted_list})\"\n",
    "            else:\n",
    "                values_str = \"()\"\n",
    "            \n",
    "            rows.append({\n",
    "                \"Filter_Column\": col_name,\n",
    "                \"display_name\": display_name,         # User-facing Name\n",
    "                \"Physical_Column_ID\": physical_id, # Underlying table::column or formula_id [cite: 27, 31, 41]\n",
    "                \"Operator\": operator,\n",
    "                \"Values\": values_str,\n",
    "                \"is_single_value\" : is_single_value,\n",
    "                \"is_mandatory\" : is_mandatory\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def get_calculated_fields(self) -> pd.DataFrame:\n",
    "        root = self.model_tm.get('model') or self.model_tm.get('worksheet') or {}\n",
    "        formulas = root.get('formulas', [])\n",
    "        formula_data = []\n",
    "        for formula in formulas:\n",
    "            raw_expr = formula.get('expr', '')\n",
    "            is_nested_flag = \"formula\" in raw_expr.lower() if raw_expr else False\n",
    "            formula_data.append({\n",
    "                'id': formula.get('id', ''),\n",
    "                'name': formula.get('name', ''),\n",
    "                'expr': raw_expr,\n",
    "                'is_nested': is_nested_flag\n",
    "            })\n",
    "        return pd.DataFrame(formula_data)\n",
    "\n",
    "    def generate_column_lineage(self) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        file_name = Path(self.liveboard_path).name\n",
    "        visualizations = self.liveboard_tm.get('liveboard', {}).get('visualizations', [])\n",
    "        for viz in visualizations:\n",
    "            viz_id = viz.get('id')\n",
    "            viz_name = viz.get('answer', {}).get('name', 'Unknown')\n",
    "            viz_formulas = {form['name']: form.get('expr', '') for form in viz.get('answer', {}).get('formulas', [])}\n",
    "            answer_cols = viz.get('answer', {}).get('answer_columns', [])\n",
    "            for col in answer_cols:\n",
    "                lb_col = col.get('name')\n",
    "                if not lb_col: continue\n",
    "                clean_name = self._clean_col_name(lb_col)\n",
    "                base_model_col = clean_name\n",
    "                ft, pt, pc = \"UNKNOWN\", \"UNKNOWN\", \"UNKNOWN\"\n",
    "                agg_property = None\n",
    "                if lb_col in viz_formulas:\n",
    "                    ft, pt, pc = self._resolve_expr(viz_formulas[lb_col], viz_formulas)\n",
    "                else:\n",
    "                    m_info = self.model_col_map.get(clean_name)\n",
    "                    if m_info:\n",
    "                        agg_property = m_info.get('aggregation')\n",
    "                        if m_info['type'] == 'DIRECT':\n",
    "                            ft, pt, pc = self._get_physical_info(m_info['table_name'], m_info['physical_col_id'])\n",
    "                        elif m_info['type'] == 'FORMULA':\n",
    "                            ft, pt, pc = self._resolve_expr(m_info['expr'], viz_formulas)\n",
    "                    else:\n",
    "                        ft, pt, pc = self._resolve_expr(f\"[{clean_name}]\", viz_formulas)\n",
    "                rows.append({\n",
    "                    \"tml_file\": file_name,\n",
    "                    \"visualization_id\": viz_id,\n",
    "                    \"Visualization\": viz_name,\n",
    "                    \"Liveboard_Column\": lb_col,\n",
    "                    \"Model_Base_Column\": base_model_col,\n",
    "                    \"Model_Aggregation\": agg_property,\n",
    "                    \"DBX_Full_Table\": ft,\n",
    "                    \"Physical_Table\": pt,\n",
    "                    \"Physical_DB_Column\": pc\n",
    "                })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def generate_join_details(self) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        for t_name, t_def in self.tables_tm.items():\n",
    "            for join in t_def.get('joins_with', []):\n",
    "                t1, t2 = t_name, join.get('destination', {}).get('name')\n",
    "                on = join.get('on', '')\n",
    "                def replacer(m):\n",
    "                    tb, cl = m.group(1), m.group(2)\n",
    "                    if ' ' in cl: cl = f\"`{cl}`\"\n",
    "                    if ' ' in tb: tb = f\"`{tb}`\"\n",
    "                    return f\"{tb}.{cl}\"\n",
    "                clean_on = re.sub(r'\\[([^\\]]+)::([^\\]]+)\\]', replacer, on)\n",
    "                rows.append({\n",
    "                    \"Table_1__From\": t1,\n",
    "                    \"Table_2__To\": t2,\n",
    "                    \"Join_Type\": join.get('type', 'INNER'),\n",
    "                    \"Explicit_Condition\": clean_on,\n",
    "                    \"Relationship_Key\": join.get('name')\n",
    "                })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# EXECUTION\n",
    "# -------------------------------------------------------------------------\n",
    "BASE_PATH = \"/Volumes/dbx_migration_poc/dbx_migration_ts/lv_dashfiles_ak\" \n",
    "LIVEBOARD_FILE = f\"{BASE_PATH}/liveboard/{tml_file}\"\n",
    "\n",
    "raw_name = os.path.basename(LIVEBOARD_FILE).split('.')[0]\n",
    "asset_name = re.sub(r'[\\s\\-]+', '_', raw_name)\n",
    "\n",
    "LINEAGE_TABLE_NAME = f\"{CATALOG}.{SCHEMA}.{asset_name}_column_lineage\"\n",
    "JOINS_TABLE_NAME = f\"{CATALOG}.{SCHEMA}.{asset_name}_join_details\"\n",
    "EXPR_TABLE_NAME = f\"{CATALOG}.{SCHEMA}.{asset_name}_calculated_fields_details\"\n",
    "FILTER_TABLE_NAME = f\"{CATALOG}.{SCHEMA}.{asset_name}_filter_details\"\n",
    "\n",
    "try:\n",
    "    print(f\"Starting analysis for: {LIVEBOARD_FILE}\")\n",
    "    analyzer = TMLAnalyzer(LIVEBOARD_FILE, BASE_PATH)\n",
    "    \n",
    "    df_lineage = analyzer.generate_column_lineage()\n",
    "    df_joins = analyzer.generate_join_details()\n",
    "    df_calculated_fields = analyzer.get_calculated_fields()\n",
    "    df_filters = analyzer.generate_filter_details()\n",
    "    \n",
    "    print(f\"\\nGenerated {len(df_lineage)} lineage rows.\")\n",
    "    print(f\"Generated {len(df_joins)} join rows.\")\n",
    "    print(f\"Generated {len(df_calculated_fields)} calculated field rows.\")\n",
    "    print(f\"Generated {len(df_filters)} filter detail rows.\")\n",
    "    \n",
    "    # Save to Delta Tables (Assuming Spark environment)\n",
    "    if not df_lineage.empty:\n",
    "        spark.createDataFrame(df_lineage).write.mode(\"overwrite\").saveAsTable(LINEAGE_TABLE_NAME)\n",
    "    if not df_joins.empty:\n",
    "        spark.createDataFrame(df_joins).write.mode(\"overwrite\").saveAsTable(JOINS_TABLE_NAME)\n",
    "    if not df_calculated_fields.empty:\n",
    "        spark.createDataFrame(df_calculated_fields).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(EXPR_TABLE_NAME)\n",
    "    if not df_filters.empty:\n",
    "        spark.createDataFrame(df_filters).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(FILTER_TABLE_NAME)\n",
    "        print(f\"Saved: {FILTER_TABLE_NAME}\")\n",
    "        display(df_filters.head(5))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f921111d-de8d-4717-9514-b76a5ee36b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1 Tables Availability Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "379e0436-44d6-451e-9bc4-ac6a608e0867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f'''\n",
    "    SELECT \n",
    "        DISTINCT\n",
    "        A.Physical_Table AS Model_Table, \n",
    "        concat(table_catalog,'.',table_schema,'.',Physical_Table) AS Physical_Table\n",
    "    FROM {LINEAGE_TABLE_NAME} AS A\n",
    "    LEFT JOIN system.information_schema.tables AS B \n",
    "        ON UPPER(A.Physical_Table) = UPPER(B.table_name) \n",
    "''' \n",
    "table_availability = spark.sql(query)\n",
    "\n",
    "# 1. Filter specifically for the missing tables\n",
    "missing_tables_df = table_availability.filter(F.col(\"Physical_Table\").isNull())\n",
    "table_count = missing_tables_df.count()\n",
    "\n",
    "print(f\"{table_count} tables are not available in the target\")\n",
    "\n",
    "# 2. Logic to stop execution and return missing names\n",
    "if table_count > 0:\n",
    "    missing_list = [row.Model_Table for row in missing_tables_df.select(\"Model_Table\").collect()] \n",
    "    missing_str = \", \".join(missing_list)\n",
    "    error_message = f\"FAILURE: The following {table_count} tables are missing in Databricks: {missing_str}\"\n",
    "    print(error_message)\n",
    "    dbutils.notebook.exit(json.dumps({\n",
    "    \"Error_Message\": error_message\n",
    "}))\n",
    "print(\"All tables validated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2a646b6-702e-4c57-9949-e2f4bddf1794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2 Restructure Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1449724-c9e3-4bc6-ab37-bd65a521619c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def restructure_if_expression(expr):\n",
    "    if not isinstance(expr, str) or not expr:\n",
    "        return expr\n",
    "\n",
    "    # Search for the start of an 'if (' block\n",
    "    if_match = re.search(r'\\bif\\s*\\(', expr, flags=re.IGNORECASE)\n",
    "    if if_match and if_match.start() > 0:\n",
    "        start_idx = if_match.start()\n",
    "        \n",
    "        # Examine the part before the 'if'\n",
    "        prefix_raw = expr[:start_idx].rstrip()\n",
    "        \n",
    "        # Check if the 'if' block was wrapped in a parenthesis (e.g., \"+ ( if ... )\")\n",
    "        paren_wrap = False\n",
    "        if prefix_raw.endswith('('):\n",
    "            prefix_raw = prefix_raw[:-1].rstrip()\n",
    "            paren_wrap = True\n",
    "            \n",
    "        # Identify the operator (+, -, *, /) immediately preceding the if block\n",
    "        op_match = re.search(r'([\\+\\-\\*\\/])\\s*$', prefix_raw)\n",
    "        if op_match:\n",
    "            operator = op_match.group(1)\n",
    "            arith_part = prefix_raw[:op_match.start()].strip()\n",
    "            \n",
    "            # Extract the 'if' functional block\n",
    "            if_part = expr[start_idx:]\n",
    "            if paren_wrap:\n",
    "                # Assuming the closing parenthesis for the wrap is at the end\n",
    "                if if_part.endswith(')'):\n",
    "                    if_part = if_part[:-1].strip()\n",
    "                # Reconstruct: ( if (...) ) <operator> ( <arithmetic_part> )\n",
    "                return f\" ( {if_part} )  {operator} ( {arith_part} )\"\n",
    "            else:\n",
    "                # Reconstruct for unwrapped cases\n",
    "                return f\"( {if_part} ) {operator} ( {arith_part} )\"\n",
    "\n",
    "    return expr\n",
    "\n",
    "# Update the expr column in the dataframe\n",
    "df_calculated_fields['expr'] = df_calculated_fields['expr'].apply(restructure_if_expression)\n",
    "spark.createDataFrame(df_calculated_fields).write.mode(\"overwrite\").saveAsTable(f\"{EXPR_TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7995f684-a20e-4f4f-96ff-a453a05c676b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Conversion of TML Expressions to SQL Syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc7ad3b5-316f-4459-9111-b675932ecf63",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766161053188}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install sqlglot\n",
    "import re\n",
    "import sqlglot\n",
    "from sqlglot import exp\n",
    "\n",
    "def convert_tml_to_spark_sql(tml_expression: str) -> str:\n",
    "    \"\"\"\n",
    "    Robustly converts ThoughtSpot TML expressions to Spark SQL syntax.\n",
    "    \n",
    "    Key improvements:\n",
    "    - Converts IF-THEN-ELSE to CASE WHEN (more reliable parsing)\n",
    "    - Recursive handling for nested conditions\n",
    "    - Better suited for SQLGlot parsing\n",
    "    \"\"\"\n",
    "    if not tml_expression:\n",
    "        return None\n",
    "\n",
    "    # --- PHASE 1: TOKENIZATION (Hide Column Names) ---\n",
    "    column_map = {}\n",
    "    \n",
    "    def token_replacer(match):\n",
    "        table = match.group(1)\n",
    "        col = match.group(2)\n",
    "        \n",
    "        def quote(s):\n",
    "            # 0. Handle Safety checks\n",
    "            if not s:\n",
    "                return s\n",
    "            if s.startswith('`') and s.endswith('`'):\n",
    "                return s\n",
    "            if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', s):\n",
    "                return f\"`{s}`\"\n",
    "            return s\n",
    "        \n",
    "        if table:\n",
    "            spark_col = f\"{quote(table)}.{quote(col)}\"\n",
    "        else:\n",
    "            spark_col = quote(col)\n",
    "            \n",
    "        token = f\"__TML_COL_{len(column_map)}__\"\n",
    "        column_map[token] = spark_col\n",
    "        return token\n",
    "\n",
    "    clean_expr = re.sub(r'\\[(?:([^:]+)::)?([^\\]]+)\\]', token_replacer, tml_expression)\n",
    "\n",
    "    # --- PHASE 2: SYNTAX NORMALIZATION ---\n",
    "    clean_expr = clean_expr.replace('{', '(').replace('}', ')')\n",
    "    clean_expr = clean_expr.replace('\"', \"'\")\n",
    "    \n",
    "    # Convert unique count early\n",
    "    clean_expr = re.sub(r'unique\\s+count\\s*\\(', 'COUNT_DISTINCT(', clean_expr, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Convert unique_count_if to COUNT(DISTINCT CASE WHEN ... THEN ... END)\n",
    "    # Pattern: unique_count_if(condition, value) -> COUNT(DISTINCT CASE WHEN condition THEN value END)\n",
    "    def convert_unique_count_if(text):\n",
    "        \"\"\"Convert unique_count_if(condition, value) to Spark SQL syntax\"\"\"\n",
    "        pattern = re.compile(r'unique_count_if\\s*\\(', flags=re.IGNORECASE)\n",
    "        \n",
    "        while True:\n",
    "            match = pattern.search(text)\n",
    "            if not match:\n",
    "                break\n",
    "            \n",
    "            start_pos = match.start()\n",
    "            paren_start = match.end() - 1\n",
    "            \n",
    "            # Find the comma separating condition and value\n",
    "            depth = 1\n",
    "            i = paren_start + 1\n",
    "            comma_pos = None\n",
    "            \n",
    "            while i < len(text) and depth > 0:\n",
    "                if text[i] == '(':\n",
    "                    depth += 1\n",
    "                elif text[i] == ')':\n",
    "                    depth -= 1\n",
    "                    if depth == 0:\n",
    "                        break\n",
    "                elif text[i] == ',' and depth == 1:\n",
    "                    comma_pos = i\n",
    "                    break\n",
    "                i += 1\n",
    "            \n",
    "            if not comma_pos:\n",
    "                # No comma found, skip\n",
    "                text = text[:start_pos] + \"SKIP_UNIQUE_COUNT_IF\" + text[start_pos+15:]\n",
    "                continue\n",
    "            \n",
    "            condition = text[paren_start + 1:comma_pos].strip()\n",
    "            \n",
    "            # Find the closing paren for the value\n",
    "            depth = 1\n",
    "            i = comma_pos + 1\n",
    "            while i < len(text) and depth > 0:\n",
    "                if text[i] == '(':\n",
    "                    depth += 1\n",
    "                elif text[i] == ')':\n",
    "                    depth -= 1\n",
    "                i += 1\n",
    "            \n",
    "            value = text[comma_pos + 1:i - 1].strip()\n",
    "            \n",
    "            # Build COUNT(DISTINCT CASE WHEN condition THEN value END)\n",
    "            replacement = f\"COUNT(DISTINCT CASE WHEN {condition} THEN {value} END)\"\n",
    "            \n",
    "            text = text[:start_pos] + replacement + text[i:]\n",
    "        \n",
    "        text = text.replace(\"SKIP_UNIQUE_COUNT_IF\", \"unique_count_if\")\n",
    "        return text\n",
    "    \n",
    "    clean_expr = convert_unique_count_if(clean_expr)\n",
    "\n",
    "    # --- PHASE 2.5: CONVERT IF-THEN-ELSE TO CASE WHEN ---\n",
    "    def convert_if_to_case(text):\n",
    "        \"\"\"\n",
    "        Recursively convert IF-THEN-ELSE statements to CASE WHEN syntax.\n",
    "        Process from innermost to outermost by finding IFs without nested IFs in their condition.\n",
    "        \"\"\"\n",
    "        max_iterations = 100\n",
    "        iteration = 0\n",
    "        \n",
    "        while iteration < max_iterations:\n",
    "            iteration += 1\n",
    "            original = text\n",
    "            \n",
    "            # Find all \"if (\" positions\n",
    "            if_pattern = re.compile(r'\\bif\\s*\\(', flags=re.IGNORECASE)\n",
    "            if_matches = list(if_pattern.finditer(text))\n",
    "            \n",
    "            if not if_matches:\n",
    "                break\n",
    "            \n",
    "            # Process the LAST if (innermost/rightmost) to build from inside out\n",
    "            if_match = if_matches[-1]\n",
    "            \n",
    "            start_pos = if_match.start()\n",
    "            paren_start = if_match.end() - 1\n",
    "            \n",
    "            # Find matching ) for the condition - properly track depth\n",
    "            depth = 1\n",
    "            i = paren_start + 1\n",
    "            while i < len(text) and depth > 0:\n",
    "                if text[i] == '(':\n",
    "                    depth += 1\n",
    "                elif text[i] == ')':\n",
    "                    depth -= 1\n",
    "                i += 1\n",
    "            \n",
    "            if depth != 0:\n",
    "                # Unmatched parens, skip and mark it\n",
    "                text = text[:start_pos] + \"SKIP_IF\" + text[start_pos+2:]\n",
    "                continue\n",
    "                \n",
    "            cond_end = i - 1\n",
    "            condition = text[paren_start + 1:cond_end].strip()\n",
    "            \n",
    "            # Find \"then\" - must be right after the condition's closing paren\n",
    "            then_match = re.match(r'\\s*then\\s+', text[cond_end + 1:], flags=re.IGNORECASE)\n",
    "            if not then_match:\n",
    "                # No THEN found, skip\n",
    "                text = text[:start_pos] + \"SKIP_IF\" + text[start_pos+2:]\n",
    "                continue\n",
    "            \n",
    "            true_start = cond_end + 1 + then_match.end()\n",
    "            \n",
    "            # Find \"else\" at the same nesting level (depth 0)\n",
    "            # Count parens to know when we're at the top level\n",
    "            depth = 0\n",
    "            i = true_start\n",
    "            else_pos = None\n",
    "            true_end = len(text)\n",
    "            \n",
    "            while i < len(text):\n",
    "                if text[i] == '(':\n",
    "                    depth += 1\n",
    "                elif text[i] == ')':\n",
    "                    if depth == 0:\n",
    "                        # Hit closing paren of wrapping expression\n",
    "                        true_end = i\n",
    "                        break\n",
    "                    depth -= 1\n",
    "                elif depth == 0:\n",
    "                    # At top level - check for \"else\"\n",
    "                    if re.match(r'\\s*else\\s+', text[i:], flags=re.IGNORECASE):\n",
    "                        else_match = re.match(r'\\s*else\\s+', text[i:], flags=re.IGNORECASE)\n",
    "                        else_pos = i + else_match.end()\n",
    "                        true_end = i\n",
    "                        break\n",
    "                i += 1\n",
    "            \n",
    "            true_value = text[true_start:true_end].strip()\n",
    "            \n",
    "            if else_pos:\n",
    "                # Find end of false value - same depth tracking\n",
    "                depth = 0\n",
    "                i = else_pos\n",
    "                false_end = len(text)\n",
    "                \n",
    "                while i < len(text):\n",
    "                    if text[i] == '(':\n",
    "                        depth += 1\n",
    "                    elif text[i] == ')':\n",
    "                        if depth == 0:\n",
    "                            false_end = i\n",
    "                            break\n",
    "                        depth -= 1\n",
    "                    i += 1\n",
    "                \n",
    "                false_value = text[else_pos:false_end].strip()\n",
    "                end_pos = false_end\n",
    "            else:\n",
    "                false_value = \"NULL\"\n",
    "                end_pos = true_end\n",
    "            \n",
    "            # Build CASE WHEN statement\n",
    "            case_stmt = f\"CASE WHEN {condition} THEN {true_value} ELSE {false_value} END\"\n",
    "            \n",
    "            # Replace in text\n",
    "            text = text[:start_pos] + case_stmt + text[end_pos:]\n",
    "            \n",
    "            # If nothing changed, break to avoid infinite loop\n",
    "            if text == original:\n",
    "                break\n",
    "        \n",
    "        # Restore any SKIP_IF back to IF (ones we couldn't process)\n",
    "        text = text.replace(\"SKIP_IF\", \"if\")\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    clean_expr = convert_if_to_case(clean_expr)\n",
    "\n",
    "        # --- PHASE 3: PARSING & TRANSFORMATION ---\n",
    "    try:\n",
    "        parsed = sqlglot.parse_one(clean_expr)\n",
    "        \n",
    "        def transformer(node):\n",
    "            def func(name, args):\n",
    "                return exp.Anonymous(this=name, expressions=args)\n",
    "\n",
    "            if isinstance(node, exp.Anonymous):\n",
    "                name = node.this.lower()\n",
    "                args = node.expressions\n",
    "                \n",
    "                # FUNCTION MAPPINGS\n",
    "                if name == \"safe_divide\":\n",
    "                    return func(\"try_divide\", args)\n",
    "                if name == \"diff_days\":\n",
    "                    return func(\"datediff\", args)\n",
    "                if name == \"day_number_of_week\":\n",
    "                    return func(\"dayofweek\", args)\n",
    "                if name == \"now\":\n",
    "                    return func(\"current_timestamp\", []) \n",
    "                if name == \"isnull\":\n",
    "                    return func(\"isnull\", args)\n",
    "                if name == \"contains\":\n",
    "                    if len(args) >= 2:\n",
    "                        return exp.Like(this=args[0], expression=exp.Literal.string(f\"%{args[1].name}%\"))\n",
    "                if name == \"count_distinct\":\n",
    "                    return exp.Count(this=exp.Distinct(expressions=args))\n",
    "                     \n",
    "            return node\n",
    "\n",
    "        transformed = parsed.transform(transformer)\n",
    "        spark_sql = transformed.sql(dialect=\"spark\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Parse failed ({e}). Returning partially processed string.\")\n",
    "        spark_sql = clean_expr\n",
    "\n",
    "    # --- PHASE 4: RESTORE COLUMN NAMES ---\n",
    "    def restore(match):\n",
    "        token = match.group(0)\n",
    "        return column_map.get(token, token)\n",
    "        \n",
    "    final_sql = re.sub(r'__TML_COL_\\d+__', restore, spark_sql)\n",
    "    \n",
    "    return final_sql\n",
    "\n",
    "\n",
    "# Apply the function row-by-row to create the new column\n",
    "df_calculated_fields['spark_sql'] = df_calculated_fields['expr'].apply(convert_tml_to_spark_sql)\n",
    "spark.createDataFrame(df_calculated_fields).write.mode(\"overwrite\").saveAsTable(f\"{EXPR_TABLE_NAME}_v2\")\n",
    "\n",
    "# Verify\n",
    "display(df_calculated_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa76867c-960e-41ae-9e9e-748596be0b92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Model Object SQL Generator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b5314c-4135-42ca-b23a-622531a3eb4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.1: Hierarchy Builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf0f692-3a71-4a0b-8d0c-db935098433a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_hierarchy_column(df):\n",
    "    \"\"\"\n",
    "    Adds a 'hierarchy' column to the dataframe determining the dependency level.\n",
    "    Level 1: No dependencies.\n",
    "    Level 2: Depends on Level 1.\n",
    "    Level N: 1 + Max(dependency levels).\n",
    "    \"\"\"\n",
    "    # Create a mapping of id to spark_sql for easy lookup\n",
    "    id_to_sql = df.set_index('id')['spark_sql'].to_dict()\n",
    "    \n",
    "    # Get all IDs and sort them by length descending\n",
    "    # This ensures that when checking for dependencies, we match 'formula_Net_Ageing Group'\n",
    "    # before 'formula_Ageing Group', preventing false partial matches.\n",
    "    all_ids = sorted(df['id'].unique(), key=len, reverse=True)\n",
    "    \n",
    "    # Memoization dictionary to store calculated levels\n",
    "    levels = {}\n",
    "    \n",
    "    # Set to keep track of visiting nodes for cycle detection\n",
    "    visiting = set()\n",
    "\n",
    "    def get_level(current_id):\n",
    "        # Return memoized value if available\n",
    "        if current_id in levels:\n",
    "            return levels[current_id]\n",
    "        \n",
    "        # Cycle detection\n",
    "        if current_id in visiting:\n",
    "            raise ValueError(f\"Circular dependency detected involving {current_id}\")\n",
    "        \n",
    "        visiting.add(current_id)\n",
    "        \n",
    "        sql_text = id_to_sql.get(current_id, \"\")\n",
    "        \n",
    "        # Identify dependencies\n",
    "        # We check if other IDs appear in this SQL text.\n",
    "        # We rely on the sorted order (longest first) to avoid substring issues.\n",
    "        dependencies = []\n",
    "        temp_sql = sql_text # Work on a copy to \"consume\" found IDs\n",
    "        \n",
    "        for candidate_id in all_ids:\n",
    "            # We skip the current_id itself to avoid self-reference counting\n",
    "            if candidate_id == current_id:\n",
    "                continue\n",
    "                \n",
    "            if candidate_id in temp_sql:\n",
    "                dependencies.append(candidate_id)\n",
    "                # Remove the found ID from temp_sql so it doesn't match shorter substrings later\n",
    "                temp_sql = temp_sql.replace(candidate_id, \"\")\n",
    "        \n",
    "        # Base Case: No dependencies found\n",
    "        if not dependencies:\n",
    "            lvl = 1\n",
    "        else:\n",
    "            # Recursive Step\n",
    "            dependency_levels = [get_level(dep_id) for dep_id in dependencies]\n",
    "            lvl = 1 + max(dependency_levels)\n",
    "        \n",
    "        visiting.remove(current_id)\n",
    "        levels[current_id] = lvl\n",
    "        return lvl\n",
    "\n",
    "    # Apply the function to all rows\n",
    "    df['hierarchy'] = df['id'].apply(get_level)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the hierarchy logic first\n",
    "df_result = add_hierarchy_column(df_calculated_fields)\n",
    "\n",
    "# --- NEW STEP: Remove 'formula_' prefix from the spark_sql column ---\n",
    "# This safely replaces the string \"formula_\" with an empty string in the generated SQL\n",
    "df_result['spark_sql'] = df_result['spark_sql'].str.replace('formula_', '', regex=False)\n",
    "\n",
    "# Display result\n",
    "print(df_result[['id', 'spark_sql', 'hierarchy']].head())\n",
    "\n",
    "# Save logic\n",
    "spark.createDataFrame(df_result).write.mode(\"overwrite\").option(\"mergeSchema\",\"true\").saveAsTable(f\"{EXPR_TABLE_NAME}_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a124df6-12eb-49f5-8b9d-d1f11cd3c615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.2: SQL Generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54133579-5165-4752-926f-57548525109a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "CALC_FIELDS_TABLE_NAME = f\"{EXPR_TABLE_NAME}_v2\"\n",
    "FILTER_COLUMNS = f\"{FILTER_TABLE_NAME}\"\n",
    "\n",
    "AGGREGATE_FUNCTIONS = {\n",
    "    \"count\", \"sum\", \"avg\", \"mean\", \"min\", \"max\", \n",
    "    \"first\", \"last\", \"collect_list\", \"collect_set\", \n",
    "    \"stddev\", \"variance\", \"kurtosis\", \"skewness\", \n",
    "    \"approx_distinct\", \"corr\", \"covar_pop\", \"covar_samp\"\n",
    "}\n",
    "\n",
    "def quote_column_name(column_name):\n",
    "    if not column_name:\n",
    "        return column_name\n",
    "    if column_name.startswith('#'):\n",
    "        return f\"`{column_name}`\"\n",
    "    if column_name.startswith('`') and column_name.endswith('`'):\n",
    "        return column_name\n",
    "    if ' ' in column_name or column_name[0].isdigit():\n",
    "        return f\"`{column_name}`\"\n",
    "    return column_name\n",
    "\n",
    "def parse_filter_column_id(physical_column_id):\n",
    "    if not physical_column_id or '::' not in physical_column_id:\n",
    "        return None, None\n",
    "    parts = physical_column_id.split('::', 1)\n",
    "    table_name = parts[0].strip()\n",
    "    column_name = parts[1].strip()\n",
    "    return table_name, column_name\n",
    "\n",
    "def is_aggregate_expression(expression):\n",
    "    if not expression:\n",
    "        return False\n",
    "    for func in AGGREGATE_FUNCTIONS:\n",
    "        pattern = r'\\b' + re.escape(func) + r'\\s*\\('\n",
    "        if re.search(pattern, expression, re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def clean_spark_sql_expr(expr, alias_map=None, column_alias_map=None):\n",
    "    if not expr: \n",
    "        return expr\n",
    "    cleaned_expr = expr\n",
    "    if alias_map is not None:\n",
    "        for full_table_name, alias in alias_map.items():\n",
    "            raw_table_name = full_table_name.split('.')[-1]\n",
    "            pattern = r'\\b' + re.escape(raw_table_name) + r'\\.(?=`?[a-zA-Z0-9_#])'\n",
    "            cleaned_expr = re.sub(pattern, f\"{alias}.\", cleaned_expr)\n",
    "    else:\n",
    "        cleaned_expr = re.sub(r'\\b[a-zA-Z0-9_]+\\.(?=`?[a-zA-Z0-9_#])', '', cleaned_expr)\n",
    "    if column_alias_map is not None:\n",
    "        for physical_col, model_alias in column_alias_map.items():\n",
    "            physical_col_clean = physical_col.strip('`')\n",
    "            model_alias_quoted = quote_column_name(model_alias)\n",
    "            pattern1 = r'`' + re.escape(physical_col_clean) + r'`'\n",
    "            pattern2 = r'\\b' + re.escape(physical_col_clean) + r'\\b'\n",
    "            cleaned_expr = re.sub(pattern1, model_alias_quoted, cleaned_expr)\n",
    "            cleaned_expr = re.sub(pattern2, model_alias_quoted, cleaned_expr)\n",
    "    return cleaned_expr\n",
    "\n",
    "lineage_query = f'''\n",
    "    SELECT \n",
    "        A.Visualization, \n",
    "        A.Liveboard_Column, \n",
    "        A.Model_Base_Column, \n",
    "        concat(table_catalog,'.',table_schema,'.',Physical_Table) AS Physical_Table, \n",
    "        Physical_DB_Column,\n",
    "        C.spark_sql,\n",
    "        COALESCE(C.hierarchy, 1) AS hierarchy\n",
    "    FROM {LINEAGE_TABLE_NAME} AS A\n",
    "    LEFT JOIN system.information_schema.tables AS B \n",
    "        ON UPPER(A.Physical_Table) = UPPER(B.table_name)\n",
    "    LEFT JOIN {CALC_FIELDS_TABLE_NAME} AS C \n",
    "        ON A.Model_Base_Column = C.name\n",
    "'''\n",
    "\n",
    "filter_query = f'''\n",
    "    SELECT \n",
    "        Physical_Column_ID\n",
    "    FROM {FILTER_COLUMNS}\n",
    "'''\n",
    "\n",
    "filter_pdf = spark.sql(filter_query).toPandas()\n",
    "\n",
    "filter_columns = []\n",
    "for index, row in filter_pdf.iterrows():\n",
    "    table_name, column_name = parse_filter_column_id(row['Physical_Column_ID'])\n",
    "    if table_name and column_name:\n",
    "        filter_columns.append({\n",
    "            'raw_table': table_name,\n",
    "            'column': column_name\n",
    "        })\n",
    "\n",
    "calc_fields_query = f'''\n",
    "    SELECT \n",
    "        name,\n",
    "        spark_sql,\n",
    "        COALESCE(hierarchy, 1) AS hierarchy\n",
    "    FROM {CALC_FIELDS_TABLE_NAME}\n",
    "'''\n",
    "\n",
    "calc_fields_pdf = spark.sql(calc_fields_query).toPandas()\n",
    "\n",
    "def find_referenced_calc_fields(expression, calc_fields_df):\n",
    "    if not expression or str(expression) == 'nan' or str(expression) == 'None':\n",
    "        return set()\n",
    "    referenced = set()\n",
    "    pattern = r'`([^`]+)`'\n",
    "    matches = re.findall(pattern, str(expression))\n",
    "    for match in matches:\n",
    "        if match in calc_fields_df['name'].values:\n",
    "            referenced.add(match)\n",
    "    for calc_name in calc_fields_df['name'].values:\n",
    "        if re.search(r'\\b' + re.escape(calc_name) + r'\\b', str(expression)):\n",
    "            referenced.add(calc_name)\n",
    "    return referenced\n",
    "\n",
    "def add_filter_columns_to_select(select_list, group_by_list, selected_cols, filter_columns, \n",
    "                                   joined_tables, base_table, alias_map=None, use_table_alias=True):\n",
    "    for filter_col in filter_columns:\n",
    "        raw_table = filter_col['raw_table']\n",
    "        column_name = filter_col['column']\n",
    "        quoted_column = quote_column_name(column_name)\n",
    "        matched_table = None\n",
    "        for full_table in joined_tables.keys():\n",
    "            if full_table.endswith(raw_table) or full_table.split('.')[-1] == raw_table:\n",
    "                matched_table = full_table\n",
    "                break\n",
    "        if matched_table:\n",
    "            table_alias = joined_tables.get(matched_table, 't1')\n",
    "            unique_key = (matched_table, column_name)\n",
    "            if unique_key not in selected_cols:\n",
    "                if use_table_alias:\n",
    "                    select_clause = f\"{table_alias}.{quoted_column}\"\n",
    "                    group_by_clause = f\"{table_alias}.{quoted_column}\"\n",
    "                else:\n",
    "                    select_clause = quoted_column\n",
    "                    group_by_clause = quoted_column\n",
    "                if select_clause not in select_list:\n",
    "                    select_list.append(select_clause)\n",
    "                    group_by_list.append(group_by_clause)\n",
    "                    selected_cols.add(unique_key)\n",
    "\n",
    "if df_joins.empty:\n",
    "    print(f\"WARNING: Join Details DataFrame is empty. No Joins available\")\n",
    "    lineage_pdf = spark.sql(lineage_query).toPandas()\n",
    "    df_select = lineage_pdf\n",
    "    all_tables = df_select['Physical_Table']\n",
    "    base_table = all_tables.value_counts().index[0] if not all_tables.value_counts().empty else \"UNKNOWN_TABLE\"\n",
    "    joined_tables = {base_table: 't1'} \n",
    "    max_hierarchy = df_select['hierarchy'].max()\n",
    "    \n",
    "    if max_hierarchy == 1:\n",
    "        select_list = []\n",
    "        group_by_list = [] \n",
    "        selected_cols = set()\n",
    "        for index, row in df_select.iterrows():\n",
    "            table = row['Physical_Table']\n",
    "            db_col = quote_column_name(row['Physical_DB_Column'])\n",
    "            alias_col = quote_column_name(row['Model_Base_Column'])\n",
    "            spark_sql_expr = row['spark_sql'] \n",
    "            unique_key = (table, db_col) if not spark_sql_expr else (table, spark_sql_expr)\n",
    "            if unique_key not in selected_cols:\n",
    "                if spark_sql_expr and str(spark_sql_expr) != 'nan' and str(spark_sql_expr) != 'None':\n",
    "                    clean_expr = clean_spark_sql_expr(spark_sql_expr, alias_map=None, column_alias_map=None)\n",
    "                    select_clause = f\"{clean_expr} AS {alias_col}\"\n",
    "                    if not is_aggregate_expression(clean_expr):\n",
    "                        group_by_list.append(clean_expr)\n",
    "                else:\n",
    "                    select_clause = f\"{db_col} AS {alias_col}\"\n",
    "                    group_by_list.append(db_col)\n",
    "                select_list.append(select_clause)\n",
    "                selected_cols.add(unique_key)\n",
    "        add_filter_columns_to_select(select_list, group_by_list, selected_cols, \n",
    "                                       filter_columns, joined_tables, base_table, \n",
    "                                       alias_map=None, use_table_alias=False)\n",
    "        select_statement = \",\\n  \".join(select_list)\n",
    "        group_by_clause = \"\"\n",
    "        if group_by_list:\n",
    "            group_by_clause = \"\\nGROUP BY\\n  \" + \",\\n  \".join(group_by_list)\n",
    "        final_sql_query = (\n",
    "            \"SELECT\\n\"\n",
    "            f\"  {select_statement}\\n\"\n",
    "            \"FROM\\n\"\n",
    "            f\"  {base_table}\"\n",
    "            f\"{group_by_clause};\"\n",
    "        )\n",
    "    else:\n",
    "        cte_queries = []\n",
    "        calc_fields_by_level = {}\n",
    "        fields_needed = set()\n",
    "        column_alias_map = {}\n",
    "        for index, row in df_select.iterrows():\n",
    "            spark_sql_expr = row['spark_sql']\n",
    "            if not spark_sql_expr or str(spark_sql_expr) == 'nan' or str(spark_sql_expr) == 'None':\n",
    "                physical_col = row['Physical_DB_Column']\n",
    "                model_col = row['Model_Base_Column']\n",
    "                column_alias_map[physical_col] = model_col\n",
    "        for index, row in df_select.iterrows():\n",
    "            alias_col = row['Model_Base_Column']\n",
    "            spark_sql_expr = row['spark_sql']\n",
    "            hierarchy_level = row['hierarchy']\n",
    "            if spark_sql_expr and str(spark_sql_expr) != 'nan' and str(spark_sql_expr) != 'None':\n",
    "                fields_needed.add(alias_col)\n",
    "                referenced = find_referenced_calc_fields(spark_sql_expr, calc_fields_pdf)\n",
    "                fields_needed.update(referenced)\n",
    "        for index, row in calc_fields_pdf.iterrows():\n",
    "            calc_name = row['name']\n",
    "            calc_expr = row['spark_sql']\n",
    "            calc_hierarchy = row['hierarchy']\n",
    "            if calc_name in fields_needed:\n",
    "                if calc_hierarchy not in calc_fields_by_level:\n",
    "                    calc_fields_by_level[calc_hierarchy] = []\n",
    "                calc_fields_by_level[calc_hierarchy].append({\n",
    "                    'name': calc_name,\n",
    "                    'expr': calc_expr,\n",
    "                    'hierarchy': calc_hierarchy\n",
    "                })\n",
    "        for level in range(1, int(max_hierarchy) + 1):\n",
    "            level_data = df_select[df_select['hierarchy'] == level]\n",
    "            select_list = []\n",
    "            group_by_list = []\n",
    "            selected_cols = set()\n",
    "            if level == 1:\n",
    "                for index, row in df_select.iterrows():\n",
    "                    spark_sql_expr = row['spark_sql']\n",
    "                    if not spark_sql_expr or str(spark_sql_expr) == 'nan' or str(spark_sql_expr) == 'None':\n",
    "                        table = row['Physical_Table']\n",
    "                        db_col = quote_column_name(row['Physical_DB_Column'])\n",
    "                        alias_col = quote_column_name(row['Model_Base_Column'])\n",
    "                        unique_key = (table, db_col)\n",
    "                        if unique_key not in selected_cols:\n",
    "                            select_clause = f\"{db_col} AS {alias_col}\"\n",
    "                            group_by_list.append(db_col)\n",
    "                            select_list.append(select_clause)\n",
    "                            selected_cols.add(unique_key)\n",
    "                for filter_col in filter_columns:\n",
    "                    raw_table = filter_col['raw_table']\n",
    "                    column_name = filter_col['column']\n",
    "                    quoted_column = quote_column_name(column_name)\n",
    "                    matched_table = None\n",
    "                    for full_table in joined_tables.keys():\n",
    "                        if full_table.endswith(raw_table) or full_table.split('.')[-1] == raw_table:\n",
    "                            matched_table = full_table\n",
    "                            break\n",
    "                    if matched_table:\n",
    "                        unique_key = (matched_table, column_name)\n",
    "                        if unique_key not in selected_cols:\n",
    "                            select_list.append(quoted_column)\n",
    "                            group_by_list.append(quoted_column)\n",
    "                            selected_cols.add(unique_key)\n",
    "            if level in calc_fields_by_level:\n",
    "                for calc_field in calc_fields_by_level[level]:\n",
    "                    calc_name = quote_column_name(calc_field['name'])\n",
    "                    calc_expr = calc_field['expr']\n",
    "                    if calc_name not in [s.split(' AS ')[-1] for s in select_list]:\n",
    "                        if level == 1:\n",
    "                            clean_expr = clean_spark_sql_expr(calc_expr, alias_map=None, column_alias_map=None)\n",
    "                        else:\n",
    "                            clean_expr = clean_spark_sql_expr(calc_expr, alias_map=None, column_alias_map=column_alias_map)\n",
    "                        select_clause = f\"{clean_expr} AS {calc_name}\"\n",
    "                        if not is_aggregate_expression(clean_expr):\n",
    "                            group_by_list.append(clean_expr)\n",
    "                        select_list.append(select_clause)\n",
    "            if level > 1:\n",
    "                for prev_level in range(1, level):\n",
    "                    if prev_level in calc_fields_by_level:\n",
    "                        for calc_field in calc_fields_by_level[prev_level]:\n",
    "                            calc_name = quote_column_name(calc_field['name'])\n",
    "                            if calc_name not in [s.split(' AS ')[-1] for s in select_list]:\n",
    "                                select_list.append(calc_name)\n",
    "                                group_by_list.append(calc_name) \n",
    "                for index, row in df_select.iterrows():\n",
    "                    spark_sql_expr = row['spark_sql']\n",
    "                    if not spark_sql_expr or str(spark_sql_expr) == 'nan' or str(spark_sql_expr) == 'None':\n",
    "                        alias_col = quote_column_name(row['Model_Base_Column'])\n",
    "                        if alias_col not in [s.split(' AS ')[-1] for s in select_list]:\n",
    "                            select_list.append(alias_col)\n",
    "                            group_by_list.append(alias_col)\n",
    "                for filter_col in filter_columns:\n",
    "                    column_name = filter_col['column']\n",
    "                    quoted_column = quote_column_name(column_name)\n",
    "                    if quoted_column not in select_list:\n",
    "                        select_list.append(quoted_column)\n",
    "                        group_by_list.append(quoted_column)\n",
    "            select_statement = \",\\n    \".join(select_list)\n",
    "            group_by_clause = \"\"\n",
    "            if group_by_list:\n",
    "                group_by_clause = \"\\n  GROUP BY\\n    \" + \",\\n    \".join(group_by_list)\n",
    "            from_clause = base_table if level == 1 else f\"cte_level_{level-1}\"\n",
    "            cte_query = (\n",
    "                f\"  cte_level_{level} AS (\\n\"\n",
    "                f\"    SELECT\\n\"\n",
    "                f\"      {select_statement}\\n\"\n",
    "                f\"    FROM\\n\"\n",
    "                f\"      {from_clause}\"\n",
    "                f\"{group_by_clause}\\n\"\n",
    "                f\"  )\"\n",
    "            )\n",
    "            cte_queries.append(cte_query)\n",
    "        with_clause = \"WITH\\n\" + \",\\n\".join(cte_queries)\n",
    "        final_sql_query = (\n",
    "            f\"{with_clause}\\n\"\n",
    "            f\"SELECT * FROM cte_level_{int(max_hierarchy)};\"\n",
    "        )\n",
    "else:\n",
    "    df_joins_pdf = spark.sql(f'''\n",
    "    WITH CTE AS (\n",
    "    SELECT Table_1__From, \n",
    "        Table_2__To, \n",
    "        CASE WHEN UPPER(Join_Type) == 'OUTER' THEN 'FULL OUTER' ELSE Join_Type END AS Join_Type, \n",
    "        Explicit_Condition, \n",
    "        Relationship_Key,\n",
    "        b.table_catalog as tbl1_catalog,\n",
    "        b.table_schema as tbl1_schema,\n",
    "        C.table_catalog as tbl2_catalog,\n",
    "        C.table_schema as tbl2_schema\n",
    "        FROM {JOINS_TABLE_NAME} AS A\n",
    "        LEFT JOIN system.information_schema.tables AS B ON UPPER(A.Table_1__From) = UPPER(B.table_name)\n",
    "        LEFT JOIN system.information_schema.tables AS C ON UPPER(A.Table_2__To) = UPPER(C.table_name))\n",
    "        SELECT \n",
    "        concat(tbl1_catalog,'.',tbl1_schema,'.',Table_1__From) AS Table_1__From,\n",
    "        concat(tbl2_catalog,'.',tbl2_schema,'.',Table_2__To) AS Table_2__To,\n",
    "        Join_Type, \n",
    "        Explicit_Condition, \n",
    "        Relationship_Key\n",
    "    FROM CTE''').toPandas()\n",
    "    lineage_pdf = spark.sql(lineage_query).toPandas()\n",
    "    df_joins = df_joins_pdf\n",
    "    df_select = lineage_pdf\n",
    "    all_tables = df_joins['Table_1__From']\n",
    "    table_counts = all_tables.value_counts()\n",
    "    base_table = table_counts.index[0] if not table_counts.empty else df_joins.iloc[0]['Table_1__From']\n",
    "    if not table_counts[table_counts > 1].empty:\n",
    "        base_table = table_counts[table_counts > 1].index[0]\n",
    "    joined_tables = {base_table: 't1'}\n",
    "    alias_counter = 2\n",
    "    sql_joins = []\n",
    "    joins_to_process = df_joins.to_dict('records')\n",
    "    while joins_to_process:\n",
    "        processed_count = 0\n",
    "        for row in list(joins_to_process):\n",
    "            table1 = row['Table_1__From']\n",
    "            table2 = row['Table_2__To']\n",
    "            join_type = row['Join_Type'].replace('_', ' ')\n",
    "            condition = row['Explicit_Condition']\n",
    "            new_table = None\n",
    "            if table1 in joined_tables and table2 not in joined_tables:\n",
    "                current_table = table1\n",
    "                new_table = table2\n",
    "            elif table2 in joined_tables and table1 not in joined_tables:\n",
    "                current_table = table2\n",
    "                new_table = table1\n",
    "            if new_table:\n",
    "                new_alias = f't{alias_counter}'\n",
    "                joined_tables[new_table] = new_alias\n",
    "                alias_counter += 1\n",
    "                rawtable1 = table1.split('.')[-1]\n",
    "                rawtable2 = table2.split('.')[-1]\n",
    "                aliased_condition = condition.replace(f'{rawtable1}.', f'{joined_tables.get(table1, table1)}.')\n",
    "                aliased_condition = aliased_condition.replace(f'{rawtable2}.', f'{joined_tables.get(table2, table2)}.')\n",
    "                join_target = table2 if new_table == table2 else table1\n",
    "                sql_joins.append(f\"{join_type} JOIN {join_target} AS {new_alias} ON {aliased_condition}\")\n",
    "                joins_to_process.remove(row)\n",
    "                processed_count += 1\n",
    "            elif table1 in joined_tables and table2 in joined_tables:\n",
    "                joins_to_process.remove(row)\n",
    "        if processed_count == 0 and joins_to_process:\n",
    "            break \n",
    "    max_hierarchy = df_select['hierarchy'].max()\n",
    "    \n",
    "    if max_hierarchy == 1:\n",
    "        select_list = []\n",
    "        group_by_list = []\n",
    "        selected_cols = set()\n",
    "        for index, row in df_select.iterrows():\n",
    "            table = row['Physical_Table']\n",
    "            db_col = quote_column_name(row['Physical_DB_Column'])\n",
    "            alias_col = quote_column_name(row['Model_Base_Column'])\n",
    "            spark_sql_expr = row['spark_sql'] \n",
    "            if table in joined_tables:\n",
    "                table_alias = joined_tables[table]\n",
    "                unique_key = (table, db_col) if not spark_sql_expr else (table, spark_sql_expr)\n",
    "                if unique_key not in selected_cols:\n",
    "                    if spark_sql_expr and str(spark_sql_expr) != 'nan' and str(spark_sql_expr) != 'None':\n",
    "                        clean_expr = clean_spark_sql_expr(spark_sql_expr, alias_map=joined_tables, column_alias_map=None)\n",
    "                        select_clause = f\"{clean_expr} AS {alias_col}\"\n",
    "                        if not is_aggregate_expression(clean_expr):\n",
    "                             group_by_list.append(clean_expr)\n",
    "                    else:\n",
    "                        select_clause = f\"{table_alias}.{db_col} AS {alias_col}\"\n",
    "                        group_by_list.append(f\"{table_alias}.{db_col}\")\n",
    "                    select_list.append(select_clause)\n",
    "                    selected_cols.add(unique_key)\n",
    "        add_filter_columns_to_select(select_list, group_by_list, selected_cols, \n",
    "                                       filter_columns, joined_tables, base_table, \n",
    "                                       alias_map=joined_tables, use_table_alias=True)\n",
    "        select_statement = \",\\n  \".join(select_list)\n",
    "        join_block = \"\\n  \".join(sql_joins)\n",
    "        group_by_clause = \"\"\n",
    "        if group_by_list:\n",
    "            group_by_clause = \"\\nGROUP BY\\n  \" + \",\\n  \".join(group_by_list)\n",
    "        final_sql_query = (\n",
    "            \"SELECT\\n\"\n",
    "            f\"  {select_statement}\\n\"\n",
    "            \"FROM\\n\"\n",
    "            f\"  {base_table} AS t1\\n\"\n",
    "            f\"  {join_block}\"\n",
    "            f\"{group_by_clause};\"\n",
    "        )\n",
    "    else:\n",
    "        cte_queries = []\n",
    "        join_block = \"\\n    \".join(sql_joins)\n",
    "        calc_fields_by_level = {}\n",
    "        fields_needed = set()\n",
    "        column_alias_map = {}\n",
    "        for index, row in df_select.iterrows():\n",
    "            spark_sql_expr = row['spark_sql']\n",
    "            if not spark_sql_expr or str(spark_sql_expr) == 'nan' or str(spark_sql_expr) == 'None':\n",
    "                physical_col = row['Physical_DB_Column']\n",
    "                model_col = row['Model_Base_Column']\n",
    "                column_alias_map[physical_col] = model_col\n",
    "        for index, row in df_select.iterrows():\n",
    "            alias_col = row['Model_Base_Column']\n",
    "            spark_sql_expr = row['spark_sql']\n",
    "            hierarchy_level = row['hierarchy']\n",
    "            if spark_sql_expr and str(spark_sql_expr) != 'nan' and str(spark_sql_expr) != 'None':\n",
    "                fields_needed.add(alias_col)\n",
    "                referenced = find_referenced_calc_fields(spark_sql_expr, calc_fields_pdf)\n",
    "                fields_needed.update(referenced)\n",
    "        for index, row in calc_fields_pdf.iterrows():\n",
    "            calc_name = row['name']\n",
    "            calc_expr = row['spark_sql']\n",
    "            calc_hierarchy = row['hierarchy']\n",
    "            if calc_name in fields_needed:\n",
    "                if calc_hierarchy not in calc_fields_by_level:\n",
    "                    calc_fields_by_level[calc_hierarchy] = []\n",
    "                calc_fields_by_level[calc_hierarchy].append({\n",
    "                    'name': calc_name,\n",
    "                    'expr': calc_expr,\n",
    "                    'hierarchy': calc_hierarchy\n",
    "                })\n",
    "        for level in range(1, int(max_hierarchy) + 1):\n",
    "            select_list = []\n",
    "            group_by_list = []\n",
    "            selected_cols = set()\n",
    "            if level == 1:\n",
    "                for index, row in df_select.iterrows():\n",
    "                    spark_sql_expr = row['spark_sql']\n",
    "                    if not spark_sql_expr or str(spark_sql_expr) == 'nan' or str(spark_sql_expr) == 'None':\n",
    "                        table = row['Physical_Table']\n",
    "                        db_col = quote_column_name(row['Physical_DB_Column'])\n",
    "                        alias_col = quote_column_name(row['Model_Base_Column'])\n",
    "                        if table in joined_tables:\n",
    "                            table_alias = joined_tables[table]\n",
    "                            unique_key = (table, db_col)\n",
    "                            if unique_key not in selected_cols:\n",
    "                                select_clause = f\"{table_alias}.{db_col} AS {alias_col}\"\n",
    "                                group_by_list.append(f\"{table_alias}.{db_col}\")\n",
    "                                select_list.append(select_clause)\n",
    "                                selected_cols.add(unique_key)\n",
    "                for filter_col in filter_columns:\n",
    "                    raw_table = filter_col['raw_table']\n",
    "                    column_name = filter_col['column']\n",
    "                    quoted_column = quote_column_name(column_name)\n",
    "                    matched_table = None\n",
    "                    for full_table in joined_tables.keys():\n",
    "                        if full_table.endswith(raw_table) or full_table.split('.')[-1] == raw_table:\n",
    "                            matched_table = full_table\n",
    "                            break\n",
    "                    if matched_table:\n",
    "                        table_alias = joined_tables.get(matched_table, 't1')\n",
    "                        unique_key = (matched_table, column_name)\n",
    "                        if unique_key not in selected_cols:\n",
    "                            select_clause = f\"{table_alias}.{quoted_column}\"\n",
    "                            group_by_clause = f\"{table_alias}.{quoted_column}\"\n",
    "                            select_list.append(select_clause)\n",
    "                            group_by_list.append(group_by_clause)\n",
    "                            selected_cols.add(unique_key)\n",
    "            if level in calc_fields_by_level:\n",
    "                for calc_field in calc_fields_by_level[level]:\n",
    "                    calc_name = quote_column_name(calc_field['name'])\n",
    "                    calc_expr = calc_field['expr']\n",
    "                    if calc_name not in [s.split(' AS ')[-1] for s in select_list]:\n",
    "                        if level == 1:\n",
    "                            clean_expr = clean_spark_sql_expr(calc_expr, alias_map=joined_tables, column_alias_map=None)\n",
    "                        else:\n",
    "                            clean_expr = clean_spark_sql_expr(calc_expr, alias_map=None, column_alias_map=column_alias_map)\n",
    "                        select_clause = f\"{clean_expr} AS {calc_name}\"\n",
    "                        if not is_aggregate_expression(clean_expr):\n",
    "                            group_by_list.append(clean_expr)\n",
    "                        select_list.append(select_clause)\n",
    "            if level > 1:\n",
    "                for prev_level in range(1, level):\n",
    "                    if prev_level in calc_fields_by_level:\n",
    "                        for calc_field in calc_fields_by_level[prev_level]:\n",
    "                            calc_name = quote_column_name(calc_field['name'])\n",
    "                            if calc_name not in [s.split(' AS ')[-1] for s in select_list]:\n",
    "                                select_list.append(calc_name)\n",
    "                                group_by_list.append(calc_name)\n",
    "                for index, row in df_select.iterrows():\n",
    "                    spark_sql_expr = row['spark_sql']\n",
    "                    if not spark_sql_expr or str(spark_sql_expr) == 'nan' or str(spark_sql_expr) == 'None':\n",
    "                        alias_col = quote_column_name(row['Model_Base_Column'])\n",
    "                        if alias_col not in [s.split(' AS ')[-1] for s in select_list]:\n",
    "                            select_list.append(alias_col)\n",
    "                            group_by_list.append(alias_col)\n",
    "                for filter_col in filter_columns:\n",
    "                    column_name = filter_col['column']\n",
    "                    quoted_column = quote_column_name(column_name)\n",
    "                    if quoted_column not in select_list:\n",
    "                        select_list.append(quoted_column)\n",
    "                        group_by_list.append(quoted_column)\n",
    "            select_statement = \",\\n    \".join(select_list)\n",
    "            group_by_clause = \"\"\n",
    "            if group_by_list:\n",
    "                group_by_clause = \"\\n  GROUP BY\\n    \" + \",\\n    \".join(group_by_list)\n",
    "            if level == 1:\n",
    "                from_clause = f\"{base_table} AS t1\\n    {join_block}\"\n",
    "            else:\n",
    "                from_clause = f\"cte_level_{level-1}\"\n",
    "            cte_query = (\n",
    "                f\"  cte_level_{level} AS (\\n\"\n",
    "                f\"    SELECT\\n\"\n",
    "                f\"      {select_statement}\\n\"\n",
    "                f\"    FROM\\n\"\n",
    "                f\"      {from_clause}\"\n",
    "                f\"{group_by_clause}\\n\"\n",
    "                f\"  )\"\n",
    "            )\n",
    "            cte_queries.append(cte_query)\n",
    "        with_clause = \"WITH\\n\" + \",\\n\".join(cte_queries)\n",
    "        final_sql_query = (\n",
    "            f\"{with_clause}\\n\"\n",
    "            f\"SELECT * FROM cte_level_{int(max_hierarchy)};\"\n",
    "        )\n",
    "\n",
    "pyspark_code = f\"\"\"\n",
    "sql_query = \\\"\\\"\\\"\n",
    "{final_sql_query}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "result_df = spark.sql(sql_query)\n",
    "result_df.display()\n",
    "\"\"\"\n",
    "\n",
    "print(pyspark_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3adec7-6458-4988-9cb7-457cc932997b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Saving the SQL query as File and View Creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76070320-630f-49c3-81bd-0a780754d8e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TML_BASE_DIR = '/Volumes/dbx_migration_poc/dbx_migration_ts/lv_dashfiles_ak'\n",
    "Query_FILE_PATH = f\"{TML_BASE_DIR}/SqlOutputs\"\n",
    "file_name = f\"{asset_name}.sql\"\n",
    "full_file_path = os.path.join(Query_FILE_PATH, file_name)\n",
    "# --- Incase Path NA---\n",
    "os.makedirs(Query_FILE_PATH, exist_ok=True)\n",
    "try:\n",
    "    with open(full_file_path, 'w') as f:\n",
    "        f.write(final_sql_query)\n",
    "    \n",
    "    print(f\"Successfully saved the query to: {full_file_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to Volume: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cad9909-d2c8-4614-973e-05d29ec4d9da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {CATALOG}.{SCHEMA}.{asset_name}_View\n",
    "AS {final_sql_query}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "114814b0-1238-4629-afa1-edcaceed641e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Unified Dataset Creation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f82c3e0c-b48c-4b9f-875d-db3e35e2dc8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.1 Dataset SQL creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a3ab1d-8680-4d18-bd51-8ccd6fc04353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Model and Dataset naming\n",
    "model_name = f\"{CATALOG}.{SCHEMA}.{asset_name}_View\"\n",
    "unified_dataset_name = f\"{asset_name}_Unified_Dataset\"\n",
    "\n",
    "def map_aggregation_to_spark(agg):\n",
    "  if not agg or pd.isna(agg): return None\n",
    "  agg_upper = str(agg).upper().strip()\n",
    "  mapping = {\n",
    "    \"MAXIMUM\": \"MAX\", \"MINIMUM\": \"MIN\", \"AVERAGE\": \"AVG\", \"MEAN\": \"AVG\",\n",
    "    \"UNIQUE COUNT\": \"COUNT_DISTINCT\", \"COUNT DISTINCT\": \"COUNT_DISTINCT\",\n",
    "    \"SUM\": \"SUM\", \"COUNT\": \"COUNT\", \"STD_DEV\": \"STDDEV\", \"VARIANCE\": \"VARIANCE\"\n",
    "  }\n",
    "  return mapping.get(agg_upper, agg_upper)\n",
    "\n",
    "def infer_aggregation_and_clean_name(col_name):\n",
    "  col_lower = col_name.lower()\n",
    "  prefixes = [\n",
    "    (\"maximum \", \"MAX\"), (\"minimum \", \"MIN\"), (\"average \", \"AVG\"), (\"avg \", \"AVG\"),\n",
    "    (\"sum \", \"SUM\"), (\"count of \", \"COUNT\"), (\"count \", \"COUNT\"),\n",
    "    (\"unique count of \", \"COUNT_DISTINCT\"), (\"unique count \", \"COUNT_DISTINCT\"),\n",
    "    (\"unique number of \", \"COUNT_DISTINCT\") \n",
    "  ]\n",
    "  for prefix, func in prefixes:\n",
    "    if col_lower.startswith(prefix):\n",
    "      clean_name = col_name[len(prefix):].strip()\n",
    "      return func, clean_name\n",
    "  return None, col_name\n",
    "\n",
    "def quote_identifier(name):\n",
    "  if not name: return name\n",
    "  if re.match(r'^[a-zA-Z0-9_]+$', name): return name\n",
    "  return f\"`{name}`\"\n",
    "\n",
    "def format_sql_agg(func, col_expr):\n",
    "  if func == \"COUNT_DISTINCT\":\n",
    "    return f\"COUNT(DISTINCT {col_expr})\"\n",
    "  return f\"{func}({col_expr})\"\n",
    "\n",
    "def parse_filter_column_id(physical_column_id):\n",
    "  if not physical_column_id or '::' not in physical_column_id:\n",
    "    return None, None\n",
    "  parts = physical_column_id.split('::', 1)\n",
    "  return parts[0].strip(), parts[1].strip()\n",
    "\n",
    "def load_filter_columns():\n",
    "  try:\n",
    "    filter_query = f\"SELECT Physical_Column_ID FROM {FILTER_TABLE_NAME}\"\n",
    "    filter_pdf = spark.sql(filter_query).toPandas()\n",
    "    filter_columns = []\n",
    "    for _, row in filter_pdf.iterrows():\n",
    "      table_name, column_name = parse_filter_column_id(row['Physical_Column_ID'])\n",
    "      if table_name and column_name:\n",
    "        filter_columns.append({\n",
    "          'column': column_name,\n",
    "          'quoted_column': quote_identifier(column_name)\n",
    "        })\n",
    "    return filter_columns\n",
    "  except Exception as e:\n",
    "    print(f\"Warning: Could not load filter columns: {e}\")\n",
    "    return []\n",
    "\n",
    "def generate_unified_query(df, filter_columns):\n",
    "  df_unique = df[['Liveboard_Column', 'Model_Aggregation']].drop_duplicates()\n",
    "  \n",
    "  select_items = {}\n",
    "  group_by_cols = set()\n",
    "  has_aggregation = False\n",
    "  \n",
    "  # Helper to normalize names for deduplication check (removes all non-alphanumeric)\n",
    "  def normalize(name):\n",
    "      return re.sub(r'[^a-zA-Z0-9]', '', name).lower()\n",
    "\n",
    "  existing_cleaned_names = set()\n",
    "\n",
    "  for _, row in df_unique.iterrows():\n",
    "    raw_col = row['Liveboard_Column']\n",
    "    meta_agg = map_aggregation_to_spark(row['Model_Aggregation'])\n",
    "    inferred_agg, clean_name = infer_aggregation_and_clean_name(raw_col)\n",
    "    \n",
    "    final_expr = \"\"\n",
    "    source_col_for_alias = \"\"\n",
    "\n",
    "    if meta_agg and inferred_agg:\n",
    "      # Aggregated items\n",
    "      final_expr = format_sql_agg(inferred_agg, quote_identifier(clean_name))\n",
    "      source_col_for_alias = clean_name\n",
    "      has_aggregation = True\n",
    "    elif meta_agg or inferred_agg:\n",
    "      # Single aggregation items\n",
    "      source_col_for_alias = clean_name if inferred_agg else raw_col\n",
    "      final_expr = quote_identifier(source_col_for_alias)\n",
    "      group_by_cols.add(final_expr)\n",
    "    else:\n",
    "      # Dimensions: Strip wrappers like Month(Date)\n",
    "      unwrapped_col = re.sub(r'^\\w+\\s*\\(\\s*(.+?)\\s*\\)$', r'\\1', raw_col)\n",
    "      source_col_for_alias = unwrapped_col\n",
    "      final_expr = quote_identifier(unwrapped_col)\n",
    "      group_by_cols.add(final_expr)\n",
    "    \n",
    "    # Generate Alias from the unwrapped column name\n",
    "    alias = re.sub(r'[^a-zA-Z0-9]+', '_', source_col_for_alias).strip('_')\n",
    "    \n",
    "    # Store in select list and track for deduplication\n",
    "    select_items[alias] = f\"{final_expr} AS {alias}\"\n",
    "    existing_cleaned_names.add(normalize(source_col_for_alias))\n",
    "\n",
    "  # Add filter columns to select if not already present\n",
    "  for f_col in filter_columns:\n",
    "    f_raw = f_col['column']\n",
    "    f_cleaned = normalize(f_raw)\n",
    "    \n",
    "    if f_cleaned not in existing_cleaned_names:\n",
    "      # Generate alias for filter columns (e.g. \"Start Date\" -> Start_Date)\n",
    "      f_alias = re.sub(r'[^a-zA-Z0-9]+', '_', f_raw).strip('_')\n",
    "      select_items[f_alias] = f\"{f_col['quoted_column']} AS {f_alias}\"\n",
    "      group_by_cols.add(f_col['quoted_column'])\n",
    "      existing_cleaned_names.add(f_cleaned)\n",
    "\n",
    "  # Assemble SQL\n",
    "  select_clause = \"SELECT \" + \", \".join(select_items.values())\n",
    "  sql = f\"{select_clause} FROM {model_name}\"\n",
    "  \n",
    "  if has_aggregation and group_by_cols:\n",
    "    sql += \" GROUP BY \" + \", \".join(group_by_cols)\n",
    "    \n",
    "  return sql\n",
    "\n",
    "# --- Main Execution ---\n",
    "df_lineage = spark.table(LINEAGE_TABLE_NAME).toPandas()\n",
    "filter_columns = load_filter_columns()\n",
    "\n",
    "common_query = generate_unified_query(df_lineage, filter_columns)\n",
    "\n",
    "dashboard_queries = df_lineage[['visualization_id', 'Visualization']].drop_duplicates().copy()\n",
    "dashboard_queries.rename(columns={'Visualization': 'visualization_name'}, inplace=True)\n",
    "dashboard_queries['common_sql_query'] = common_query\n",
    "dashboard_queries['common_dataset_name'] = unified_dataset_name\n",
    "\n",
    "display(dashboard_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b79c3b9-b902-44d7-ac66-286d8d4705bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.2: Mapping Table creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14351d05-a8bd-4d3a-af9a-c75d69e05d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, TimestampType\n",
    "\n",
    "# --- Configuration ---\n",
    "CATALOG = \"dbx_migration_poc\"\n",
    "SCHEMA = \"dbx_migration_ts\"\n",
    "TML_VOLUME = \"lv_dashfiles_ak\"\n",
    "\n",
    "TML_INPUT_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{TML_VOLUME}/liveboard\"\n",
    "MAPPING_TABLE = f\"{CATALOG}.{SCHEMA}.tml_dbx_metadata_mapping\"\n",
    "FAILURE_LOG_TABLE = f\"{CATALOG}.{SCHEMA}.tml_dbx_mapping_failures\"\n",
    "\n",
    "\n",
    "# Setup Functions\n",
    "def setup_failure_log_table():\n",
    "    \"\"\"Create or recreate the failure log table.\"\"\"\n",
    "    parts = FAILURE_LOG_TABLE.split('.')\n",
    "    catalog = parts[0]\n",
    "    schema = parts[1]\n",
    "    table_name = parts[2]\n",
    "    \n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog}`.`{schema}`\")\n",
    "    \n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS `{catalog}`.`{schema}`.`{table_name}`\")\n",
    "        print(f\"Dropped existing failure log table: {table_name}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    create_sql = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{catalog}`.`{schema}`.`{table_name}` (\n",
    "            tml_file STRING,\n",
    "            error_type STRING,\n",
    "            error_message STRING,\n",
    "            failure_timestamp TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(create_sql)\n",
    "    print(f\"Created failure log table: {table_name}\")\n",
    "\n",
    "def setup_mapping_table():\n",
    "    \"\"\"Create or recreate the metadata mapping table.\"\"\"\n",
    "    parts = MAPPING_TABLE.split('.')\n",
    "    catalog = parts[0]\n",
    "    schema = parts[1]\n",
    "    table_name = parts[2]\n",
    "    \n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{catalog}`.`{schema}`\")\n",
    "    \n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS `{catalog}`.`{schema}`.`{table_name}`\")\n",
    "        print(f\"Dropped existing table: {table_name}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    create_sql = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{catalog}`.`{schema}`.`{table_name}` (\n",
    "            tml_file STRING,\n",
    "            visualization_id STRING,\n",
    "            visualization_name STRING,\n",
    "            chart_type STRING,\n",
    "            tml_table_name STRING,\n",
    "            tml_table_id STRING,\n",
    "            tml_columns_used ARRAY<STRING>,\n",
    "            tml_columns_raw ARRAY<STRING>,\n",
    "            databricks_table_name_ToBeFilled STRING COMMENT 'For unique datasets per viz',\n",
    "            databricks_column_mapping_ToBeFilled STRING COMMENT 'For unique datasets per viz - JSON format',\n",
    "            common_dataset_name STRING COMMENT 'Shared dataset name for reuse across visualizations',\n",
    "            common_sql_query STRING COMMENT 'Common SQL query for the shared dataset',\n",
    "            common_column_mapping STRING COMMENT 'JSON mapping of common columns for shared dataset',\n",
    "            search_query STRING,\n",
    "            notes STRING,\n",
    "            extraction_timestamp TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(create_sql)\n",
    "    print(f\"Created mapping table: {table_name}\")\n",
    "\n",
    "def parse_tml_file(file_path):\n",
    "    \"\"\"Parse TML file (YAML or JSON).\"\"\"\n",
    "    content = dbutils.fs.head(file_path, 10 * 1024 * 1024)\n",
    "    try:\n",
    "        return yaml.safe_load(content)\n",
    "    except yaml.YAMLError:\n",
    "        return json.loads(content)\n",
    "\n",
    "# Metadata Extraction Functions\n",
    "def extract_columns_from_answer(answer: Dict) -> List[str]:\n",
    "    \"\"\"Extract all column names used in the answer.\"\"\"\n",
    "    columns = []\n",
    "    for col in answer.get('answer_columns', []):\n",
    "        col_name = col.get('name')\n",
    "        if col_name:\n",
    "            columns.append(col_name)\n",
    "    table_cols = answer.get('table', {}).get('ordered_column_ids', [])\n",
    "    columns.extend([c for c in table_cols if c and c not in columns])\n",
    "    return columns\n",
    "\n",
    "def extract_table_info(answer: Dict) -> tuple:\n",
    "    \"\"\"Extract table name and ID from answer.\"\"\"\n",
    "    tables = answer.get('tables', [])\n",
    "    if tables and len(tables) > 0:\n",
    "        first_table = tables[0]\n",
    "        return (first_table.get('name', ''), first_table.get('id', ''))\n",
    "    return ('', '')\n",
    "\n",
    "def clean_field_name(field_name: str) -> str:\n",
    "    \"\"\"Remove aggregate prefixes and date wrappers.\"\"\"\n",
    "    if not field_name:\n",
    "        return \"\"\n",
    "    name = re.sub(r'^(Total |Maximum |Minimum |Average |Unique Number of )\\s*', '', field_name, flags=re.IGNORECASE)\n",
    "    wrapper_keywords = r'Sum|Count|Avg|Min|Max|Unique Count|Monthly|Daily|Weekly|Quarterly|Yearly|Week|Month|Quarter|Year|Day'\n",
    "    while True:\n",
    "        match = re.match(r'^(' + wrapper_keywords + r')\\s*\\((.*)\\)$', name, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            name = match.group(2).strip()\n",
    "        else:\n",
    "            break\n",
    "    return name.strip()\n",
    "\n",
    "def extract_base_columns(columns: List[str]) -> List[str]:\n",
    "    \"\"\"Extract base column names without aggregations.\"\"\"\n",
    "    base_columns = []\n",
    "    for col in columns:\n",
    "        cleaned = clean_field_name(col)\n",
    "        if cleaned and cleaned not in base_columns:\n",
    "            base_columns.append(cleaned)\n",
    "    return base_columns\n",
    "\n",
    "\n",
    "# Main Extraction Logic\n",
    "def extract_tml_metadata():\n",
    "    \"\"\"Extract metadata from all TML files for mapping purposes.\"\"\"\n",
    "    print(\"--- Setting up mapping and failure log tables ---\")\n",
    "    setup_mapping_table()\n",
    "    setup_failure_log_table()\n",
    "    \n",
    "    try:\n",
    "        tml_files = [f.path for f in dbutils.fs.ls(TML_INPUT_PATH) \n",
    "                     if f.path.endswith(('.tml', '.yaml', '.json'))]\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Cannot list files in '{TML_INPUT_PATH}'. Error: {e}\")\n",
    "        return\n",
    "    \n",
    "    if not tml_files:\n",
    "        print(f\"No TML files found in {TML_INPUT_PATH}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(tml_files)} TML files to process.\")\n",
    "    \n",
    "    metadata_records = []\n",
    "    failure_records = []\n",
    "    \n",
    "    for tml_file_path in tml_files:\n",
    "        filename = Path(tml_file_path).name\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n--- Processing: {filename} ---\")\n",
    "            \n",
    "            try:\n",
    "                tml_data = parse_tml_file(tml_file_path)\n",
    "            except Exception as parse_error:\n",
    "                print(f\"  ERROR: Failed to parse TML file - {parse_error}\")\n",
    "                failure_records.append({\n",
    "                    'tml_file': filename,\n",
    "                    'error_type': 'PARSE_ERROR',\n",
    "                    'error_message': str(parse_error)[:1000],\n",
    "                    'failure_timestamp': datetime.now()\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            liveboard = tml_data.get('liveboard')\n",
    "            if not liveboard:\n",
    "                print(f\"  WARNING: No 'liveboard' key found in {filename}\")\n",
    "                failure_records.append({\n",
    "                    'tml_file': filename,\n",
    "                    'error_type': 'INVALID_STRUCTURE',\n",
    "                    'error_message': \"Missing 'liveboard' root key in TML file\",\n",
    "                    'failure_timestamp': datetime.now()\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            visualizations = liveboard.get('visualizations', [])\n",
    "            \n",
    "            if not visualizations:\n",
    "                print(f\"  WARNING: No visualizations found in {filename}\")\n",
    "                failure_records.append({\n",
    "                    'tml_file': filename,\n",
    "                    'error_type': 'NO_VISUALIZATIONS',\n",
    "                    'error_message': \"No visualizations found in liveboard\",\n",
    "                    'failure_timestamp': datetime.now()\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Found {len(visualizations)} visualizations\")\n",
    "            \n",
    "            for viz in visualizations:\n",
    "                try:\n",
    "                    answer = viz.get('answer', {})\n",
    "                    chart = answer.get('chart', {})\n",
    "                    \n",
    "                    viz_id = viz.get('id', 'unknown')\n",
    "                    viz_name = answer.get('name', 'Unnamed')\n",
    "                    \n",
    "                    display_mode = answer.get('display_mode', '')\n",
    "                    chart_type = chart.get('type', 'TABLE_MODE' if display_mode == 'TABLE_MODE' else 'UNKNOWN')\n",
    "                    \n",
    "                    table_name, table_id = extract_table_info(answer)\n",
    "                    columns_used_raw = extract_columns_from_answer(answer)\n",
    "                    base_columns = extract_base_columns(columns_used_raw)\n",
    "                    search_query = answer.get('search_query', '')\n",
    "                    \n",
    "                    record = {\n",
    "                        'tml_file': filename,\n",
    "                        'visualization_id': viz_id,\n",
    "                        'visualization_name': viz_name,\n",
    "                        'chart_type': chart_type,\n",
    "                        'tml_table_name': table_name,\n",
    "                        'tml_table_id': table_id,\n",
    "                        'tml_columns_used': base_columns,\n",
    "                        'tml_columns_raw': columns_used_raw,\n",
    "                        'databricks_table_name_ToBeFilled': '',\n",
    "                        'databricks_column_mapping_ToBeFilled': '{}',\n",
    "                        'common_dataset_name': None,\n",
    "                        'common_sql_query': None,\n",
    "                        'common_column_mapping': None,\n",
    "                        'search_query': search_query,\n",
    "                        'notes': f\"Extracted {len(base_columns)} unique columns\",\n",
    "                        'extraction_timestamp': datetime.now()\n",
    "                    }\n",
    "                    \n",
    "                    metadata_records.append(record)\n",
    "                    print(f\"  - {viz_name} ({chart_type}): {len(base_columns)} columns from table '{table_name}'\")\n",
    "                \n",
    "                except Exception as viz_error:\n",
    "                    print(f\"  ERROR processing visualization '{viz.get('id', 'unknown')}': {viz_error}\")\n",
    "                    failure_records.append({\n",
    "                        'tml_file': filename,\n",
    "                        'error_type': 'VISUALIZATION_ERROR',\n",
    "                        'error_message': f\"Viz ID: {viz.get('id', 'unknown')} - {str(viz_error)[:900]}\",\n",
    "                        'failure_timestamp': datetime.now()\n",
    "                    })\n",
    "                    continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing {filename}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            failure_records.append({\n",
    "                'tml_file': filename,\n",
    "                'error_type': 'PROCESSING_ERROR',\n",
    "                'error_message': str(e)[:1000],\n",
    "                'failure_timestamp': datetime.now()\n",
    "            })\n",
    "    \n",
    "    # --- SAVE METADATA & MERGE WITH DASHBOARD_QUERIES ---\n",
    "    if metadata_records:\n",
    "        print(f\"\\n--- Saving {len(metadata_records)} metadata records ---\")\n",
    "        df = pd.DataFrame(metadata_records)\n",
    "        \n",
    "        # ===> NEW: MERGE WITH GENERATED SQL QUERIES <===\n",
    "        if 'dashboard_queries' in globals() and isinstance(dashboard_queries, pd.DataFrame) and not dashboard_queries.empty:\n",
    "            print(\"Merging with generated SQL queries (dashboard_queries DataFrame)...\")\n",
    "            \n",
    "            # Identify columns to merge (common_sql_query is mandatory, common_dataset_name if exists)\n",
    "            cols_to_merge = ['visualization_id', 'visualization_name']\n",
    "            \n",
    "            # Check if dashboard_queries has the required columns\n",
    "            has_sql = 'common_sql_query' in dashboard_queries.columns\n",
    "            has_ds_name = 'common_dataset_name' in dashboard_queries.columns\n",
    "            \n",
    "            if has_sql:\n",
    "                cols_to_merge.append('common_sql_query')\n",
    "            if has_ds_name:\n",
    "                cols_to_merge.append('common_dataset_name')\n",
    "                \n",
    "            if has_sql:\n",
    "                try:\n",
    "                    # Ensure matching types for merge keys\n",
    "                    df['visualization_id'] = df['visualization_id'].astype(str)\n",
    "                    df['visualization_name'] = df['visualization_name'].astype(str)\n",
    "                    dashboard_queries['visualization_id'] = dashboard_queries['visualization_id'].astype(str)\n",
    "                    dashboard_queries['visualization_name'] = dashboard_queries['visualization_name'].astype(str)\n",
    "                    \n",
    "                    # Perform Left Join\n",
    "                    merged_df = pd.merge(\n",
    "                        df, \n",
    "                        dashboard_queries[cols_to_merge], \n",
    "                        on=['visualization_id', 'visualization_name'], \n",
    "                        how='left', \n",
    "                        suffixes=('', '_gen')\n",
    "                    )\n",
    "                    \n",
    "                    # Update common_sql_query\n",
    "                    merged_df['common_sql_query'] = merged_df['common_sql_query_gen'].combine_first(merged_df['common_sql_query'])\n",
    "                    \n",
    "                    # Update common_dataset_name if it was present\n",
    "                    if has_ds_name:\n",
    "                        merged_df['common_dataset_name'] = merged_df['common_dataset_name_gen'].combine_first(merged_df['common_dataset_name'])\n",
    "                        merged_df = merged_df.drop(columns=['common_dataset_name_gen'])\n",
    "                        \n",
    "                    # Drop temporary merge column\n",
    "                    merged_df = merged_df.drop(columns=['common_sql_query_gen'])\n",
    "                    \n",
    "                    df = merged_df\n",
    "                    print(\"Merge successful: common_sql_query updated.\")\n",
    "                    \n",
    "                except Exception as merge_e:\n",
    "                    print(f\"WARNING: Failed to merge generated SQL queries: {merge_e}\")\n",
    "            else:\n",
    "                print(\"WARNING: dashboard_queries DataFrame missing 'common_sql_query' column. Skipping merge.\")\n",
    "        else:\n",
    "            print(\"No 'dashboard_queries' DataFrame found or it is empty. Skipping SQL merge.\")\n",
    "\n",
    "        df['extraction_timestamp'] = pd.to_datetime(df['extraction_timestamp'])\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"tml_file\", StringType(), True),\n",
    "            StructField(\"visualization_id\", StringType(), True),\n",
    "            StructField(\"visualization_name\", StringType(), True),\n",
    "            StructField(\"chart_type\", StringType(), True),\n",
    "            StructField(\"tml_table_name\", StringType(), True),\n",
    "            StructField(\"tml_table_id\", StringType(), True),\n",
    "            StructField(\"tml_columns_used\", ArrayType(StringType()), True),\n",
    "            StructField(\"tml_columns_raw\", ArrayType(StringType()), True),\n",
    "            StructField(\"databricks_table_name_ToBeFilled\", StringType(), True),\n",
    "            StructField(\"databricks_column_mapping_ToBeFilled\", StringType(), True),\n",
    "            StructField(\"common_dataset_name\", StringType(), True),\n",
    "            StructField(\"common_sql_query\", StringType(), True),\n",
    "            StructField(\"common_column_mapping\", StringType(), True),\n",
    "            StructField(\"search_query\", StringType(), True),\n",
    "            StructField(\"notes\", StringType(), True),\n",
    "            StructField(\"extraction_timestamp\", TimestampType(), True)\n",
    "        ])\n",
    "        \n",
    "        spark_df = spark.createDataFrame(df, schema=schema)\n",
    "        spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(MAPPING_TABLE)\n",
    "        \n",
    "        print(f\"Successfully saved metadata to {MAPPING_TABLE}\")\n",
    "    else:\n",
    "        print(\"\\nNo metadata records extracted.\")\n",
    "    \n",
    "    if failure_records:\n",
    "        print(f\"\\n--- Saving {len(failure_records)} failure records ---\")\n",
    "        fail_df = pd.DataFrame(failure_records)\n",
    "        fail_df['failure_timestamp'] = pd.to_datetime(fail_df['failure_timestamp'])\n",
    "        \n",
    "        spark_fail_df = spark.createDataFrame(fail_df)\n",
    "        spark_fail_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(FAILURE_LOG_TABLE)\n",
    "        \n",
    "        print(f\"Failed TML files logged to {FAILURE_LOG_TABLE}\")\n",
    "    else:\n",
    "        print(\"\\nNo failures encountered - all TML files processed successfully!\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# Execute Extraction\n",
    "extract_tml_metadata()\n",
    "\n",
    "# View Results\n",
    "try:\n",
    "    df = spark.table(MAPPING_TABLE)\n",
    "    # Display results showing the merged SQL query\n",
    "    print(\"\\n--- Displaying Result with Generated SQL ---\")\n",
    "    display(df.select(\"visualization_name\", \"tml_columns_used\", \"common_sql_query\").orderBy(\"tml_file\"))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display table. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ea9d2d0-97cc-4326-a9e4-85c1d34456f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.3: Aggregation Expression Builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c351ac31-2f77-4e5d-861c-b6d867a839b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. CONFIGURATION\n",
    "# ----------------------------------------------------------------------\n",
    "CATALOG = \"dbx_migration_poc\"\n",
    "SCHEMA = \"dbx_migration_ts\"\n",
    "LINEAGE_TABLE = f\"{LINEAGE_TABLE_NAME}\"\n",
    "OUTPUT_TABLE = f\"{CATALOG}.{SCHEMA}.{asset_name}_support_viz_column_details\"\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ----------------------------------------------------------------------\n",
    "def sanitize_name(name):\n",
    "    if not name: return name\n",
    "    # Clean special characters and replace with underscore\n",
    "    clean = re.sub(r'[^a-zA-Z0-9]', '_', name)\n",
    "    clean = re.sub(r'_+', '_', clean)\n",
    "    return clean.strip('_')\n",
    "\n",
    "def normalize_for_check(name):\n",
    "    \"\"\"Removes all special characters and spaces for duplicate validation.\"\"\"\n",
    "    if not name: return \"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', name).lower()\n",
    "\n",
    "def quote_identifier(name):\n",
    "    if not name: return name\n",
    "    clean_name = name.strip('`')\n",
    "    if re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', clean_name):\n",
    "        return clean_name\n",
    "    return f\"`{clean_name}`\"\n",
    "\n",
    "def map_aggregation_to_spark(agg):\n",
    "    if not agg or pd.isna(agg): return None\n",
    "    agg_upper = str(agg).upper().strip()\n",
    "    mapping = {\n",
    "        \"MAXIMUM\": \"MAX\", \"MINIMUM\": \"MIN\", \"AVERAGE\": \"AVG\", \"MEAN\": \"AVG\",\n",
    "        \"UNIQUE COUNT\": \"COUNT_DISTINCT\", \"COUNT DISTINCT\": \"COUNT_DISTINCT\",\n",
    "        \"SUM\": \"SUM\", \"COUNT\": \"COUNT\", \"STD_DEV\": \"STDDEV\", \"VARIANCE\": \"VARIANCE\"\n",
    "    }\n",
    "    return mapping.get(agg_upper, agg_upper)\n",
    "\n",
    "def infer_aggregation_from_name(col_name):\n",
    "    col_lower = col_name.lower()\n",
    "    prefixes = [\n",
    "        (\"maximum \", \"MAX\"), (\"minimum \", \"MIN\"), (\"average \", \"AVG\"), (\"avg \", \"AVG\"),\n",
    "        (\"sum \", \"SUM\"), (\"count of \", \"COUNT\"), (\"count \", \"COUNT\"),\n",
    "        (\"unique count of \", \"COUNT_DISTINCT\"), (\"unique count \", \"COUNT_DISTINCT\"),\n",
    "        (\"unique number of \", \"COUNT_DISTINCT\")\n",
    "    ]\n",
    "    for prefix, func in prefixes:\n",
    "        if col_lower.startswith(prefix):\n",
    "            return func, col_name[len(prefix):].strip()\n",
    "    return None, col_name\n",
    "\n",
    "def format_sql_agg(func, column):\n",
    "    if func == \"COUNT_DISTINCT\":\n",
    "        return f\"COUNT(DISTINCT {column})\"\n",
    "    return f\"{func}({column})\"\n",
    "\n",
    "def get_agg_label(func):\n",
    "    if func == \"COUNT_DISTINCT\":\n",
    "        return \"COUNT(DISTINCT)\"\n",
    "    return func\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. GENERATION LOGIC\n",
    "# ----------------------------------------------------------------------\n",
    "def generate_column_details(df):\n",
    "    results = []\n",
    "    \n",
    "    # Drop duplicates to handle lineage grain\n",
    "    df_subset = df[['visualization_id', 'Visualization', 'Liveboard_Column', 'Model_Base_Column', 'Model_Aggregation']].drop_duplicates()\n",
    "    \n",
    "    for viz_id, group in df_subset.groupby('visualization_id'):\n",
    "        # Track existing columns in this specific visualization to prevent duplicates\n",
    "        seen_normalized_cols = set()\n",
    "        \n",
    "        for _, row in group.iterrows():\n",
    "            viz_name = row['Visualization']\n",
    "            col_name = row['Liveboard_Column']\n",
    "            base_col = row['Model_Base_Column']\n",
    "            meta_agg = row['Model_Aggregation']\n",
    "            \n",
    "            # Validation Step: Check if column is already available in the list\n",
    "            norm_name = normalize_for_check(col_name)\n",
    "            if norm_name in seen_normalized_cols:\n",
    "                continue\n",
    "            seen_normalized_cols.add(norm_name)\n",
    "            \n",
    "            # 1. Determine Aggregations\n",
    "            inferred_agg, clean_col_name = infer_aggregation_from_name(col_name)\n",
    "            spark_meta_agg = map_aggregation_to_spark(meta_agg)\n",
    "            \n",
    "            agg_val = \"NA\"\n",
    "            expr_val = \"\"\n",
    "            final_sanitized_name = sanitize_name(col_name) # Default: Full name (e.g. Month_Date)\n",
    "            \n",
    "            if spark_meta_agg:\n",
    "                agg_val = get_agg_label(spark_meta_agg)\n",
    "                # Expr uses the inner column name, Sanitized uses full name\n",
    "                inner_ref = quote_identifier(sanitize_name(clean_col_name))\n",
    "                expr_val = format_sql_agg(spark_meta_agg, inner_ref)\n",
    "                \n",
    "            elif inferred_agg:\n",
    "                agg_val = get_agg_label(inferred_agg)\n",
    "                inner_ref = quote_identifier(sanitize_name(clean_col_name))\n",
    "                expr_val = format_sql_agg(inferred_agg, inner_ref)\n",
    "                \n",
    "            else:\n",
    "                # 2. Handle Date Functions (DATE_TRUNC)\n",
    "                date_match = re.search(r'^(Year|Month|Day|Week|Quarter)\\s*\\(\\s*(.+?)\\s*\\)$', col_name, re.IGNORECASE)\n",
    "                \n",
    "                if date_match:\n",
    "                    granularity = date_match.group(1).upper()\n",
    "                    inner_col = date_match.group(2)\n",
    "                    \n",
    "                    agg_val = f\"TRUNC_{granularity}\"\n",
    "                    # Expression: DATE_TRUNC('MONTH', `Date`)\n",
    "                    inner_ref = quote_identifier(sanitize_name(inner_col))\n",
    "                    expr_val = f\"DATE_TRUNC('{granularity}', {inner_ref})\"\n",
    "                else:\n",
    "                    agg_val = \"NA\"\n",
    "                    expr_val = quote_identifier(sanitize_name(col_name))\n",
    "\n",
    "            results.append({\n",
    "                'VizID': viz_id,\n",
    "                'VizName': viz_name,\n",
    "                'ColumnName': col_name,\n",
    "                'ModelBaseColumn': base_col,\n",
    "                'Aggregation': agg_val,\n",
    "                'Expression': expr_val,\n",
    "                'Santized_Column' : final_sanitized_name # Kept as Month_Date, Start_Date etc.\n",
    "            })\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. EXECUTION\n",
    "# ----------------------------------------------------------------------\n",
    "try:\n",
    "    df_lineage = spark.table(LINEAGE_TABLE).toPandas()\n",
    "    df_details = generate_column_details(df_lineage)\n",
    "    \n",
    "    spark_df = spark.createDataFrame(df_details)\n",
    "    spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(OUTPUT_TABLE)\n",
    "    \n",
    "    print(f\"✅ Successfully saved {len(df_details)} rows to {OUTPUT_TABLE}\")\n",
    "    display(spark_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d381111a-a229-47f3-89a6-95959fb9e205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.4: Widget filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b6d352d-038e-44bd-9acb-9f6ea4bec455",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. CONFIGURATION\n",
    "# ----------------------------------------------------------------------\n",
    "CATALOG = \"dbx_migration_poc\"\n",
    "SCHEMA = \"dbx_migration_ts\"\n",
    "LINEAGE_TABLE = f\"{LINEAGE_TABLE_NAME}\"\n",
    "\n",
    "# Output table for this specific metadata\n",
    "OUTPUT_TABLE = f\"{CATALOG}.{SCHEMA}.{asset_name}_support_viz_filter_metadata\"\n",
    "\n",
    "# Path to the TML file\n",
    "TML_FILE_PATH = f\"/Volumes/dbx_migration_poc/dbx_migration_ts/lv_dashfiles_ak/liveboard/{tml_file}\"\n",
    "\n",
    "print(f\"Source Lineage: {LINEAGE_TABLE}\")\n",
    "print(f\"Source TML: {TML_FILE_PATH}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ----------------------------------------------------------------------\n",
    "def sanitize_name(name):\n",
    "    \"\"\"\n",
    "    Replaces spaces and special characters with underscores.\n",
    "    \"\"\"\n",
    "    if not name: return name\n",
    "    clean = re.sub(r'[^a-zA-Z0-9]', '_', name)\n",
    "    clean = re.sub(r'_+', '_', clean)\n",
    "    return clean.strip('_')\n",
    "\n",
    "def extract_viz_search_queries(tml_path):\n",
    "    \"\"\"\n",
    "    Reads TML file and returns a dict: {viz_id: search_query_string}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(tml_path, 'r') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        \n",
    "        viz_map = {}\n",
    "        for viz in data.get('liveboard', {}).get('visualizations', []):\n",
    "            viz_id = viz.get('id')\n",
    "            query = viz.get('answer', {}).get('search_query', '')\n",
    "            if viz_id:\n",
    "                viz_map[viz_id] = query\n",
    "        return viz_map\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not extract filters from TML: {e}\")\n",
    "        return {}\n",
    "\n",
    "def get_filters_for_column(search_query, col_name):\n",
    "    \"\"\"\n",
    "    Parses the search query to find filters applied specifically to 'col_name'.\n",
    "    Captures:\n",
    "      1. Dot notation: .weekly, .true\n",
    "      2. Operators: = 'Val', > 10, in {...}\n",
    "    \"\"\"\n",
    "    if not search_query or not col_name: return None\n",
    "    \n",
    "    filters = []\n",
    "    \n",
    "    # Escape column name for regex (e.g., escape parenthesis in \"Count(x)\")\n",
    "    # TML queries wrap columns in brackets: [Column Name]\n",
    "    col_pattern = re.escape(col_name)\n",
    "    \n",
    "    # 1. Dot Notation Filters (e.g., .weekly, .true, .open)\n",
    "    # Matches: [Column].value\n",
    "    # Group 1 captures the value after the dot\n",
    "    dot_regex = rf\"\\[{col_pattern}\\]\\.((?:'[^']*')|(?:\\S.*?))(?=\\s*\\[|$)\"\n",
    "    dot_matches = re.findall(dot_regex, search_query, re.IGNORECASE)\n",
    "    \n",
    "    for val in dot_matches:\n",
    "        # Strip quotes if present in the dot value (e.g. .'value')\n",
    "        clean_val = val.strip(\"'\")\n",
    "        # Just append the condition, not the column name\n",
    "        filters.append(f\".{clean_val}\")\n",
    "\n",
    "    # 2. Standard Operators (e.g., =, !=, >, in)\n",
    "    # Matches: [Column] op Value\n",
    "    # Value can be quoted string, number, or {set}\n",
    "    op_regex = rf\"\\[{col_pattern}\\]\\s*(=|!=|<>|>|<|>=|<=|in|not\\s+in)\\s*(\\'?[^\\'\\[\\]\\s]+\\'?|\\{{.*?\\}}|\\d+(?:\\.\\d+)?)\"\n",
    "    op_matches = re.findall(op_regex, search_query, re.IGNORECASE)\n",
    "    \n",
    "    for op, val in op_matches:\n",
    "        # Just append the condition, not the column name\n",
    "        filters.append(f\"{op} {val}\")\n",
    "        \n",
    "    return \", \".join(filters) if filters else None\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3. GENERATION LOGIC\n",
    "# ----------------------------------------------------------------------\n",
    "def generate_column_filter_metadata(df, viz_queries):\n",
    "    results = []\n",
    "    \n",
    "    # Get unique columns per visualization from lineage\n",
    "    df_subset = df[['visualization_id', 'Visualization', 'Liveboard_Column']].drop_duplicates()\n",
    "    \n",
    "    for (viz_id, viz_name), group in df_subset.groupby(['visualization_id', 'Visualization']):\n",
    "        \n",
    "        # Get the full search query for this visualization\n",
    "        search_query = viz_queries.get(viz_id, \"\")\n",
    "        \n",
    "        for _, row in group.iterrows():\n",
    "            raw_col = row['Liveboard_Column']\n",
    "            \n",
    "            # 1. Sanitize Name (as requested)\n",
    "            sanitized_col = sanitize_name(raw_col)\n",
    "            \n",
    "            # 2. Extract specific filters for this column\n",
    "            filter_details = get_filters_for_column(search_query, raw_col)\n",
    "            \n",
    "            results.append({\n",
    "                'viz_id': viz_id,\n",
    "                'viz_name': viz_name,\n",
    "                'Sanitized_Column': sanitized_col,\n",
    "                'Filter_Details': filter_details\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4. EXECUTION\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Load Lineage\n",
    "df_lineage = spark.table(LINEAGE_TABLE).toPandas()\n",
    "print(f\"Loaded {len(df_lineage)} lineage rows.\")\n",
    "\n",
    "# 2. Load TML Search Queries\n",
    "viz_queries = extract_viz_search_queries(TML_FILE_PATH)\n",
    "print(f\"Loaded queries for {len(viz_queries)} visualizations.\")\n",
    "\n",
    "# 3. Generate Metadata\n",
    "df_result = generate_column_filter_metadata(df_lineage, viz_queries)\n",
    "\n",
    "# 4. Save and Display\n",
    "if not df_result.empty:\n",
    "    spark_df = spark.createDataFrame(df_result)\n",
    "    spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(OUTPUT_TABLE)\n",
    "    print(f\"\\n✅ Successfully saved {len(df_result)} rows to {OUTPUT_TABLE}\")\n",
    "    display(spark_df)\n",
    "else:\n",
    "    print(\"No data generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c403d5a6-ba6b-4df8-af0e-540986c3e1de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(json.dumps({\n",
    "    \"Query\": final_sql_query.replace(\"\\n\", \"\"),\n",
    "    \"Filepath\": full_file_path\n",
    "}))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6672641058525296,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "TS_DBX_Model_Query_Builder",
   "widgets": {
    "tml_file": {
     "currentValue": "ServiceNow_BU.liveboard.tml",
     "nuid": "8501f9d8-6cf0-4fb5-9a55-b72ee79522a7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "tml_file",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "tml_file",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
